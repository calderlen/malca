{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Bayesian light-curve excursion output EDA\n",
    "\n",
    "Inspect `output/lc_excursions_bayes_results_12.5_13.parquet` produced by `malca/events.py`. Update `DATA_PATH` below if you want to point at another run (e.g., a different magnitude bin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "sns.set_theme(style=\"ticks\", context=\"talk\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.width\", 160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../output/lc_excursions_bayes_results_12.5_13.parquet\")\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Expected Parquet at {DATA_PATH}. Update DATA_PATH to match your run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def assert_parquet_magic(path: Path):\n",
    "    with path.open('rb') as f:\n",
    "        head = f.read(4)\n",
    "        try:\n",
    "            f.seek(-4, 2)\n",
    "            tail = f.read(4)\n",
    "        except OSError:\n",
    "            tail = b''\n",
    "    if head != b'PAR1' or tail != b'PAR1':\n",
    "        raise RuntimeError(\n",
    "            \"File does not look like a complete Parquet file (missing magic bytes). \"\n",
    "            \"It may be truncated or still writing; rerun the generator to regenerate it.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def load_results(path: Path) -> pd.DataFrame:\n",
    "    assert_parquet_magic(path)\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "    except ImportError as exc:\n",
    "        raise ImportError(\n",
    "            \"Install a Parquet engine (e.g., `pip install pyarrow` or `conda install -c conda-forge pyarrow`).\"\n",
    "        ) from exc\n",
    "    except Exception as exc:\n",
    "        # Fallback that ignores pandas extension metadata to avoid ArrowKeyError\n",
    "        try:\n",
    "            import pyarrow.parquet as pq\n",
    "            table = pq.read_table(path, use_pandas_metadata=False)\n",
    "            df = table.to_pandas()\n",
    "            print(f\"Loaded via pyarrow fallback (ignored pandas metadata). Original error: {exc}\")\n",
    "        except Exception as exc2:\n",
    "            raise RuntimeError(f\"Failed to read parquet at {path}: {exc}\") from exc2\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"source_file\"] = df[\"path\"].astype(str)\n",
    "    df[\"target_id\"] = df[\"source_file\"].map(lambda p: Path(p).stem.replace(\"-light-curves\", \"\"))\n",
    "    stem = path.stem\n",
    "    mag_token = stem.split(\"results_\", 1)[-1] if \"results_\" in stem else stem\n",
    "    df[\"mag_bin\"] = mag_token.split(\"_PROCESSED\", 1)[0]\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_results(DATA_PATH)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = pd.Series({\n",
    "    \"rows\": len(df),\n",
    "    \"unique_targets\": df[\"target_id\"].nunique(),\n",
    "    \"mag_bins\": \", \".join(sorted(df[\"mag_bin\"].unique())),\n",
    "    \"significant_dips\": int(df[\"dip_significant\"].sum()),\n",
    "    \"significant_jumps\": int(df[\"jump_significant\"].sum()),\n",
    "    \"significant_both\": int((df[\"dip_significant\"] & df[\"jump_significant\"]).sum()),\n",
    "})\n",
    "overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "df[numeric_cols].describe().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Significance flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_status = np.select(\n",
    "    [\n",
    "        df[\"dip_significant\"] & df[\"jump_significant\"],\n",
    "        df[\"dip_significant\"],\n",
    "        df[\"jump_significant\"],\n",
    "    ],\n",
    "    [\"dip & jump\", \"dip only\", \"jump only\"],\n",
    "    default=\"none\",\n",
    ")\n",
    "sig_counts = pd.Series(sig_status).value_counts().rename_axis(\"status\").reset_index(name=\"count\")\n",
    "sig_counts[\"fraction\"] = sig_counts[\"count\"] / len(df)\n",
    "\n",
    "display(sig_counts)\n",
    "\n",
    "sns.barplot(data=sig_counts, x=\"status\", y=\"count\", palette=\"viridis\")\n",
    "plt.title(\"Significance counts\")\n",
    "sns.despine()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Bayes factor distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "for ax, kind in zip(axes, [\"dip\", \"jump\"]):\n",
    "    col = f\"{kind}_bayes_factor\"\n",
    "    vals = df[col].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    sns.histplot(np.log10(np.clip(vals, 1e-8, None)), bins=40, ax=ax)\n",
    "    ax.set_xlabel(f\"log10({col})\")\n",
    "    ax.set_title(f\"{kind.title()} Bayes factor (log10)\")\n",
    "    ax.axvline(0, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Run-level diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharex=False, sharey=False)\n",
    "pairs = [\n",
    "    (\"dip_max_run_sum\", \"dip_max_log_bf_local\", \"dip_significant\", \"Dip\"),\n",
    "    (\"jump_max_run_sum\", \"jump_max_log_bf_local\", \"jump_significant\", \"Jump\"),\n",
    "]\n",
    "\n",
    "for ax, (xcol, ycol, flag, label) in zip(axes, pairs):\n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x=xcol,\n",
    "        y=ycol,\n",
    "        hue=flag,\n",
    "        palette={True: \"#1b9e77\", False: \"#d95f02\"},\n",
    "        alpha=0.6,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(f\"{label}: run summary vs. per-point log BF\")\n",
    "    ax.legend(title=\"significant\")\n",
    "    ax.grid(alpha=0.3)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Morphology breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = (\n",
    "    df[[\"target_id\", \"dip_best_morph\", \"jump_best_morph\"]]\n",
    "    .rename(columns={\"dip_best_morph\": \"dip\", \"jump_best_morph\": \"jump\"})\n",
    "    .melt(id_vars=\"target_id\", var_name=\"kind\", value_name=\"morphology\")\n",
    ")\n",
    "order = morph[\"morphology\"].value_counts().index\n",
    "\n",
    "sns.countplot(data=morph, y=\"morphology\", hue=\"kind\", order=order)\n",
    "plt.title(\"Best-fit morphology counts\")\n",
    "sns.despine()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Top candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_candidates(df: pd.DataFrame, kind: str, n: int = 15):\n",
    "    cols = [\n",
    "        \"target_id\",\n",
    "        \"path\",\n",
    "        f\"{kind}_bayes_factor\",\n",
    "        f\"{kind}_max_run_sum\",\n",
    "        f\"{kind}_max_log_bf_local\",\n",
    "        f\"{kind}_max_run_duration\",\n",
    "        f\"{kind}_max_run_points\",\n",
    "        f\"{kind}_best_morph\",\n",
    "        f\"{kind}_best_delta_bic\",\n",
    "        f\"{kind}_best_width_param\",\n",
    "    ]\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    subset = df[df[f\"{kind}_significant\"]].sort_values(by=f\"{kind}_bayes_factor\", ascending=False)\n",
    "    return subset[cols].head(n)\n",
    "\n",
    "top_dips = top_candidates(df, \"dip\")\n",
    "top_jumps = top_candidates(df, \"jump\")\n",
    "\n",
    "display(top_dips)\n",
    "display(top_jumps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
