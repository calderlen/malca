{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Comprehensive EDA: Light Curve Events Detection Results\n",
    "\n",
    "**MALCA Pipeline Analysis** - Multi-timescale ASAS-SN Light Curve Analysis\n",
    "\n",
    "This notebook analyzes detection results from the MALCA events.py pipeline for two magnitude bins:\n",
    "- `lc_events_results_12_12.5.csv` - Brighter stars (12.0 < mag < 12.5)\n",
    "- `lc_events_results_local_12.5_13.csv` - Fainter stars (12.5 < mag < 13.0)\n",
    "\n",
    "## Scientific Context\n",
    "\n",
    "**Goal**: Discover **young stellar objects (YSOs) with circumstellar disks** that periodically occult their host stars, creating characteristic dips in brightness lasting days to weeks.\n",
    "\n",
    "### What we're looking for:\n",
    "- **Dips**: Discrete, isolated dimming events (>0.3 mag) with flat baselines\n",
    "- **Duration**: Days to weeks (not hours like EBs, not years like long-term trends)\n",
    "- **Morphology**: Gaussian or skew-Gaussian shapes (asymmetric ingress/egress typical of disk occultations)\n",
    "- **Multi-camera consistency**: Real dips appear in all cameras simultaneously\n",
    "\n",
    "### What we're filtering out:\n",
    "- Eclipsing binaries (symmetric, periodic, short duration)\n",
    "- Semi-regular pulsators (continuous variability)\n",
    "- Known VSX variables\n",
    "- Single-camera artifacts\n",
    "- Noise fluctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 60)\n",
    "pd.set_option('display.width', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## 1. Load Both Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": "# Platform-independent paths\nif platform.system() == 'Darwin':  # macOS\n    base_path = Path('/Users/calder/code/malca/output')\nelse:  # Linux\n    base_path = Path('/home/calder/code/malca/output')\n\n# Load all three datasets\ndf_bright = pd.read_csv(base_path / 'lc_events_results_12_12.5.csv')\ndf_mid = pd.read_csv(base_path / 'lc_events_results_local_12.5_13.csv')\ndf_faint = pd.read_csv(base_path / 'lc_events_results_13_13.5.csv')\n\n# Add magnitude bin labels\ndf_bright['mag_bin'] = '12.0-12.5'\ndf_mid['mag_bin'] = '12.5-13.0'\ndf_faint['mag_bin'] = '13.0-13.5'\n\nprint(f\"Bright sample (12.0-12.5 mag): {len(df_bright):,} light curves\")\nprint(f\"Mid sample (12.5-13.0 mag):    {len(df_mid):,} light curves\")\nprint(f\"Faint sample (13.0-13.5 mag):  {len(df_faint):,} light curves\")\nprint(f\"Total: {len(df_bright) + len(df_mid) + len(df_faint):,} light curves\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combine-data",
   "metadata": {},
   "outputs": [],
   "source": "# Combine for comparative analysis\ndf_combined = pd.concat([df_bright, df_mid, df_faint], ignore_index=True)\nprint(f\"Combined dataset: {len(df_combined):,} light curves\")\nprint(f\"\\nColumns ({len(df_combined.columns)}):\")\nprint(df_combined.columns.tolist())\n\n# Create a list of dataframes for iteration\ndatasets = [\n    (df_bright, '12.0-12.5', 'steelblue'),\n    (df_mid, '12.5-13.0', 'coral'),\n    (df_faint, '13.0-13.5', 'seagreen')\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-info",
   "metadata": {},
   "outputs": [],
   "source": "# Check for column differences between datasets\ncols_bright = set(df_bright.columns)\ncols_mid = set(df_mid.columns)\ncols_faint = set(df_faint.columns)\n\nall_cols = cols_bright | cols_mid | cols_faint\ncommon_cols = cols_bright & cols_mid & cols_faint\n\nif len(all_cols) == len(common_cols):\n    print(\"All three datasets have identical columns (except mag_bin)\")\nelse:\n    print(\"Column differences detected:\")\n    for name, cols in [('Bright', cols_bright), ('Mid', cols_mid), ('Faint', cols_faint)]:\n        unique = cols - common_cols\n        if unique:\n            print(f\"  Only in {name}: {unique}\")"
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 2. Detection Summary\n",
    "\n",
    "Key detection flags:\n",
    "- `dip_significant`: Passed all thresholds for dip detection (likely disk occultation)\n",
    "- `jump_significant`: Passed all thresholds for jump detection (possibly microlensing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detection-summary",
   "metadata": {},
   "outputs": [],
   "source": "def detection_summary(df, label):\n    n = len(df)\n    n_dip = df['dip_significant'].sum()\n    n_jump = df['jump_significant'].sum()\n    n_either = (df['dip_significant'] | df['jump_significant']).sum()\n    n_both = (df['dip_significant'] & df['jump_significant']).sum()\n    \n    print(f\"=== {label} ===\")\n    print(f\"Total light curves:     {n:>8,}\")\n    print(f\"Dip detections:         {n_dip:>8,} ({n_dip/n*100:5.2f}%)\")\n    print(f\"Jump detections:        {n_jump:>8,} ({n_jump/n*100:5.2f}%)\")\n    print(f\"Either dip or jump:     {n_either:>8,} ({n_either/n*100:5.2f}%)\")\n    print(f\"Both dip and jump:      {n_both:>8,} ({n_both/n*100:5.2f}%)\")\n    print()\n    return {'label': label, 'n': n, 'n_dip': n_dip, 'n_jump': n_jump, 'n_either': n_either, 'n_both': n_both}\n\nstats_bright = detection_summary(df_bright, 'Bright Sample (12.0-12.5 mag)')\nstats_mid = detection_summary(df_mid, 'Mid Sample (12.5-13.0 mag)')\nstats_faint = detection_summary(df_faint, 'Faint Sample (13.0-13.5 mag)')\nstats_combined = detection_summary(df_combined, 'Combined')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detection-comparison-plot",
   "metadata": {},
   "outputs": [],
   "source": "# Visual comparison of detection rates\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Bar chart of detection counts\nx = np.arange(4)\nwidth = 0.25\nlabels = ['Dips', 'Jumps', 'Either', 'Both']\n\nbright_vals = [stats_bright['n_dip'], stats_bright['n_jump'], stats_bright['n_either'], stats_bright['n_both']]\nmid_vals = [stats_mid['n_dip'], stats_mid['n_jump'], stats_mid['n_either'], stats_mid['n_both']]\nfaint_vals = [stats_faint['n_dip'], stats_faint['n_jump'], stats_faint['n_either'], stats_faint['n_both']]\n\nbars1 = axes[0].bar(x - width, bright_vals, width, label='12.0-12.5 mag', color='steelblue')\nbars2 = axes[0].bar(x, mid_vals, width, label='12.5-13.0 mag', color='coral')\nbars3 = axes[0].bar(x + width, faint_vals, width, label='13.0-13.5 mag', color='seagreen')\naxes[0].set_ylabel('Count')\naxes[0].set_title('Detection Counts by Magnitude Bin')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(labels)\naxes[0].legend()\naxes[0].set_yscale('log')\n\n# Detection rates (percentage)\nbright_rates = [v/stats_bright['n']*100 for v in bright_vals]\nmid_rates = [v/stats_mid['n']*100 for v in mid_vals]\nfaint_rates = [v/stats_faint['n']*100 for v in faint_vals]\n\nbars1 = axes[1].bar(x - width, bright_rates, width, label='12.0-12.5 mag', color='steelblue')\nbars2 = axes[1].bar(x, mid_rates, width, label='12.5-13.0 mag', color='coral')\nbars3 = axes[1].bar(x + width, faint_rates, width, label='13.0-13.5 mag', color='seagreen')\naxes[1].set_ylabel('Detection Rate (%)')\naxes[1].set_title('Detection Rates by Magnitude Bin')\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(labels)\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "jxn2b7m85w",
   "source": "## 2.1 Key Detection Stage Counts\n\nProgressive filtering stages:\n1. **Dip Detected** (`dip_significant=True`): Light curves with statistically significant dip events\n2. **Gaussian Morphology** (`dip_best_morph='gaussian'`): Dips with symmetric Gaussian shape\n3. **Final Filter (cams>=2)**: Multi-camera confirmed dips (robust against instrumental artifacts)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "r8tlycc0csj",
   "source": "# Key detection stage counts\nprint(\"=\" * 70)\nprint(\"KEY DETECTION STAGE COUNTS\")\nprint(\"=\" * 70)\n\ndef count_detection_stages(df, label):\n    \"\"\"Count light curves at each detection stage.\"\"\"\n    n_total = len(df)\n    \n    # Stage 1: Dip detected (dip_significant = True)\n    n_dip_detected = df['dip_significant'].sum()\n    \n    # Stage 2: Gaussian dip morphology\n    n_gaussian_morph = (df['dip_best_morph'] == 'gaussian').sum()\n    \n    # Stage 3: Final filter - multi-camera confirmed (cams >= 2)\n    # This requires having a dip run with at least 2 cameras\n    n_final_filter = (\n        (df['dip_run_count'] >= 1) & \n        (df['dip_max_run_cameras'] >= 2)\n    ).sum()\n    \n    # Intersection: Gaussian morphology AND multi-camera\n    n_gaussian_multicam = (\n        (df['dip_best_morph'] == 'gaussian') & \n        (df['dip_max_run_cameras'] >= 2)\n    ).sum()\n    \n    print(f\"\\n{label}:\")\n    print(f\"  {'Stage':<40} {'Count':>10} {'% of Total':>12}\")\n    print(f\"  {'-'*40} {'-'*10} {'-'*12}\")\n    print(f\"  {'Total light curves':<40} {n_total:>10,} {100.0:>11.2f}%\")\n    print(f\"  {'(1) Dip detected (dip_significant)':<40} {n_dip_detected:>10,} {n_dip_detected/n_total*100:>11.2f}%\")\n    print(f\"  {'(2) Gaussian dip morphology':<40} {n_gaussian_morph:>10,} {n_gaussian_morph/n_total*100:>11.2f}%\")\n    print(f\"  {'(3) Final filter (cams >= 2)':<40} {n_final_filter:>10,} {n_final_filter/n_total*100:>11.2f}%\")\n    print(f\"  {'(2) + (3) Gaussian AND cams >= 2':<40} {n_gaussian_multicam:>10,} {n_gaussian_multicam/n_total*100:>11.2f}%\")\n    \n    return {\n        'label': label,\n        'total': n_total,\n        'dip_detected': n_dip_detected,\n        'gaussian_morph': n_gaussian_morph,\n        'final_filter_cams2': n_final_filter,\n        'gaussian_and_cams2': n_gaussian_multicam\n    }\n\n# Count for each sample\ncounts_bright = count_detection_stages(df_bright, \"Bright Sample (12.0-12.5 mag)\")\ncounts_mid = count_detection_stages(df_mid, \"Mid Sample (12.5-13.0 mag)\")\ncounts_faint = count_detection_stages(df_faint, \"Faint Sample (13.0-13.5 mag)\")\ncounts_combined = count_detection_stages(df_combined, \"Combined\")\n\n# Visual comparison\nprint(\"\\n\" + \"=\" * 70)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Bar chart comparison\nstages = ['Dip Detected', 'Gaussian Morph', 'Cams >= 2', 'Gaussian +\\nCams >= 2']\nbright_vals = [counts_bright['dip_detected'], counts_bright['gaussian_morph'], \n               counts_bright['final_filter_cams2'], counts_bright['gaussian_and_cams2']]\nmid_vals = [counts_mid['dip_detected'], counts_mid['gaussian_morph'], \n            counts_mid['final_filter_cams2'], counts_mid['gaussian_and_cams2']]\nfaint_vals = [counts_faint['dip_detected'], counts_faint['gaussian_morph'], \n              counts_faint['final_filter_cams2'], counts_faint['gaussian_and_cams2']]\n\nx = np.arange(len(stages))\nwidth = 0.25\n\nbars1 = axes[0].bar(x - width, bright_vals, width, label='12.0-12.5 mag', color='steelblue')\nbars2 = axes[0].bar(x, mid_vals, width, label='12.5-13.0 mag', color='coral')\nbars3 = axes[0].bar(x + width, faint_vals, width, label='13.0-13.5 mag', color='seagreen')\naxes[0].set_ylabel('Count')\naxes[0].set_title('Detection Stage Counts')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(stages)\naxes[0].legend()\naxes[0].set_yscale('log')\n\n# Combined funnel chart\ncombined_vals = [counts_combined['total'], counts_combined['dip_detected'], \n                 counts_combined['gaussian_morph'], counts_combined['final_filter_cams2'],\n                 counts_combined['gaussian_and_cams2']]\ncombined_labels = ['Total LCs', 'Dip Detected', 'Gaussian Morph', 'Cams >= 2', 'Gaussian + Cams>=2']\ncolors = plt.cm.Blues(np.linspace(0.3, 0.9, len(combined_vals)))\n\nbars = axes[1].barh(range(len(combined_vals)), combined_vals, color=colors, edgecolor='black')\naxes[1].set_yticks(range(len(combined_vals)))\naxes[1].set_yticklabels(combined_labels)\naxes[1].set_xlabel('Count')\naxes[1].set_title('Combined Detection Stages')\naxes[1].set_xscale('log')\naxes[1].invert_yaxis()\n\nfor bar, val in zip(bars, combined_vals):\n    axes[1].text(val, bar.get_y() + bar.get_height()/2, f'  {val:,}', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "x69rfnq7jgr",
   "source": "## 2.2 Known Candidate Validation\n\nCheck if known candidates from Brayden's list are recovered by our filters.\nOnly checking candidates that fall within the magnitude bins of these datasets (12_12.5 and 12.5_13).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bjy79j2ffjv",
   "source": "# Known candidates list\nbrayden_candidates = [\n    {\"source\": \"J042214+152530\", \"source_id\": \"377957522430\", \"category\": \"Dippers\", \"mag_bin\": \"13_13.5\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J202402+383938\", \"source_id\": \"42950993887\", \"category\": \"Dippers\", \"mag_bin\": \"13_13.5\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J174328+343315\", \"source_id\": \"223339338105\", \"category\": \"Dippers\", \"mag_bin\": \"13_13.5\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J080327-261620\", \"source_id\": \"601296043597\", \"category\": \"Dippers\", \"mag_bin\": \"13_13.5\", \"search_method\": \"Pipeline\", \"expected_detected\": False},\n    {\"source\": \"J184916-473251\", \"source_id\": \"472447294641\", \"category\": \"Dippers\", \"mag_bin\": \"13_13.5\", \"search_method\": \"Known\", \"expected_detected\": True},\n    {\"source\": \"J183153-284827\", \"source_id\": \"455267102087\", \"category\": \"Dippers\", \"mag_bin\": \"13.5_14\", \"search_method\": \"Known\", \"expected_detected\": False},\n    {\"source\": \"J070519+061219\", \"source_id\": \"266288137752\", \"category\": \"Dippers\", \"mag_bin\": \"13.5_14\", \"search_method\": \"Known\", \"expected_detected\": False},\n    {\"source\": \"J081523-385923\", \"source_id\": \"532576686103\", \"category\": \"Dippers\", \"mag_bin\": \"13.5_14\", \"search_method\": \"Known\", \"expected_detected\": False},\n    {\"source\": \"J085816-430955\", \"source_id\": \"352187470767\", \"category\": \"Dippers\", \"mag_bin\": \"12_12.5\", \"search_method\": \"Known\", \"expected_detected\": False},\n    {\"source\": \"J114712-621037\", \"source_id\": \"609886184506\", \"category\": \"Dippers\", \"mag_bin\": \"13_13.5\", \"search_method\": \"Known\", \"expected_detected\": False},\n    {\"source\": \"J005437+644347\", \"source_id\": \"68720274411\", \"category\": \"Multiple Eclipse Binaries\", \"mag_bin\": \"13_13.5\", \"search_method\": \"Known\", \"expected_detected\": True},\n    {\"source\": \"J062510-075341\", \"source_id\": \"377958261591\", \"category\": \"Multiple Eclipse Binaries\", \"mag_bin\": \"13.5_14\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J124745-622756\", \"source_id\": \"515397118400\", \"category\": \"Multiple Eclipse Binaries\", \"mag_bin\": \"13.5_14\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J175912-120956\", \"source_id\": \"326417831663\", \"category\": \"Multiple Eclipse Binaries\", \"mag_bin\": \"13_13.5\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J181752-580749\", \"source_id\": \"644245387906\", \"category\": \"Multiple Eclipse Binaries\", \"mag_bin\": \"12_12.5\", \"search_method\": \"Known\", \"expected_detected\": True},\n    {\"source\": \"J160757-574540\", \"source_id\": \"661425129485\", \"category\": \"Multiple Eclipse Binaries\", \"mag_bin\": \"13.5_14\", \"search_method\": \"Pipeline\", \"expected_detected\": False},\n    {\"source\": \"J073924-272916\", \"source_id\": \"438086977939\", \"category\": \"Single Eclipse Binaries\", \"mag_bin\": \"13.5_14\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J074007-161608\", \"source_id\": \"360777377116\", \"category\": \"Single Eclipse Binaries\", \"mag_bin\": \"13_13.5\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J094848-545959\", \"source_id\": \"635655234580\", \"category\": \"Single Eclipse Binaries\", \"mag_bin\": \"13_13.5\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J162209-444247\", \"source_id\": \"412317159120\", \"category\": \"Single Eclipse Binaries\", \"mag_bin\": \"13.5_14\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J183606-314826\", \"source_id\": \"438086901547\", \"category\": \"Single Eclipse Binaries\", \"mag_bin\": \"13.5_14\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J205245-713514\", \"source_id\": \"463856535113\", \"category\": \"Single Eclipse Binaries\", \"mag_bin\": \"13_13.5\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J212132+480140\", \"source_id\": \"120259184943\", \"category\": \"Single Eclipse Binaries\", \"mag_bin\": \"13_13.5\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J225702+562312\", \"source_id\": \"25770019815\", \"category\": \"Single Eclipse Binaries\", \"mag_bin\": \"13.5_14\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J190316-195739\", \"source_id\": \"515396514761\", \"category\": \"Single Eclipse Binaries\", \"mag_bin\": \"13.5_14\", \"search_method\": \"Pipeline\", \"expected_detected\": True},\n    {\"source\": \"J175602+013135\", \"source_id\": \"231929175915\", \"category\": \"Single Eclipse Binaries\", \"mag_bin\": \"14_14.5\", \"search_method\": \"Known\", \"expected_detected\": True},\n    {\"source\": \"J073234-200049\", \"source_id\": \"335007754417\", \"category\": \"Single Eclipse Binaries\", \"mag_bin\": \"14.5_15\", \"search_method\": \"Known\", \"expected_detected\": True},\n    {\"source\": \"J223332+565552\", \"source_id\": \"60130040391\", \"category\": \"Single Eclipse Binaries\", \"mag_bin\": \"12.5_13\", \"search_method\": \"Known\", \"expected_detected\": True},\n    {\"source\": \"J183210-173432\", \"source_id\": \"317827964025\", \"category\": \"Single Eclipse Binaries\", \"mag_bin\": \"12.5_13\", \"search_method\": \"Pipeline\", \"expected_detected\": False},\n]\n\n# Filter to candidates in our magnitude bins (now includes 13_13.5)\nrelevant_mag_bins = [\"12_12.5\", \"12.5_13\", \"13_13.5\"]\nrelevant_candidates = [c for c in brayden_candidates if c[\"mag_bin\"] in relevant_mag_bins]\n\nprint(\"=\" * 80)\nprint(\"KNOWN CANDIDATE VALIDATION - ALL FILTERS (including Gaussian morphology)\")\nprint(\"=\" * 80)\nprint(f\"\\nCandidates in relevant mag bins ({relevant_mag_bins}): {len(relevant_candidates)}\")\n\n# Extract source_id from path (filename without .dat2)\ndf_combined['source_id'] = df_combined['path'].apply(lambda x: x.split('/')[-1].replace('.dat2', ''))\n\n# Define ALL filters (strict)\ndef passes_all_filters(row):\n    \"\"\"Check if a light curve passes ALL filters including Gaussian morphology.\"\"\"\n    return (\n        (row['dip_bayes_factor'] > 10) &\n        (row['dip_max_event_prob'] > 0.5) &\n        (row['dip_run_count'] >= 1) &\n        (row['dip_max_run_points'] >= 2) &\n        (row['dip_max_run_cameras'] >= 2) &\n        (row['dip_best_morph'] in ['gaussian', 'skew_gaussian']) &\n        (row['dip_best_delta_bic'] > 10)\n    )\n\n# Check each candidate\nresults_all_filters = []\nfor cand in relevant_candidates:\n    source_id = cand[\"source_id\"]\n    mag_bin = cand[\"mag_bin\"]\n    \n    # Select appropriate dataframe based on mag_bin\n    if mag_bin == \"12_12.5\":\n        df_check = df_bright\n    elif mag_bin == \"12.5_13\":\n        df_check = df_mid\n    else:  # 13_13.5\n        df_check = df_faint\n    \n    # Extract source_id for this dataframe\n    df_check = df_check.copy()\n    df_check['source_id'] = df_check['path'].apply(lambda x: x.split('/')[-1].replace('.dat2', ''))\n    \n    # Find the candidate\n    match = df_check[df_check['source_id'] == source_id]\n    \n    if len(match) == 0:\n        status = \"NOT FOUND\"\n        passed = False\n        details = \"Source ID not in dataset\"\n    else:\n        row = match.iloc[0]\n        passed = passes_all_filters(row)\n        status = \"PASSED\" if passed else \"FAILED\"\n        details = f\"BF={row['dip_bayes_factor']:.1f}, morph={row['dip_best_morph']}, cams={row['dip_max_run_cameras']}\"\n    \n    results_all_filters.append({\n        'source': cand['source'],\n        'source_id': source_id,\n        'category': cand['category'],\n        'mag_bin': mag_bin,\n        'expected': cand['expected_detected'],\n        'passed': passed if len(match) > 0 else None,\n        'status': status,\n        'details': details\n    })\n\n# Display results\nprint(f\"\\n{'Source':<20} {'Category':<25} {'Mag Bin':<10} {'Expected':<10} {'Status':<12} {'Details'}\")\nprint(\"-\" * 120)\nfor r in results_all_filters:\n    exp_str = \"Yes\" if r['expected'] else \"No\"\n    match_icon = \"\"\n    if r['status'] != \"NOT FOUND\":\n        if r['passed'] == r['expected']:\n            match_icon = \"OK\"\n        else:\n            match_icon = \"MISMATCH\"\n    print(f\"{r['source']:<20} {r['category']:<25} {r['mag_bin']:<10} {exp_str:<10} {r['status']:<12} {r['details']:<40} {match_icon}\")\n\n# Summary\nn_found = sum(1 for r in results_all_filters if r['status'] != \"NOT FOUND\")\nn_passed = sum(1 for r in results_all_filters if r['status'] == \"PASSED\")\nn_expected_pass = sum(1 for r in results_all_filters if r['expected'] and r['status'] != \"NOT FOUND\")\nn_correct = sum(1 for r in results_all_filters if r['status'] != \"NOT FOUND\" and r['passed'] == r['expected'])\n\nprint(f\"\\n{'='*80}\")\nprint(f\"SUMMARY (ALL FILTERS):\")\nprint(f\"  Candidates in mag bins: {len(relevant_candidates)}\")\nprint(f\"  Found in data:          {n_found}\")\nprint(f\"  Passed all filters:     {n_passed}\")\nprint(f\"  Expected to pass:       {n_expected_pass}\")\ncorrect_pct = f\"{n_correct/n_found*100:.1f}%\" if n_found > 0 else \"N/A\"\nprint(f\"  Correct predictions:    {n_correct}/{n_found} ({correct_pct})\")\nprint(f\"{'='*80}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "96em8xou5c",
   "source": "# Check candidates WITHOUT Gaussian morphology filter\nprint(\"=\" * 80)\nprint(\"KNOWN CANDIDATE VALIDATION - WITHOUT GAUSSIAN FILTER\")\nprint(\"(All other filters still applied: BF>10, prob>0.5, runs>=1, pts>=2, cams>=2)\")\nprint(\"=\" * 80)\n\n# Define filters WITHOUT morphology requirement\ndef passes_filters_no_morph(row):\n    \"\"\"Check if a light curve passes all filters EXCEPT Gaussian morphology.\"\"\"\n    return (\n        (row['dip_bayes_factor'] > 10) &\n        (row['dip_max_event_prob'] > 0.5) &\n        (row['dip_run_count'] >= 1) &\n        (row['dip_max_run_points'] >= 2) &\n        (row['dip_max_run_cameras'] >= 2)\n        # NO morphology filter\n        # NO delta BIC filter (since that's tied to morphology fitting)\n    )\n\n# Check each candidate\nresults_no_morph = []\nfor cand in relevant_candidates:\n    source_id = cand[\"source_id\"]\n    mag_bin = cand[\"mag_bin\"]\n    \n    # Select appropriate dataframe based on mag_bin\n    if mag_bin == \"12_12.5\":\n        df_check = df_bright\n    elif mag_bin == \"12.5_13\":\n        df_check = df_mid\n    else:  # 13_13.5\n        df_check = df_faint\n    \n    # Extract source_id for this dataframe\n    df_check = df_check.copy()\n    df_check['source_id'] = df_check['path'].apply(lambda x: x.split('/')[-1].replace('.dat2', ''))\n    \n    # Find the candidate\n    match = df_check[df_check['source_id'] == source_id]\n    \n    if len(match) == 0:\n        status = \"NOT FOUND\"\n        passed = False\n        passed_all = False\n        details = \"Source ID not in dataset\"\n    else:\n        row = match.iloc[0]\n        passed = passes_filters_no_morph(row)\n        passed_all = passes_all_filters(row)\n        status = \"PASSED\" if passed else \"FAILED\"\n        details = f\"BF={row['dip_bayes_factor']:.1f}, runs={row['dip_run_count']}, cams={row['dip_max_run_cameras']}, morph={row['dip_best_morph']}\"\n    \n    results_no_morph.append({\n        'source': cand['source'],\n        'source_id': source_id,\n        'category': cand['category'],\n        'mag_bin': mag_bin,\n        'expected': cand['expected_detected'],\n        'passed_no_morph': passed if len(match) > 0 else None,\n        'passed_all': passed_all if len(match) > 0 else None,\n        'status': status,\n        'details': details\n    })\n\n# Display results with comparison\nprint(f\"\\n{'Source':<20} {'Category':<25} {'Mag Bin':<10} {'Expected':<10} {'No Morph':<10} {'All Filt':<10} {'Details'}\")\nprint(\"-\" * 130)\nfor r in results_no_morph:\n    exp_str = \"Yes\" if r['expected'] else \"No\"\n    no_morph_str = \"PASSED\" if r['passed_no_morph'] else (\"FAILED\" if r['passed_no_morph'] is not None else \"N/A\")\n    all_filt_str = \"PASSED\" if r['passed_all'] else (\"FAILED\" if r['passed_all'] is not None else \"N/A\")\n    \n    # Highlight cases where removing morph filter changes result\n    highlight = \"\"\n    if r['passed_no_morph'] is not None and r['passed_all'] is not None:\n        if r['passed_no_morph'] and not r['passed_all']:\n            highlight = \"<-- MORPH FILTER REMOVED THIS\"\n    \n    print(f\"{r['source']:<20} {r['category']:<25} {r['mag_bin']:<10} {exp_str:<10} {no_morph_str:<10} {all_filt_str:<10} {r['details']:<45} {highlight}\")\n\n# Summary comparison\nn_found = sum(1 for r in results_no_morph if r['status'] != \"NOT FOUND\")\nn_passed_no_morph = sum(1 for r in results_no_morph if r['passed_no_morph'])\nn_passed_all = sum(1 for r in results_no_morph if r['passed_all'])\nn_recovered_by_removing_morph = sum(1 for r in results_no_morph if r['passed_no_morph'] and not r['passed_all'])\n\nprint(f\"\\n{'='*80}\")\nprint(f\"SUMMARY COMPARISON:\")\nprint(f\"  Candidates in mag bins:            {len(relevant_candidates)}\")\nprint(f\"  Found in data:                     {n_found}\")\nprint(f\"  Passed WITHOUT morph filter:       {n_passed_no_morph}\")\nprint(f\"  Passed WITH all filters:           {n_passed_all}\")\nprint(f\"  Recovered by removing morph filter: {n_recovered_by_removing_morph}\")\nprint(f\"{'='*80}\")\n\n# Visual comparison\nif n_found > 0:\n    fig, ax = plt.subplots(figsize=(10, 5))\n    \n    categories = ['Expected\\nto Pass', 'Passed\\n(No Morph)', 'Passed\\n(All Filters)', 'Recovered by\\nRemoving Morph']\n    n_expected = sum(1 for r in results_no_morph if r['expected'] and r['status'] != \"NOT FOUND\")\n    values = [n_expected, n_passed_no_morph, n_passed_all, n_recovered_by_removing_morph]\n    colors = ['steelblue', 'seagreen', 'coral', 'gold']\n    \n    bars = ax.bar(categories, values, color=colors, edgecolor='black')\n    ax.set_ylabel('Count')\n    ax.set_title('Known Candidate Recovery: Effect of Gaussian Morphology Filter')\n    \n    for bar, val in zip(bars, values):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, str(val), \n                ha='center', va='bottom', fontsize=12, fontweight='bold')\n    \n    ax.set_ylim(0, max(values) * 1.2 if max(values) > 0 else 1)\n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "quality-header",
   "metadata": {},
   "source": [
    "## 3. Light Curve Quality Metrics\n",
    "\n",
    "Key quality metrics that affect detection sensitivity:\n",
    "- `n_points`: Number of photometric measurements\n",
    "- `cadence_median_days`: Typical time between observations\n",
    "- `n_cameras`: Number of ASAS-SN cameras with data (multi-camera detections are more robust)\n",
    "- Time span: Total observation baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-distributions",
   "metadata": {},
   "outputs": [],
   "source": "# Calculate time span\ndf_combined['time_span_days'] = df_combined['jd_last'] - df_combined['jd_first']\ndf_bright['time_span_days'] = df_bright['jd_last'] - df_bright['jd_first']\ndf_mid['time_span_days'] = df_mid['jd_last'] - df_mid['jd_first']\ndf_faint['time_span_days'] = df_faint['jd_last'] - df_faint['jd_first']\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Define datasets for iteration\nplot_datasets = [(df_bright, '12.0-12.5', 'steelblue'), (df_mid, '12.5-13.0', 'coral'), (df_faint, '13.0-13.5', 'seagreen')]\n\n# n_points distribution\nfor df, label, color in plot_datasets:\n    axes[0, 0].hist(df['n_points'], bins=50, alpha=0.5, label=label, edgecolor='black', color=color)\naxes[0, 0].set_xlabel('Number of Points')\naxes[0, 0].set_ylabel('Count')\naxes[0, 0].set_title('Light Curve Length Distribution')\naxes[0, 0].legend()\n\n# Time span distribution\nfor df, label, color in plot_datasets:\n    axes[0, 1].hist(df['time_span_days'], bins=50, alpha=0.5, label=label, edgecolor='black', color=color)\naxes[0, 1].set_xlabel('Time Span (days)')\naxes[0, 1].set_ylabel('Count')\naxes[0, 1].set_title('Observing Baseline Distribution')\naxes[0, 1].legend()\n\n# Cadence distribution\nfor df, label, color in plot_datasets:\n    axes[0, 2].hist(df['cadence_median_days'], bins=50, alpha=0.5, label=label, edgecolor='black', color=color)\naxes[0, 2].set_xlabel('Median Cadence (days)')\naxes[0, 2].set_ylabel('Count')\naxes[0, 2].set_title('Cadence Distribution')\naxes[0, 2].legend()\n\n# n_cameras distribution\nfor i, (df, label, color) in enumerate(plot_datasets):\n    cam_counts = df['n_cameras'].value_counts().sort_index()\n    axes[1, 0].bar(cam_counts.index + i*0.25 - 0.25, cam_counts.values, width=0.25, label=label, alpha=0.7, color=color)\naxes[1, 0].set_xlabel('Number of Cameras')\naxes[1, 0].set_ylabel('Count')\naxes[1, 0].set_title('Camera Count Distribution')\naxes[1, 0].legend()\n\n# Points vs time span scatter\nfor df, label, color in plot_datasets:\n    axes[1, 1].scatter(df['n_points'], df['time_span_days'], alpha=0.2, s=5, label=label, color=color)\naxes[1, 1].set_xlabel('Number of Points')\naxes[1, 1].set_ylabel('Time Span (days)')\naxes[1, 1].set_title('Points vs Time Span')\naxes[1, 1].legend()\n\n# Cameras vs points scatter\nfor df, label, color in plot_datasets:\n    axes[1, 2].scatter(df['n_cameras'], df['n_points'], alpha=0.2, s=5, label=label, color=color)\naxes[1, 2].set_xlabel('Number of Cameras')\naxes[1, 2].set_ylabel('Number of Points')\naxes[1, 2].set_title('Cameras vs Points')\naxes[1, 2].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-stats",
   "metadata": {},
   "outputs": [],
   "source": "# Summary statistics for quality metrics\nquality_cols = ['n_points', 'time_span_days', 'cadence_median_days', 'n_cameras']\n\nprint(\"=== Quality Metrics: Bright Sample (12.0-12.5 mag) ===\")\nprint(df_bright[quality_cols].describe().round(2))\nprint()\nprint(\"=== Quality Metrics: Mid Sample (12.5-13.0 mag) ===\")\nprint(df_mid[quality_cols].describe().round(2))\nprint()\nprint(\"=== Quality Metrics: Faint Sample (13.0-13.5 mag) ===\")\nprint(df_faint[quality_cols].describe().round(2))"
  },
  {
   "cell_type": "markdown",
   "id": "bayes-header",
   "metadata": {},
   "source": [
    "## 4. Bayesian Detection Metrics\n",
    "\n",
    "The pipeline uses Bayesian hypothesis testing to detect events:\n",
    "\n",
    "- **`dip_bayes_factor`**: Global evidence ratio for dip vs baseline model (BF > 10 is \"strong evidence\")\n",
    "- **`dip_max_event_prob`**: Maximum per-point probability of being in an event\n",
    "- **`dip_max_log_bf_local`**: Maximum per-point log Bayes factor\n",
    "\n",
    "Typical thresholds:\n",
    "- Bayes Factor > 10-20 for significance\n",
    "- Event probability > 0.5-0.99 depending on strictness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bayes-distributions",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Define datasets for iteration\nplot_datasets = [(df_bright, '12.0-12.5', 'steelblue'), (df_mid, '12.5-13.0', 'coral'), (df_faint, '13.0-13.5', 'seagreen')]\n\n# Dip Bayes Factor (log scale)\nfor df, label, color in plot_datasets:\n    bf_log = np.log10(df['dip_bayes_factor'].replace(0, np.nan).dropna())\n    bf_log = bf_log[np.isfinite(bf_log)]\n    axes[0, 0].hist(bf_log, bins=50, alpha=0.5, label=label, edgecolor='black', color=color)\naxes[0, 0].axvline(np.log10(10), color='red', linestyle='--', lw=2, label='BF=10')\naxes[0, 0].axvline(np.log10(100), color='darkred', linestyle=':', lw=2, label='BF=100')\naxes[0, 0].set_xlabel('log10(Dip Bayes Factor)')\naxes[0, 0].set_ylabel('Count')\naxes[0, 0].set_title('Dip Bayes Factor Distribution')\naxes[0, 0].legend()\n\n# Jump Bayes Factor (log scale)\nfor df, label, color in plot_datasets:\n    bf_log = np.log10(df['jump_bayes_factor'].replace(0, np.nan).dropna())\n    bf_log = bf_log[np.isfinite(bf_log)]\n    axes[0, 1].hist(bf_log, bins=50, alpha=0.5, label=label, edgecolor='black', color=color)\naxes[0, 1].axvline(np.log10(10), color='red', linestyle='--', lw=2, label='BF=10')\naxes[0, 1].axvline(np.log10(100), color='darkred', linestyle=':', lw=2, label='BF=100')\naxes[0, 1].set_xlabel('log10(Jump Bayes Factor)')\naxes[0, 1].set_ylabel('Count')\naxes[0, 1].set_title('Jump Bayes Factor Distribution')\naxes[0, 1].legend()\n\n# Dip max event probability\nfor df, label, color in plot_datasets:\n    axes[0, 2].hist(df['dip_max_event_prob'], bins=50, alpha=0.5, label=label, edgecolor='black', color=color)\naxes[0, 2].axvline(0.5, color='red', linestyle='--', lw=2, label='p=0.5')\naxes[0, 2].axvline(0.99, color='darkred', linestyle=':', lw=2, label='p=0.99')\naxes[0, 2].set_xlabel('Max Dip Event Probability')\naxes[0, 2].set_ylabel('Count')\naxes[0, 2].set_title('Dip Event Probability Distribution')\naxes[0, 2].legend()\n\n# Jump max event probability\nfor df, label, color in plot_datasets:\n    axes[1, 0].hist(df['jump_max_event_prob'], bins=50, alpha=0.5, label=label, edgecolor='black', color=color)\naxes[1, 0].axvline(0.5, color='red', linestyle='--', lw=2, label='p=0.5')\naxes[1, 0].axvline(0.99, color='darkred', linestyle=':', lw=2, label='p=0.99')\naxes[1, 0].set_xlabel('Max Jump Event Probability')\naxes[1, 0].set_ylabel('Count')\naxes[1, 0].set_title('Jump Event Probability Distribution')\naxes[1, 0].legend()\n\n# Dip vs Jump Bayes Factor correlation (for detections)\nmask_detected = (df_combined['dip_significant'] | df_combined['jump_significant'])\ndf_detected = df_combined[mask_detected]\nscatter = axes[1, 1].scatter(\n    np.log10(df_detected['dip_bayes_factor'].replace(0, 1)),\n    np.log10(df_detected['jump_bayes_factor'].replace(0, 1)),\n    c=df_detected['mag_bin'].map({'12.0-12.5': 0, '12.5-13.0': 1, '13.0-13.5': 2}),\n    alpha=0.5, s=10, cmap='viridis'\n)\naxes[1, 1].axhline(np.log10(10), color='gray', linestyle='--', alpha=0.5)\naxes[1, 1].axvline(np.log10(10), color='gray', linestyle='--', alpha=0.5)\naxes[1, 1].set_xlabel('log10(Dip Bayes Factor)')\naxes[1, 1].set_ylabel('log10(Jump Bayes Factor)')\naxes[1, 1].set_title('Dip vs Jump BF (Detections Only)')\n\n# Bayes Factor vs Event Probability\naxes[1, 2].scatter(\n    np.log10(df_combined['dip_bayes_factor'].replace(0, 1)),\n    df_combined['dip_max_event_prob'],\n    alpha=0.1, s=5\n)\naxes[1, 2].axhline(0.99, color='red', linestyle='--', alpha=0.7)\naxes[1, 2].axvline(np.log10(10), color='red', linestyle='--', alpha=0.7)\naxes[1, 2].set_xlabel('log10(Dip Bayes Factor)')\naxes[1, 2].set_ylabel('Max Dip Event Probability')\naxes[1, 2].set_title('Bayes Factor vs Event Probability')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "runs-header",
   "metadata": {},
   "source": [
    "## 5. Run Analysis (Temporal Clustering)\n",
    "\n",
    "A \"run\" is a cluster of temporally-adjacent trigger points (points with high event probability).\n",
    "\n",
    "Robust detections require:\n",
    "- **`dip_run_count >= 1`**: At least one run detected\n",
    "- **`dip_max_run_points >= 2`**: At least 2 points in the best run (not isolated noise)\n",
    "- **`dip_max_run_cameras >= 2`**: Multi-camera confirmation (rules out instrumental artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-distributions",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Define datasets for iteration\nplot_datasets = [(df_bright, '12.0-12.5', 'steelblue'), (df_mid, '12.5-13.0', 'coral'), (df_faint, '13.0-13.5', 'seagreen')]\n\n# Dip run count\nmax_runs = 20\nfor df, label, color in plot_datasets:\n    run_counts = df['dip_run_count'].clip(upper=max_runs)\n    axes[0, 0].hist(run_counts, bins=range(0, max_runs+2), alpha=0.5, label=label, edgecolor='black', color=color)\naxes[0, 0].set_xlabel('Dip Run Count')\naxes[0, 0].set_ylabel('Count')\naxes[0, 0].set_title(f'Number of Dip Runs (capped at {max_runs})')\naxes[0, 0].set_yscale('log')\naxes[0, 0].legend()\n\n# Jump run count\nfor df, label, color in plot_datasets:\n    run_counts = df['jump_run_count'].clip(upper=max_runs)\n    axes[0, 1].hist(run_counts, bins=range(0, max_runs+2), alpha=0.5, label=label, edgecolor='black', color=color)\naxes[0, 1].set_xlabel('Jump Run Count')\naxes[0, 1].set_ylabel('Count')\naxes[0, 1].set_title(f'Number of Jump Runs (capped at {max_runs})')\naxes[0, 1].set_yscale('log')\naxes[0, 1].legend()\n\n# Max run points (dip)\nfor df, label, color in plot_datasets:\n    axes[0, 2].hist(df['dip_max_run_points'].clip(upper=30), bins=30, alpha=0.5, label=label, edgecolor='black', color=color)\naxes[0, 2].axvline(2, color='red', linestyle='--', lw=2, label='Threshold=2')\naxes[0, 2].set_xlabel('Max Dip Run Points')\naxes[0, 2].set_ylabel('Count')\naxes[0, 2].set_title('Points in Best Dip Run')\naxes[0, 2].set_yscale('log')\naxes[0, 2].legend()\n\n# Max run cameras (dip)\nfor df, label, color in plot_datasets:\n    axes[1, 0].hist(df['dip_max_run_cameras'], bins=range(0, 15), alpha=0.5, label=label, edgecolor='black', color=color)\naxes[1, 0].axvline(2, color='red', linestyle='--', lw=2, label='Threshold=2')\naxes[1, 0].set_xlabel('Max Dip Run Cameras')\naxes[1, 0].set_ylabel('Count')\naxes[1, 0].set_title('Cameras in Best Dip Run')\naxes[1, 0].set_yscale('log')\naxes[1, 0].legend()\n\n# Run duration (for those with runs)\nfor df, label, color in plot_datasets:\n    duration = df['dip_max_run_duration'].dropna()\n    if len(duration) > 0:\n        axes[1, 1].hist(duration.clip(upper=100), bins=50, alpha=0.5, label=f'{label} (n={len(duration)})', edgecolor='black', color=color)\naxes[1, 1].set_xlabel('Max Dip Run Duration (days)')\naxes[1, 1].set_ylabel('Count')\naxes[1, 1].set_title('Duration of Best Dip Run')\naxes[1, 1].legend()\n\n# Run points vs cameras correlation\ndf_with_runs = df_combined[df_combined['dip_run_count'] > 0]\nscatter = axes[1, 2].scatter(\n    df_with_runs['dip_max_run_points'],\n    df_with_runs['dip_max_run_cameras'],\n    c=df_with_runs['mag_bin'].map({'12.0-12.5': 0, '12.5-13.0': 1, '13.0-13.5': 2}),\n    alpha=0.5, s=10, cmap='viridis'\n)\naxes[1, 2].axhline(2, color='red', linestyle='--', alpha=0.7)\naxes[1, 2].axvline(2, color='red', linestyle='--', alpha=0.7)\naxes[1, 2].set_xlabel('Max Run Points')\naxes[1, 2].set_ylabel('Max Run Cameras')\naxes[1, 2].set_title('Run Points vs Cameras (multi-camera = robust)')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-stats",
   "metadata": {},
   "outputs": [],
   "source": "# Run statistics summary\nprint(\"=== Run Statistics Summary ===\")\nfor df, label in [(df_bright, 'Bright (12.0-12.5)'), (df_mid, 'Mid (12.5-13.0)'), (df_faint, 'Faint (13.0-13.5)')]:\n    n = len(df)\n    has_dip_run = (df['dip_run_count'] >= 1).sum()\n    has_jump_run = (df['jump_run_count'] >= 1).sum()\n    multi_cam_dip = (df['dip_max_run_cameras'] >= 2).sum()\n    multi_point_dip = (df['dip_max_run_points'] >= 2).sum()\n    robust_dip = ((df['dip_run_count'] >= 1) & (df['dip_max_run_cameras'] >= 2) & (df['dip_max_run_points'] >= 2)).sum()\n    \n    print(f\"\\n{label}:\")\n    print(f\"  Has dip run (count >= 1):       {has_dip_run:>7,} ({has_dip_run/n*100:5.2f}%)\")\n    print(f\"  Has jump run (count >= 1):      {has_jump_run:>7,} ({has_jump_run/n*100:5.2f}%)\")\n    print(f\"  Multi-camera dip (cams >= 2):   {multi_cam_dip:>7,} ({multi_cam_dip/n*100:5.2f}%)\")\n    print(f\"  Multi-point dip (pts >= 2):     {multi_point_dip:>7,} ({multi_point_dip/n*100:5.2f}%)\")\n    print(f\"  Robust dip (all criteria):      {robust_dip:>7,} ({robust_dip/n*100:5.2f}%)\")"
  },
  {
   "cell_type": "markdown",
   "id": "morph-header",
   "metadata": {},
   "source": [
    "## 6. Morphology Analysis\n",
    "\n",
    "After detecting runs, the pipeline fits morphological models to classify the event shape:\n",
    "\n",
    "- **`gaussian`**: Symmetric dip - could be disk occultation or eclipsing binary\n",
    "- **`skew_gaussian`**: Asymmetric dip - characteristic of disk occultations (slower ingress/egress)\n",
    "- **`paczynski`**: Microlensing curve (for jumps)\n",
    "- **`none`**: No good fit found\n",
    "\n",
    "**`dip_best_delta_bic`**: BIC improvement over baseline. Values > 10 indicate strong morphological fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "morph-counts",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Dip morphology counts\nfor i, (df, label) in enumerate([(df_bright, '12.0-12.5'), (df_mid, '12.5-13.0'), (df_faint, '13.0-13.5')]):\n    morph_counts = df['dip_best_morph'].value_counts()\n    colors = ['lightgray' if m == 'none' else plt.cm.Set2(j) for j, m in enumerate(morph_counts.index)]\n    bars = axes[0, i].bar(range(len(morph_counts)), morph_counts.values, color=colors, edgecolor='black')\n    axes[0, i].set_xticks(range(len(morph_counts)))\n    axes[0, i].set_xticklabels(morph_counts.index, rotation=45, ha='right')\n    axes[0, i].set_ylabel('Count')\n    axes[0, i].set_title(f'Dip Morphology: {label} mag')\n    # Add count labels\n    for bar, count in zip(bars, morph_counts.values):\n        axes[0, i].text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{count:,}', \n                        ha='center', va='bottom', fontsize=8)\n\n# Jump morphology counts\nfor i, (df, label) in enumerate([(df_bright, '12.0-12.5'), (df_mid, '12.5-13.0'), (df_faint, '13.0-13.5')]):\n    morph_counts = df['jump_best_morph'].value_counts()\n    colors = ['lightgray' if m == 'none' else plt.cm.Set2(j) for j, m in enumerate(morph_counts.index)]\n    bars = axes[1, i].bar(range(len(morph_counts)), morph_counts.values, color=colors, edgecolor='black')\n    axes[1, i].set_xticks(range(len(morph_counts)))\n    axes[1, i].set_xticklabels(morph_counts.index, rotation=45, ha='right')\n    axes[1, i].set_ylabel('Count')\n    axes[1, i].set_title(f'Jump Morphology: {label} mag')\n    for bar, count in zip(bars, morph_counts.values):\n        axes[1, i].text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{count:,}', \n                        ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "morph-table",
   "metadata": {},
   "outputs": [],
   "source": "# Morphology breakdown table\nprint(\"=== Dip Morphology Breakdown ===\")\nfor df, label in [(df_bright, 'Bright (12.0-12.5)'), (df_mid, 'Mid (12.5-13.0)'), (df_faint, 'Faint (13.0-13.5)')]:\n    print(f\"\\n{label}:\")\n    morph_counts = df['dip_best_morph'].value_counts()\n    for morph, count in morph_counts.items():\n        pct = count / len(df) * 100\n        marker = '*' if morph in ['gaussian', 'skew_gaussian'] else ''\n        print(f\"  {morph:15s}: {count:>8,} ({pct:5.2f}%) {marker}\")\n\nprint(\"\\n* Indicates morphologies consistent with disk occultation dips\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "morph-bic",
   "metadata": {},
   "outputs": [],
   "source": "# Delta BIC distribution for morphological fits\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Define datasets for iteration\nplot_datasets = [(df_bright, '12.0-12.5', 'steelblue'), (df_mid, '12.5-13.0', 'coral'), (df_faint, '13.0-13.5', 'seagreen')]\n\n# Filter to non-zero BIC (has morphology fit)\nfor df, label, color in plot_datasets:\n    bic = df[df['dip_best_delta_bic'] > 0]['dip_best_delta_bic']\n    axes[0].hist(np.log10(bic + 1), bins=50, alpha=0.5, label=f'{label} (n={len(bic)})', edgecolor='black', color=color)\naxes[0].axvline(np.log10(10), color='red', linestyle='--', lw=2, label='BIC=10')\naxes[0].axvline(np.log10(100), color='darkred', linestyle=':', lw=2, label='BIC=100')\naxes[0].set_xlabel('log10(Dip Delta BIC + 1)')\naxes[0].set_ylabel('Count')\naxes[0].set_title('Dip Morphology Fit Quality (Delta BIC)')\naxes[0].legend()\n\nfor df, label, color in plot_datasets:\n    bic = df[df['jump_best_delta_bic'] > 0]['jump_best_delta_bic']\n    axes[1].hist(np.log10(bic + 1), bins=50, alpha=0.5, label=f'{label} (n={len(bic)})', edgecolor='black', color=color)\naxes[1].axvline(np.log10(10), color='red', linestyle='--', lw=2, label='BIC=10')\naxes[1].axvline(np.log10(100), color='darkred', linestyle=':', lw=2, label='BIC=100')\naxes[1].set_xlabel('log10(Jump Delta BIC + 1)')\naxes[1].set_ylabel('Count')\naxes[1].set_title('Jump Morphology Fit Quality (Delta BIC)')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "morph-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morphology width parameter (event duration proxy)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for df, label in [(df_bright, '12.0-12.5'), (df_faint, '12.5-13.0')]:\n",
    "    width = df[(df['dip_best_morph'] != 'none') & (df['dip_best_width_param'] > 0)]['dip_best_width_param']\n",
    "    width = width[width < 100]  # clip outliers\n",
    "    if len(width) > 0:\n",
    "        axes[0].hist(width, bins=50, alpha=0.6, label=f'{label} (n={len(width)})', edgecolor='black')\n",
    "axes[0].set_xlabel('Dip Width Parameter (days)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Dip Event Duration (Width Parameter)')\n",
    "axes[0].legend()\n",
    "\n",
    "for df, label in [(df_bright, '12.0-12.5'), (df_faint, '12.5-13.0')]:\n",
    "    width = df[(df['jump_best_morph'] != 'none') & (df['jump_best_width_param'] > 0)]['jump_best_width_param']\n",
    "    width = width[width < 100]\n",
    "    if len(width) > 0:\n",
    "        axes[1].hist(width, bins=50, alpha=0.6, label=f'{label} (n={len(width)})', edgecolor='black')\n",
    "axes[1].set_xlabel('Jump Width Parameter (days)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Jump Event Duration (Width Parameter)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dipper-header",
   "metadata": {},
   "source": [
    "## 7. Dipper Score Analysis\n",
    "\n",
    "The dipper score is a composite metric based on Tzanidakis+2025:\n",
    "- Measures ingress vs egress asymmetry\n",
    "- Higher scores indicate more asymmetric (dipper-like) events\n",
    "- Eclipsing binaries are typically symmetric (low score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dipper-distributions",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Dipper score distribution (filter out inf/nan)\n",
    "for df, label in [(df_bright, '12.0-12.5'), (df_faint, '12.5-13.0')]:\n",
    "    score = df['dipper_score'][np.isfinite(df['dipper_score'])]\n",
    "    score = score[score > 0]  # only positive scores\n",
    "    if len(score) > 0:\n",
    "        axes[0, 0].hist(score, bins=50, alpha=0.6, label=f'{label} (n={len(score)})', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Dipper Score')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_title('Dipper Score Distribution (score > 0)')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Number of dips per light curve\n",
    "for df, label in [(df_bright, '12.0-12.5'), (df_faint, '12.5-13.0')]:\n",
    "    n_dips = df['dipper_n_dips'].clip(upper=50)\n",
    "    axes[0, 1].hist(n_dips, bins=range(0, 52), alpha=0.6, label=label, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Number of Dips')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Dips per Light Curve')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Valid dips per light curve\n",
    "for df, label in [(df_bright, '12.0-12.5'), (df_faint, '12.5-13.0')]:\n",
    "    n_valid = df['dipper_n_valid_dips'].clip(upper=20)\n",
    "    axes[1, 0].hist(n_valid, bins=range(0, 22), alpha=0.6, label=label, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Number of Valid Dips')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Valid Dips per Light Curve')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Dipper score vs Bayes Factor\n",
    "df_valid = df_combined[(df_combined['dipper_score'] > 0) & np.isfinite(df_combined['dipper_score'])]\n",
    "sc = axes[1, 1].scatter(\n",
    "    np.log10(df_valid['dip_bayes_factor'].replace(0, 1)),\n",
    "    df_valid['dipper_score'],\n",
    "    c=df_valid['mag_bin'].map({'12.0-12.5': 0, '12.5-13.0': 1}),\n",
    "    alpha=0.5, s=10, cmap='coolwarm'\n",
    ")\n",
    "axes[1, 1].set_xlabel('log10(Dip Bayes Factor)')\n",
    "axes[1, 1].set_ylabel('Dipper Score')\n",
    "axes[1, 1].set_title('Dipper Score vs Bayes Factor')\n",
    "plt.colorbar(sc, ax=axes[1, 1], label='Mag bin')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-header",
   "metadata": {},
   "source": [
    "## 8. Baseline Magnitude Analysis\n",
    "\n",
    "The fitted baseline magnitude and event depths help characterize the detection sensitivity across magnitude bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if baseline_mag column exists in both\n",
    "has_baseline = 'baseline_mag' in df_combined.columns\n",
    "\n",
    "if has_baseline:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Baseline magnitude distribution\n",
    "    for df, label in [(df_bright, '12.0-12.5'), (df_faint, '12.5-13.0')]:\n",
    "        if 'baseline_mag' in df.columns:\n",
    "            axes[0, 0].hist(df['baseline_mag'].dropna(), bins=50, alpha=0.6, label=label, edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Baseline Magnitude')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].set_title('Baseline Magnitude Distribution')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Event depth (dip_best_mag_event - baseline_mag)\n",
    "    for df, label in [(df_bright, '12.0-12.5'), (df_faint, '12.5-13.0')]:\n",
    "        if 'baseline_mag' in df.columns and 'dip_best_mag_event' in df.columns:\n",
    "            depth = df['dip_best_mag_event'] - df['baseline_mag']\n",
    "            depth = depth[(depth > 0) & (depth < 2)]  # positive dip depths up to 2 mag\n",
    "            if len(depth) > 0:\n",
    "                axes[0, 1].hist(depth, bins=50, alpha=0.6, label=f'{label} (n={len(depth)})', edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Dip Depth (mag)')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].set_title('Event Depth Distribution')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Baseline vs Bayes Factor\n",
    "    if 'baseline_mag' in df_combined.columns:\n",
    "        axes[1, 0].scatter(\n",
    "            df_combined['baseline_mag'],\n",
    "            np.log10(df_combined['dip_bayes_factor'].replace(0, 1)),\n",
    "            c=df_combined['mag_bin'].map({'12.0-12.5': 0, '12.5-13.0': 1}),\n",
    "            alpha=0.1, s=5, cmap='coolwarm'\n",
    "        )\n",
    "    axes[1, 0].set_xlabel('Baseline Magnitude')\n",
    "    axes[1, 0].set_ylabel('log10(Dip Bayes Factor)')\n",
    "    axes[1, 0].set_title('Baseline Magnitude vs Detection Strength')\n",
    "    \n",
    "    # Detection rate by magnitude\n",
    "    if 'baseline_mag' in df_combined.columns:\n",
    "        mag_bins = np.arange(11.5, 13.5, 0.1)\n",
    "        for df, label, color in [(df_bright, '12.0-12.5', 'steelblue'), (df_faint, '12.5-13.0', 'coral')]:\n",
    "            if 'baseline_mag' in df.columns:\n",
    "                df_temp = df.copy()\n",
    "                df_temp['mag_binned'] = pd.cut(df_temp['baseline_mag'], bins=mag_bins)\n",
    "                det_rate = df_temp.groupby('mag_binned')['dip_significant'].mean() * 100\n",
    "                centers = [(b.left + b.right) / 2 for b in det_rate.index]\n",
    "                axes[1, 1].plot(centers, det_rate.values, 'o-', label=label, color=color, alpha=0.7)\n",
    "        axes[1, 1].set_xlabel('Baseline Magnitude')\n",
    "        axes[1, 1].set_ylabel('Dip Detection Rate (%)')\n",
    "        axes[1, 1].set_title('Detection Rate vs Magnitude')\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"baseline_mag column not found in one or both datasets\")\n",
    "    print(f\"Bright columns: {'baseline_mag' in df_bright.columns}\")\n",
    "    print(f\"Faint columns: {'baseline_mag' in df_faint.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filter-header",
   "metadata": {},
   "source": [
    "## 9. Filter Performance Analysis\n",
    "\n",
    "Analysis of how different filtering thresholds affect the candidate pool.\n",
    "\n",
    "### Standard Thresholds (from MALCA pipeline):\n",
    "- Bayes Factor > 10-20\n",
    "- Event probability > 0.5-0.99\n",
    "- Run count >= 1\n",
    "- Run points >= 2\n",
    "- Run cameras >= 2 (multi-camera confirmation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threshold-analysis",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Filter Threshold Analysis ===\")\nprint()\n\nfor df, label in [(df_bright, 'Bright (12.0-12.5)'), (df_mid, 'Mid (12.5-13.0)'), (df_faint, 'Faint (13.0-13.5)')]:\n    print(f\"\\n{'='*60}\")\n    print(f\"{label}: {len(df):,} total light curves\")\n    print(f\"{'='*60}\")\n    \n    print(\"\\nBayes Factor Thresholds (dip OR jump):\")\n    for thresh in [1, 3, 10, 30, 100, 1000]:\n        n = ((df['dip_bayes_factor'] > thresh) | (df['jump_bayes_factor'] > thresh)).sum()\n        print(f\"  BF > {thresh:>5}: {n:>8,} ({n/len(df)*100:5.2f}%)\")\n    \n    print(\"\\nEvent Probability Thresholds (dip OR jump):\")\n    for thresh in [0.5, 0.9, 0.99, 0.999, 0.9999]:\n        n = ((df['dip_max_event_prob'] > thresh) | (df['jump_max_event_prob'] > thresh)).sum()\n        print(f\"  P > {thresh:.4f}: {n:>8,} ({n/len(df)*100:5.2f}%)\")\n    \n    print(\"\\nRun Requirements (dip OR jump):\")\n    for min_runs in [1, 2, 3, 5]:\n        n = ((df['dip_run_count'] >= min_runs) | (df['jump_run_count'] >= min_runs)).sum()\n        print(f\"  Runs >= {min_runs}: {n:>8,} ({n/len(df)*100:5.2f}%)\")\n    \n    print(\"\\nMulti-Camera Requirement (dip OR jump):\")\n    for min_cams in [1, 2, 3, 4]:\n        n = ((df['dip_max_run_cameras'] >= min_cams) | (df['jump_max_run_cameras'] >= min_cams)).sum()\n        print(f\"  Run cams >= {min_cams}: {n:>8,} ({n/len(df)*100:5.2f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-filters",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Combined Filtering Scenarios (Progressive) ===\")\nprint()\n\nfor df, label in [(df_bright, 'Bright (12.0-12.5)'), (df_mid, 'Mid (12.5-13.0)'), (df_faint, 'Faint (13.0-13.5)'), (df_combined, 'Combined')]:\n    print(f\"\\n{label}: {len(df):,} total\")\n    print(\"-\" * 50)\n    \n    # Progressive filtering for DIPS specifically\n    n = len(df)\n    \n    # Start with Bayes Factor threshold\n    mask1 = df['dip_bayes_factor'] > 10\n    \n    # Add event probability\n    mask2 = mask1 & (df['dip_max_event_prob'] > 0.5)\n    \n    # Add run requirement\n    mask3 = mask2 & (df['dip_run_count'] >= 1)\n    \n    # Add multi-point runs\n    mask4 = mask3 & (df['dip_max_run_points'] >= 2)\n    \n    # Add multi-camera runs (CRITICAL for ruling out artifacts)\n    mask5 = mask4 & (df['dip_max_run_cameras'] >= 2)\n    \n    # Add morphology requirement\n    mask6 = mask5 & (df['dip_best_morph'].isin(['gaussian', 'skew_gaussian']))\n    \n    # Add BIC threshold\n    mask7 = mask6 & (df['dip_best_delta_bic'] > 10)\n    \n    scenarios = [\n        (\"1. BF > 10\", mask1),\n        (\"2. + Prob > 0.5\", mask2),\n        (\"3. + Run count >= 1\", mask3),\n        (\"4. + Run points >= 2\", mask4),\n        (\"5. + Run cameras >= 2\", mask5),\n        (\"6. + Morph = gauss/skew\", mask6),\n        (\"7. + Delta BIC > 10\", mask7),\n    ]\n    \n    for desc, mask in scenarios:\n        n_pass = mask.sum()\n        print(f\"  {desc:25s}: {n_pass:>7,} ({n_pass/n*100:5.2f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter-visualization",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize filter cascade\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\nfor ax, (df, label) in zip(axes, [(df_bright, '12.0-12.5'), (df_mid, '12.5-13.0'), (df_faint, '13.0-13.5')]):\n    n = len(df)\n    \n    filters = [\n        ('All', n),\n        ('BF>10', (df['dip_bayes_factor'] > 10).sum()),\n        ('+P>0.5', ((df['dip_bayes_factor'] > 10) & (df['dip_max_event_prob'] > 0.5)).sum()),\n        ('+Runs>=1', ((df['dip_bayes_factor'] > 10) & (df['dip_run_count'] >= 1)).sum()),\n        ('+Pts>=2', ((df['dip_bayes_factor'] > 10) & (df['dip_run_count'] >= 1) & (df['dip_max_run_points'] >= 2)).sum()),\n        ('+Cams>=2', ((df['dip_bayes_factor'] > 10) & (df['dip_run_count'] >= 1) & (df['dip_max_run_points'] >= 2) & (df['dip_max_run_cameras'] >= 2)).sum()),\n    ]\n    \n    labels, counts = zip(*filters)\n    bars = ax.bar(range(len(labels)), counts, color=plt.cm.Blues(np.linspace(0.3, 0.9, len(labels))), edgecolor='black')\n    ax.set_xticks(range(len(labels)))\n    ax.set_xticklabels(labels, rotation=45, ha='right')\n    ax.set_ylabel('Count')\n    ax.set_title(f'Filter Cascade: {label} mag')\n    ax.set_yscale('log')\n    \n    # Add count labels\n    for bar, count in zip(bars, counts):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{count:,}', \n                ha='center', va='bottom', fontsize=9, rotation=0)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "correlation-header",
   "metadata": {},
   "source": [
    "## 10. Correlation Analysis\n",
    "\n",
    "Understanding relationships between detection metrics helps identify:\n",
    "- Redundant filters\n",
    "- Key predictors of real events\n",
    "- Potential biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key metrics for correlation analysis\n",
    "corr_cols = [\n",
    "    'n_points', 'cadence_median_days', 'n_cameras',\n",
    "    'dip_bayes_factor', 'dip_max_event_prob', \n",
    "    'dip_run_count', 'dip_max_run_points', 'dip_max_run_cameras',\n",
    "    'dip_best_delta_bic',\n",
    "    'jump_bayes_factor', 'jump_max_event_prob',\n",
    "    'dipper_score', 'dipper_n_valid_dips'\n",
    "]\n",
    "\n",
    "# Filter to columns that exist in both datasets\n",
    "corr_cols = [c for c in corr_cols if c in df_combined.columns]\n",
    "\n",
    "# Log-transform some columns for better correlation visualization\n",
    "df_corr = df_combined[corr_cols].copy()\n",
    "for col in ['dip_bayes_factor', 'jump_bayes_factor', 'dip_best_delta_bic']:\n",
    "    if col in df_corr.columns:\n",
    "        df_corr[col] = np.log10(df_corr[col].replace(0, np.nan) + 1)\n",
    "\n",
    "# Handle infinite values in dipper_score\n",
    "if 'dipper_score' in df_corr.columns:\n",
    "    df_corr['dipper_score'] = df_corr['dipper_score'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "corr = df_corr.corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='RdBu_r', center=0, \n",
    "            square=True, mask=mask, vmin=-1, vmax=1,\n",
    "            cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Correlation Matrix of Key Detection Metrics\\n(log-transformed where appropriate)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "candidates-header",
   "metadata": {},
   "source": [
    "## 11. Strong Candidate Identification\n",
    "\n",
    "Identifying the most promising disk occultation candidates using combined filtering criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-candidates",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_candidates(df, label, \n",
    "                        bf_thresh=10, \n",
    "                        prob_thresh=0.5, \n",
    "                        min_runs=1, \n",
    "                        min_run_pts=2, \n",
    "                        min_run_cams=2,\n",
    "                        require_morph=True,\n",
    "                        min_bic=10):\n",
    "    \"\"\"\n",
    "    Identify strong dip candidates based on multiple criteria.\n",
    "    Returns DataFrame of candidates and summary statistics.\n",
    "    \"\"\"\n",
    "    mask = (\n",
    "        (df['dip_bayes_factor'] > bf_thresh) &\n",
    "        (df['dip_max_event_prob'] > prob_thresh) &\n",
    "        (df['dip_run_count'] >= min_runs) &\n",
    "        (df['dip_max_run_points'] >= min_run_pts) &\n",
    "        (df['dip_max_run_cameras'] >= min_run_cams)\n",
    "    )\n",
    "    \n",
    "    if require_morph:\n",
    "        mask = mask & (df['dip_best_morph'].isin(['gaussian', 'skew_gaussian']))\n",
    "        mask = mask & (df['dip_best_delta_bic'] > min_bic)\n",
    "    \n",
    "    candidates = df[mask].copy()\n",
    "    \n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  Total light curves: {len(df):,}\")\n",
    "    print(f\"  Strong candidates:  {len(candidates):,} ({len(candidates)/len(df)*100:.3f}%)\")\n",
    "    \n",
    "    if len(candidates) > 0:\n",
    "        print(f\"\\n  Morphology breakdown:\")\n",
    "        for morph, count in candidates['dip_best_morph'].value_counts().items():\n",
    "            print(f\"    {morph}: {count}\")\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "# Identify candidates in each sample\n",
    "candidates_bright = identify_candidates(df_bright, \"Bright Sample (12.0-12.5 mag)\")\n",
    "candidates_faint = identify_candidates(df_faint, \"Faint Sample (12.5-13.0 mag)\")\n",
    "candidates_all = identify_candidates(df_combined, \"Combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-candidates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top candidates by Bayes Factor\n",
    "display_cols = [\n",
    "    'path', 'mag_bin',\n",
    "    'dip_bayes_factor', 'dip_max_event_prob',\n",
    "    'dip_run_count', 'dip_max_run_points', 'dip_max_run_cameras',\n",
    "    'dip_best_morph', 'dip_best_delta_bic', 'dip_best_width_param',\n",
    "    'n_points', 'n_cameras', 'dipper_score'\n",
    "]\n",
    "\n",
    "# Filter to columns that exist\n",
    "display_cols = [c for c in display_cols if c in candidates_all.columns]\n",
    "\n",
    "if len(candidates_all) > 0:\n",
    "    print(\"=== Top 20 Candidates by Bayes Factor ===\")\n",
    "    top20 = candidates_all.nlargest(20, 'dip_bayes_factor')[display_cols]\n",
    "    print(top20.to_string())\n",
    "else:\n",
    "    print(\"No candidates found with the specified criteria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "candidate-properties",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate property distributions\n",
    "if len(candidates_all) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Bayes Factor distribution\n",
    "    axes[0, 0].hist(np.log10(candidates_all['dip_bayes_factor']), bins=30, edgecolor='black', color='steelblue')\n",
    "    axes[0, 0].set_xlabel('log10(Dip Bayes Factor)')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].set_title(f'Candidate BF Distribution (n={len(candidates_all)})')\n",
    "    \n",
    "    # Run duration\n",
    "    duration = candidates_all['dip_max_run_duration'].dropna()\n",
    "    if len(duration) > 0:\n",
    "        axes[0, 1].hist(duration.clip(upper=100), bins=30, edgecolor='black', color='coral')\n",
    "    axes[0, 1].set_xlabel('Max Run Duration (days)')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].set_title('Event Duration')\n",
    "    \n",
    "    # Width parameter\n",
    "    width = candidates_all['dip_best_width_param'].dropna()\n",
    "    width = width[(width > 0) & (width < 50)]\n",
    "    if len(width) > 0:\n",
    "        axes[0, 2].hist(width, bins=30, edgecolor='black', color='seagreen')\n",
    "    axes[0, 2].set_xlabel('Width Parameter (days)')\n",
    "    axes[0, 2].set_ylabel('Count')\n",
    "    axes[0, 2].set_title('Morphology Width')\n",
    "    \n",
    "    # Cameras in run\n",
    "    axes[1, 0].hist(candidates_all['dip_max_run_cameras'], bins=range(2, 15), edgecolor='black', color='purple', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Cameras in Best Run')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].set_title('Multi-Camera Confirmation')\n",
    "    \n",
    "    # Delta BIC\n",
    "    bic = candidates_all['dip_best_delta_bic']\n",
    "    axes[1, 1].hist(np.log10(bic + 1), bins=30, edgecolor='black', color='orange')\n",
    "    axes[1, 1].set_xlabel('log10(Delta BIC + 1)')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    axes[1, 1].set_title('Morphology Fit Quality')\n",
    "    \n",
    "    # By magnitude bin\n",
    "    mag_counts = candidates_all['mag_bin'].value_counts()\n",
    "    bars = axes[1, 2].bar(range(len(mag_counts)), mag_counts.values, \n",
    "                          tick_label=mag_counts.index, color=['steelblue', 'coral'], edgecolor='black')\n",
    "    axes[1, 2].set_ylabel('Count')\n",
    "    axes[1, 2].set_title('Candidates by Magnitude Bin')\n",
    "    for bar, count in zip(bars, mag_counts.values):\n",
    "        axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{count}', \n",
    "                        ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No candidates to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## 12. Magnitude Bin Comparison Summary\n",
    "\n",
    "Systematic comparison of detection characteristics between the two magnitude bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_stats(df, label):\n",
    "    \"\"\"Compute comprehensive summary statistics for a dataset.\"\"\"\n",
    "    n = len(df)\n",
    "    \n",
    "    stats = {\n",
    "        'Label': label,\n",
    "        'Total LCs': n,\n",
    "        'Dip detections': df['dip_significant'].sum(),\n",
    "        'Dip rate (%)': df['dip_significant'].sum() / n * 100,\n",
    "        'Jump detections': df['jump_significant'].sum(),\n",
    "        'Jump rate (%)': df['jump_significant'].sum() / n * 100,\n",
    "        'Median n_points': df['n_points'].median(),\n",
    "        'Median n_cameras': df['n_cameras'].median(),\n",
    "        'Median cadence (days)': df['cadence_median_days'].median(),\n",
    "        'Has dip run (%)': (df['dip_run_count'] >= 1).sum() / n * 100,\n",
    "        'Multi-cam dip run (%)': (df['dip_max_run_cameras'] >= 2).sum() / n * 100,\n",
    "        'Gaussian dips': (df['dip_best_morph'] == 'gaussian').sum(),\n",
    "        'Skew-Gaussian dips': (df['dip_best_morph'] == 'skew_gaussian').sum(),\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Compute stats for all samples\n",
    "stats_bright_full = compute_summary_stats(df_bright, '12.0-12.5 mag')\n",
    "stats_mid_full = compute_summary_stats(df_mid, '12.5-13.0 mag')\n",
    "stats_faint_full = compute_summary_stats(df_faint, '13.0-13.5 mag')\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame([stats_bright_full, stats_mid_full, stats_faint_full])\n",
    "comparison_df = comparison_df.set_index('Label').T\n",
    "\n",
    "print(\"=== Magnitude Bin Comparison ===\")\n",
    "print(comparison_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of key metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Detection rates\n",
    "rates_bright = [stats_bright_full['Dip rate (%)'], stats_bright_full['Jump rate (%)']]\n",
    "rates_mid = [stats_mid_full['Dip rate (%)'], stats_mid_full['Jump rate (%)']]\n",
    "rates_faint = [stats_faint_full['Dip rate (%)'], stats_faint_full['Jump rate (%)']]\n",
    "\n",
    "x = np.arange(2)\n",
    "width = 0.25\n",
    "axes[0, 0].bar(x - width, rates_bright, width, label='12.0-12.5', color='steelblue')\n",
    "axes[0, 0].bar(x, rates_mid, width, label='12.5-13.0', color='coral')\n",
    "axes[0, 0].bar(x + width, rates_faint, width, label='13.0-13.5', color='seagreen')\n",
    "axes[0, 0].set_ylabel('Detection Rate (%)')\n",
    "axes[0, 0].set_title('Detection Rates')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(['Dips', 'Jumps'])\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Morphology comparison\n",
    "morph_bright = [stats_bright_full['Gaussian dips'], stats_bright_full['Skew-Gaussian dips']]\n",
    "morph_mid = [stats_mid_full['Gaussian dips'], stats_mid_full['Skew-Gaussian dips']]\n",
    "morph_faint = [stats_faint_full['Gaussian dips'], stats_faint_full['Skew-Gaussian dips']]\n",
    "\n",
    "axes[0, 1].bar(x - width, morph_bright, width, label='12.0-12.5', color='steelblue')\n",
    "axes[0, 1].bar(x, morph_mid, width, label='12.5-13.0', color='coral')\n",
    "axes[0, 1].bar(x + width, morph_faint, width, label='13.0-13.5', color='seagreen')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Dip Morphology Counts')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(['Gaussian', 'Skew-Gaussian'])\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Quality comparison (boxplots)\n",
    "data_points = [df_bright['n_points'], df_mid['n_points'], df_faint['n_points']]\n",
    "labels = ['12.0-12.5', '12.5-13.0', '13.0-13.5']\n",
    "colors = ['steelblue', 'coral', 'seagreen']\n",
    "\n",
    "bp1 = axes[1, 0].boxplot(data_points, labels=labels, patch_artist=True)\n",
    "for patch, color in zip(bp1['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "axes[1, 0].set_ylabel('Number of Points')\n",
    "axes[1, 0].set_title('Light Curve Length Comparison')\n",
    "\n",
    "# Bayes Factor comparison (for detections)\n",
    "bf_bright = df_bright[df_bright['dip_significant']]['dip_bayes_factor']\n",
    "bf_mid = df_mid[df_mid['dip_significant']]['dip_bayes_factor']\n",
    "bf_faint = df_faint[df_faint['dip_significant']]['dip_bayes_factor']\n",
    "\n",
    "bf_data = []\n",
    "bf_labels = []\n",
    "bf_colors = []\n",
    "\n",
    "if len(bf_bright) > 0: bf_data.append(np.log10(bf_bright)); bf_labels.append('12.0-12.5'); bf_colors.append('steelblue')\n",
    "if len(bf_mid) > 0: bf_data.append(np.log10(bf_mid)); bf_labels.append('12.5-13.0'); bf_colors.append('coral')\n",
    "if len(bf_faint) > 0: bf_data.append(np.log10(bf_faint)); bf_labels.append('13.0-13.5'); bf_colors.append('seagreen')\n",
    "\n",
    "if bf_data:\n",
    "    bp2 = axes[1, 1].boxplot(bf_data, labels=bf_labels, patch_artist=True)\n",
    "    for patch, color in zip(bp2['boxes'], bf_colors):\n",
    "        patch.set_facecolor(color)\n",
    "    axes[1, 1].set_ylabel('log10(Bayes Factor)')\n",
    "    axes[1, 1].set_title('Detection Strength (Dip Detections Only)')\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Insufficient detections', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 13. Export Strong Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-candidates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export candidates with relaxed and strict criteria\n",
    "\n",
    "# Relaxed: multi-camera dip runs\n",
    "candidates_relaxed = df_combined[\n",
    "    (df_combined['dip_run_count'] >= 1) &\n",
    "    (df_combined['dip_max_run_cameras'] >= 2) &\n",
    "    (df_combined['dip_max_run_points'] >= 2)\n",
    "].copy()\n",
    "\n",
    "# Strict: add morphology and BIC requirements\n",
    "candidates_strict = df_combined[\n",
    "    (df_combined['dip_bayes_factor'] > 10) &\n",
    "    (df_combined['dip_run_count'] >= 1) &\n",
    "    (df_combined['dip_max_run_cameras'] >= 2) &\n",
    "    (df_combined['dip_max_run_points'] >= 2) &\n",
    "    (df_combined['dip_best_morph'].isin(['gaussian', 'skew_gaussian'])) &\n",
    "    (df_combined['dip_best_delta_bic'] > 10)\n",
    "].copy()\n",
    "\n",
    "print(f\"Relaxed candidates (multi-cam runs): {len(candidates_relaxed):,}\")\n",
    "print(f\"Strict candidates (+ morph + BIC):   {len(candidates_strict):,}\")\n",
    "\n",
    "# Save to files\n",
    "relaxed_path = base_path / 'candidates_relaxed_combined.csv'\n",
    "strict_path = base_path / 'candidates_strict_combined.csv'\n",
    "\n",
    "candidates_relaxed.to_csv(relaxed_path, index=False)\n",
    "candidates_strict.to_csv(strict_path, index=False)\n",
    "\n",
    "print(f\"\\nSaved relaxed candidates to: {relaxed_path}\")\n",
    "print(f\"Saved strict candidates to:  {strict_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-final",
   "metadata": {},
   "source": [
    "## 14. Summary & Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"COMPREHENSIVE EDA SUMMARY\")\nprint(\"=\"*70)\n\nprint(f\"\"\"\nDATASET OVERVIEW:\n  Bright sample (12.0-12.5 mag): {len(df_bright):>10,} light curves\n  Mid sample (12.5-13.0 mag):    {len(df_mid):>10,} light curves\n  Faint sample (13.0-13.5 mag):  {len(df_faint):>10,} light curves\n  Combined:                      {len(df_combined):>10,} light curves\n\nDETECTION SUMMARY:\n  Bright - Dip detections:  {df_bright['dip_significant'].sum():>6,} ({df_bright['dip_significant'].mean()*100:.2f}%)\n  Bright - Jump detections: {df_bright['jump_significant'].sum():>6,} ({df_bright['jump_significant'].mean()*100:.2f}%)\n  Mid    - Dip detections:  {df_mid['dip_significant'].sum():>6,} ({df_mid['dip_significant'].mean()*100:.2f}%)\n  Mid    - Jump detections: {df_mid['jump_significant'].sum():>6,} ({df_mid['jump_significant'].mean()*100:.2f}%)\n  Faint  - Dip detections:  {df_faint['dip_significant'].sum():>6,} ({df_faint['dip_significant'].mean()*100:.2f}%)\n  Faint  - Jump detections: {df_faint['jump_significant'].sum():>6,} ({df_faint['jump_significant'].mean()*100:.2f}%)\n\nMULTI-CAMERA ROBUST DETECTIONS (anti-artifact filter):\n  Bright - Multi-cam dip runs (cams>=2): {((df_bright['dip_run_count'] >= 1) & (df_bright['dip_max_run_cameras'] >= 2)).sum():>6,}\n  Mid    - Multi-cam dip runs (cams>=2): {((df_mid['dip_run_count'] >= 1) & (df_mid['dip_max_run_cameras'] >= 2)).sum():>6,}\n  Faint  - Multi-cam dip runs (cams>=2): {((df_faint['dip_run_count'] >= 1) & (df_faint['dip_max_run_cameras'] >= 2)).sum():>6,}\n\nMORPHOLOGY BREAKDOWN (Dips):\n  Bright - Gaussian:      {(df_bright['dip_best_morph'] == 'gaussian').sum():>6,}\n  Bright - Skew-Gaussian: {(df_bright['dip_best_morph'] == 'skew_gaussian').sum():>6,}\n  Mid    - Gaussian:      {(df_mid['dip_best_morph'] == 'gaussian').sum():>6,}\n  Mid    - Skew-Gaussian: {(df_mid['dip_best_morph'] == 'skew_gaussian').sum():>6,}\n  Faint  - Gaussian:      {(df_faint['dip_best_morph'] == 'gaussian').sum():>6,}\n  Faint  - Skew-Gaussian: {(df_faint['dip_best_morph'] == 'skew_gaussian').sum():>6,}\n\nSTRONG CANDIDATES (all criteria):\n  Relaxed (multi-cam runs):     {len(candidates_relaxed):>6,}\n  Strict (+ morph + BIC > 10):  {len(candidates_strict):>6,}\n\"\"\")\n\nprint(\"\\nKEY OBSERVATIONS:\")\nprint(\"-\" * 50)\n\n# Compare detection rates across all three bins\nrate_bright = df_bright['dip_significant'].mean() * 100\nrate_mid = df_mid['dip_significant'].mean() * 100\nrate_faint = df_faint['dip_significant'].mean() * 100\nprint(f\"  - Dip detection rates: Bright={rate_bright:.2f}%, Mid={rate_mid:.2f}%, Faint={rate_faint:.2f}%\")\n\n# Multi-camera importance\nmulti_cam_frac = ((df_combined['dip_run_count'] >= 1) & (df_combined['dip_max_run_cameras'] >= 2)).sum() / (df_combined['dip_run_count'] >= 1).sum() * 100\nprint(f\"  - {multi_cam_frac:.1f}% of dip runs have multi-camera confirmation\")\n\n# Morphology\nmorph_mask = df_combined['dip_best_morph'] != 'none'\nif morph_mask.sum() > 0:\n    gauss_frac = (df_combined[morph_mask]['dip_best_morph'].isin(['gaussian', 'skew_gaussian'])).mean() * 100\n    print(f\"  - {gauss_frac:.1f}% of morphology fits are Gaussian/Skew-Gaussian\")\n\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Create a summary bar chart\n",
    "ax1 = fig.add_subplot(131)\n",
    "categories = ['Total LCs', 'Dip Runs', 'Multi-Cam\\nRuns', 'Strict\\nCandidates']\n",
    "\n",
    "bright_vals = [\n",
    "    len(df_bright),\n",
    "    (df_bright['dip_run_count'] >= 1).sum(),\n",
    "    ((df_bright['dip_run_count'] >= 1) & (df_bright['dip_max_run_cameras'] >= 2)).sum(),\n",
    "    len(candidates_strict[candidates_strict['mag_bin'] == '12.0-12.5']) if 'mag_bin' in candidates_strict.columns else 0\n",
    "]\n",
    "\n",
    "mid_vals = [\n",
    "    len(df_mid),\n",
    "    (df_mid['dip_run_count'] >= 1).sum(),\n",
    "    ((df_mid['dip_run_count'] >= 1) & (df_mid['dip_max_run_cameras'] >= 2)).sum(),\n",
    "    len(candidates_strict[candidates_strict['mag_bin'] == '12.5-13.0']) if 'mag_bin' in candidates_strict.columns else 0\n",
    "]\n",
    "\n",
    "faint_vals = [\n",
    "    len(df_faint),\n",
    "    (df_faint['dip_run_count'] >= 1).sum(),\n",
    "    ((df_faint['dip_run_count'] >= 1) & (df_faint['dip_max_run_cameras'] >= 2)).sum(),\n",
    "    len(candidates_strict[candidates_strict['mag_bin'] == '13.0-13.5']) if 'mag_bin' in candidates_strict.columns else 0\n",
    "]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.25\n",
    "\n",
    "ax1.bar(x - width, bright_vals, width, label='12.0-12.5 mag', color='steelblue')\n",
    "ax1.bar(x, mid_vals, width, label='12.5-13.0 mag', color='coral')\n",
    "ax1.bar(x + width, faint_vals, width, label='13.0-13.5 mag', color='seagreen')\n",
    "\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Filter Cascade Summary')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(categories)\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Detection significance pie chart\n",
    "ax2 = fig.add_subplot(132)\n",
    "total = len(df_combined)\n",
    "dip_only = ((df_combined['dip_significant']) & (~df_combined['jump_significant'])).sum()\n",
    "jump_only = ((~df_combined['dip_significant']) & (df_combined['jump_significant'])).sum()\n",
    "both = ((df_combined['dip_significant']) & (df_combined['jump_significant'])).sum()\n",
    "neither = ((~df_combined['dip_significant']) & (~df_combined['jump_significant'])).sum()\n",
    "\n",
    "sizes = [dip_only, jump_only, both, neither]\n",
    "labels = [f'Dip only\\n({dip_only:,})', f'Jump only\\n({jump_only:,})', \n",
    "          f'Both\\n({both:,})', f'Neither\\n({neither:,})']\n",
    "colors = ['#2ecc71', '#3498db', '#9b59b6', '#95a5a6']\n",
    "explode = (0.02, 0.02, 0.02, 0)\n",
    "\n",
    "ax2.pie(sizes, labels=labels, colors=colors, explode=explode, autopct='%1.1f%%', \n",
    "        startangle=90, textprops={'fontsize': 9})\n",
    "ax2.set_title('Detection Classification (Combined)')\n",
    "\n",
    "# Candidate quality scatter\n",
    "ax3 = fig.add_subplot(133)\n",
    "if len(candidates_strict) > 0:\n",
    "    sc = ax3.scatter(\n",
    "        np.log10(candidates_strict['dip_bayes_factor']),\n",
    "        candidates_strict['dip_max_run_cameras'],\n",
    "        c=candidates_strict['dip_best_delta_bic'].clip(upper=1000),\n",
    "        cmap='viridis', alpha=0.7, s=30, edgecolors='black', linewidths=0.5\n",
    "    )\n",
    "    plt.colorbar(sc, ax=ax3, label='Delta BIC')\n",
    "    ax3.set_xlabel('log10(Bayes Factor)')\n",
    "    ax3.set_ylabel('Cameras in Best Run')\n",
    "    ax3.set_title(f'Strict Candidates Quality (n={len(candidates_strict)})')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No strict candidates', ha='center', va='center', transform=ax3.transAxes)\n",
    "    ax3.set_title('Strict Candidates Quality')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}