{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Profiling `malca.events` on SkyPatrol light curves\n",
    "\n",
    "Use `cProfile` to time the Bayesian event scorer on real SkyPatrol CSVs stored in `../../input/skypatrol2`. The notebook keeps everything self contained so it can run directly against the repo without extra setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "Locate the repo root, add it to `sys.path`, and collect the SkyPatrol CSVs we'll profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Find repo root whether the notebook is run from notebooks/ or repo root\n",
    "candidates = [Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent]\n",
    "repo_root = next((p for p in candidates if (p / \"malca\").exists()), Path.cwd())\n",
    "\n",
    "for path in (repo_root, repo_root / \"malca\"):\n",
    "    sp = str(path.resolve())\n",
    "    if sp not in sys.path:\n",
    "        sys.path.insert(0, sp)\n",
    "\n",
    "data_dir = repo_root / \"input\" / \"skypatrol2\"\n",
    "lc_paths = sorted(data_dir.glob(\"*-light-curves.csv\"))\n",
    "\n",
    "print(f\"Repo root: {repo_root}\")\n",
    "print(f\"Found {len(lc_paths)} light curves in {data_dir}\")\n",
    "if not lc_paths:\n",
    "    raise FileNotFoundError(\"No SkyPatrol CSVs found; adjust data_dir above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Quick peek at one SkyPatrol light curve\n",
    "\n",
    "Read a single CSV to confirm the loader works and to see the columns that flow into the scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from malca.plot import read_skypatrol_csv\n",
    "\n",
    "example_path = lc_paths[0]\n",
    "df_example = read_skypatrol_csv(example_path)\n",
    "print(f\"{example_path.name}: {len(df_example)} rows\")\n",
    "display(df_example.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Profiling helpers\n",
    "\n",
    "Wrap `process_one` so we can reuse the same kwargs as the CLI and capture both `cProfile` output and a tidy summary table of the hottest functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": "import cProfile\nimport io\nimport pstats\nimport time\n\nimport malca.events as events\n\nDEFAULT_EVENT_KWARGS = {\n    \"trigger_mode\": \"posterior_prob\",\n    \"logbf_threshold_dip\": 5.0,\n    \"logbf_threshold_jump\": 5.0,\n    \"significance_threshold\": 99.99997,\n    \"p_points\": 80,\n    \"p_min_dip\": None,\n    \"p_max_dip\": None,\n    \"p_min_jump\": None,\n    \"p_max_jump\": None,\n    \"mag_points\": 12,\n    \"run_min_points\": 3,\n    \"run_allow_gap_points\": 1,\n    \"run_max_gap_days\": None,\n    \"run_min_duration_days\": None,\n    \"baseline_tag\": \"gp\",\n    \"use_sigma_eff\": True,\n    \"require_sigma_eff\": True,\n    \"compute_event_prob\": True,\n}\n\ndef score_single_lc(path: Path, **overrides):\n    kwargs = dict(DEFAULT_EVENT_KWARGS)\n    kwargs.update(overrides)\n    return events.process_one(str(path), **kwargs)\n\ndef stats_to_frame(stats_obj, limit=20):\n    rows = []\n    for func, (cc, nc, tt, ct, callers) in stats_obj.stats.items():\n        rows.append(\n            {\n                \"func\": f\"{func[2]} ({Path(func[0]).name}:{func[1]})\",\n                \"ncalls\": nc,\n                \"tottime_s\": tt,\n                \"cumtime_s\": ct,\n            }\n        )\n    df_stats = pd.DataFrame(rows)\n    if df_stats.empty:\n        return df_stats\n    return df_stats.sort_values(\"cumtime_s\", ascending=False).head(limit)\n\ndef profile_light_curve(path: Path, stats_limit=25, **overrides):\n    profiler = cProfile.Profile()\n    start = time.perf_counter()\n    result = profiler.runcall(score_single_lc, path, **overrides)\n    elapsed = time.perf_counter() - start\n\n    stats_buffer = io.StringIO()\n    stats_obj = pstats.Stats(profiler, stream=stats_buffer).strip_dirs().sort_stats(\"cumtime\")\n    stats_obj.print_stats(stats_limit)\n    stats_text = stats_buffer.getvalue()\n\n    top_df = stats_to_frame(stats_obj, limit=stats_limit)\n    return result, stats_text, top_df, elapsed"
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Profile a single light curve\n",
    "\n",
    "Run the Bayesian scorer on one SkyPatrol CSV (default parameters match the CLI). `stats_df_single` highlights the functions consuming the most cumulative time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_path = lc_paths[0]\n",
    "result_single, stats_text_single, stats_df_single, elapsed_single = profile_light_curve(single_path, stats_limit=25)\n",
    "\n",
    "print(f\"Profiled {single_path.name} in {elapsed_single:.2f} s\")\n",
    "print(stats_text_single)\n",
    "stats_df_single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Batch timing on a handful of files (optional)\n",
    "\n",
    "Process a small subset sequentially to gauge throughput without full profiling. Adjust `N_LC` to cover more files if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LC = 3\n",
    "subset_paths = lc_paths[:N_LC]\n",
    "\n",
    "timing_rows = []\n",
    "for path in subset_paths:\n",
    "    t0 = time.perf_counter()\n",
    "    res = score_single_lc(path)\n",
    "    timing_rows.append(\n",
    "        {\n",
    "            \"path\": path.name,\n",
    "            \"elapsed_s\": time.perf_counter() - t0,\n",
    "            \"n_points\": res.get(\"n_points\"),\n",
    "            \"dip_sig\": res.get(\"dip_significant\"),\n",
    "            \"jump_sig\": res.get(\"jump_significant\"),\n",
    "            \"dip_bf\": res.get(\"dip_bayes_factor\"),\n",
    "            \"jump_bf\": res.get(\"jump_bayes_factor\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "pd.DataFrame(timing_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Grid resolution sweep\n",
    "Compare how `p_points` (probability grid) and `mag_points` (magnitude grid) affect runtime and outputs on a small SkyPatrol subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": "import time\nfrom malca.baseline import per_camera_gp_baseline\nimport malca.events as events\n\nBASELINE_KWARGS = dict(events.DEFAULT_BASELINE_KWARGS)\n\ndef prepare_light_curve(path):\n    df_raw = read_skypatrol_csv(path)\n    valid_mask = (\n        df_raw[\"JD\"].pipe(np.isfinite)\n        & df_raw[\"mag\"].pipe(np.isfinite)\n        & df_raw[\"error\"].pipe(np.isfinite)\n        & (df_raw[\"error\"] > 0)\n        & (df_raw[\"error\"] < 10)\n    )\n    df = df_raw[valid_mask].copy()\n    if len(df) < 10:\n        raise ValueError(f\"Insufficient valid data points ({len(df)}) in {path.name}\")\n    df = events.clean_lc(df)\n\n    df_base = per_camera_gp_baseline(df, **BASELINE_KWARGS)\n    baseline_mags = df_base[\"baseline\"].to_numpy(float) if \"baseline\" in df_base.columns else df_base[\"mag\"].to_numpy(float)\n    baseline_mag = float(np.nanmedian(baseline_mags))\n    mags_for_grid = df_base[\"mag\"].to_numpy(float) if \"mag\" in df_base.columns else df[\"mag\"].to_numpy(float)\n\n    def baseline_precomputed(df_in, **kwargs):\n        return df_base\n\n    return df, baseline_precomputed, baseline_mag, mags_for_grid\n\ndef run_grid_setting(path, p_points, mag_points, label):\n    df, baseline_fn, baseline_mag, mags_for_grid = prepare_light_curve(path)\n    mag_grid_dip = events.default_mag_grid(baseline_mag, mags_for_grid, \"dip\", n=mag_points)\n    mag_grid_jump = events.default_mag_grid(baseline_mag, mags_for_grid, \"jump\", n=mag_points)\n\n    start = time.perf_counter()\n    res = events.run_bayesian_significance(\n        df,\n        baseline_func=baseline_fn,\n        baseline_kwargs={},\n        p_points=p_points,\n        mag_grid_dip=mag_grid_dip,\n        mag_grid_jump=mag_grid_jump,\n        trigger_mode=\"posterior_prob\",\n        logbf_threshold_dip=5.0,\n        logbf_threshold_jump=5.0,\n        significance_threshold=99.99997,\n        run_min_points=3,\n        run_allow_gap_points=1,\n        run_max_gap_days=None,\n        run_min_duration_days=None,\n        use_sigma_eff=True,\n        require_sigma_eff=True,\n        compute_event_prob=True,\n    )\n    elapsed = time.perf_counter() - start\n\n    return {\n        \"path\": path.name,\n        \"config\": label,\n        \"p_points\": p_points,\n        \"mag_points\": mag_points,\n        \"elapsed_s\": elapsed,\n        \"dip_sig\": res[\"dip\"][\"significant\"],\n        \"jump_sig\": res[\"jump\"][\"significant\"],\n        \"dip_bf\": res[\"dip\"][\"bayes_factor\"],\n        \"jump_bf\": res[\"jump\"][\"bayes_factor\"],\n        \"dip_best_p\": res[\"dip\"][\"best_p\"],\n        \"jump_best_p\": res[\"jump\"][\"best_p\"],\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a few grid settings on a small subset of light curves\n",
    "N_LC = 3\n",
    "subset_paths = lc_paths[:N_LC]\n",
    "\n",
    "grid_settings = [\n",
    "    {\"label\": \"baseline_80x60\", \"p_points\": 80, \"mag_points\": 60},\n",
    "    {\"label\": \"coarse_40x30\", \"p_points\": 40, \"mag_points\": 30},\n",
    "    {\"label\": \"fine_p_160x60\", \"p_points\": 160, \"mag_points\": 60},\n",
    "    {\"label\": \"fine_mag_80x120\", \"p_points\": 80, \"mag_points\": 120},\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for path in subset_paths:\n",
    "    for cfg in grid_settings:\n",
    "        rows.append(run_grid_setting(path, cfg[\"p_points\"], cfg[\"mag_points\"], cfg[\"label\"]))\n",
    "\n",
    "df_grid = pd.DataFrame(rows)\n",
    "df_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare each setting to the baseline configuration\n",
    "baseline_label = \"baseline_80x60\"\n",
    "base = df_grid[df_grid[\"config\"] == baseline_label]\n",
    "comparison = df_grid.merge(base, on=\"path\", suffixes=(\"\", \"_base\"))\n",
    "comparison[\"dip_bf_delta\"] = comparison[\"dip_bf\"] - comparison[\"dip_bf_base\"]\n",
    "comparison[\"jump_bf_delta\"] = comparison[\"jump_bf\"] - comparison[\"jump_bf_base\"]\n",
    "comparison[\"elapsed_delta_s\"] = comparison[\"elapsed_s\"] - comparison[\"elapsed_s_base\"]\n",
    "cols = [\n",
    "    \"path\",\n",
    "    \"config\",\n",
    "    \"p_points\",\n",
    "    \"mag_points\",\n",
    "    \"elapsed_s\",\n",
    "    \"elapsed_delta_s\",\n",
    "    \"dip_sig\",\n",
    "    \"dip_sig_base\",\n",
    "    \"dip_bf\",\n",
    "    \"dip_bf_delta\",\n",
    "    \"jump_sig\",\n",
    "    \"jump_sig_base\",\n",
    "    \"jump_bf\",\n",
    "    \"jump_bf_delta\",\n",
    "]\n",
    "comparison[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Extended coarse grid sweep\n",
    "Test a wider range of grid configurations, especially at the coarse end, to find the optimal speed/accuracy tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build simulated light curves by injecting analytic events into a real cadence\n",
    "from malca import events\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "base_sim_path = lc_paths[0]\n",
    "df_sim_base_raw = read_skypatrol_csv(base_sim_path)\n",
    "mask = (\n",
    "    df_sim_base_raw[\"JD\"].pipe(np.isfinite)\n",
    "    & df_sim_base_raw[\"mag\"].pipe(np.isfinite)\n",
    "    & df_sim_base_raw[\"error\"].pipe(np.isfinite)\n",
    "    & (df_sim_base_raw[\"error\"] > 0)\n",
    "    & (df_sim_base_raw[\"error\"] < 10)\n",
    ")\n",
    "df_sim_base = df_sim_base_raw[mask].copy()\n",
    "df_sim_base = events.clean_lc(df_sim_base)\n",
    "\n",
    "jd_med = float(df_sim_base[\"JD\"].median())\n",
    "jd_span = float(df_sim_base[\"JD\"].max() - df_sim_base[\"JD\"].min())\n",
    "\n",
    "\n",
    "def inject_event(df_in, kind=\"dip\", shape=\"gaussian\", amp=0.25, width=25.0, t0_offset=0.0):\n",
    "    # Return a copy with an injected event (positive amp = dip, negative amp = jump).\n",
    "    df = df_in.copy()\n",
    "    t0 = jd_med + float(t0_offset)\n",
    "    amp_signed = float(amp) if kind == \"dip\" else -float(amp)\n",
    "    t_arr = df[\"JD\"].to_numpy(float)\n",
    "    if shape == \"gaussian\":\n",
    "        delta = events.gaussian(t_arr, amp_signed, t0, float(width), 0.0)\n",
    "    elif shape == \"paczynski\":\n",
    "        delta = events.paczynski(t_arr, amp_signed, t0, float(width), 0.0)\n",
    "    else:\n",
    "        raise ValueError(\"shape must be gaussian or paczynski\")\n",
    "    df[\"mag\"] = df[\"mag\"].to_numpy(float) + delta\n",
    "    return df\n",
    "\n",
    "\n",
    "def inject_mixed(df_in):\n",
    "    df = inject_event(df_in, kind=\"dip\", shape=\"gaussian\", amp=0.18, width=20.0, t0_offset=-60.0)\n",
    "    df = inject_event(df, kind=\"jump\", shape=\"paczynski\", amp=0.14, width=30.0, t0_offset=40.0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_noise_and_gaps(df_in, jitter_mag=0.02, gap_frac=0.15, spike_amp=0.25, spike_width=0.08, spike_count=3):\n",
    "    # Add photometric jitter, drop random points, and sprinkle outliers to make messy cases.\n",
    "    df = df_in.copy()\n",
    "    n = len(df)\n",
    "    df[\"mag\"] = df[\"mag\"].to_numpy(float) + rng.normal(0.0, jitter_mag, n)\n",
    "    df[\"error\"] = (df[\"error\"].to_numpy(float) * (1 + rng.normal(0.0, 0.05, n))).clip(min=1e-3)\n",
    "    keep = rng.random(n) > gap_frac\n",
    "    df = df.loc[keep].copy().reset_index(drop=True)\n",
    "    if len(df) == 0:\n",
    "        return df_in.copy()\n",
    "    for _ in range(int(spike_count)):\n",
    "        idx = int(rng.integers(0, len(df)))\n",
    "        df.loc[idx, \"mag\"] += rng.normal(spike_amp, spike_width)\n",
    "    df = df.sort_values(\"JD\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# MASSIVELY EXPANDED SIMULATED LIGHT CURVES\n",
    "simulated_lcs = {}\n",
    "\n",
    "# ========== SINGLE DIPS - GAUSSIAN ==========\n",
    "# Amplitude sweep (shallow to deep)\n",
    "for amp in [0.05, 0.08, 0.10, 0.12, 0.15, 0.18, 0.20, 0.25, 0.30, 0.40, 0.50]:\n",
    "    simulated_lcs[f\"dip_gauss_a{amp:.2f}_w20\"] = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=amp, width=20.0, t0_offset=0.0)\n",
    "\n",
    "# Width sweep (fast to slow)\n",
    "for width in [3.0, 5.0, 8.0, 10.0, 15.0, 20.0, 25.0, 30.0, 40.0, 50.0, 70.0, 100.0]:\n",
    "    simulated_lcs[f\"dip_gauss_a0.20_w{width:.0f}\"] = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.20, width=width, t0_offset=0.0)\n",
    "\n",
    "# Position sweep (early, middle, late)\n",
    "for offset in [-jd_span*0.4, -jd_span*0.2, 0.0, jd_span*0.2, jd_span*0.4]:\n",
    "    simulated_lcs[f\"dip_gauss_a0.20_w20_t{offset:.0f}\"] = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.20, width=20.0, t0_offset=offset)\n",
    "\n",
    "# ========== SINGLE DIPS - PACZYNSKI ==========\n",
    "# Amplitude sweep\n",
    "for amp in [0.05, 0.08, 0.10, 0.12, 0.15, 0.18, 0.20, 0.25, 0.30, 0.40, 0.50]:\n",
    "    simulated_lcs[f\"dip_pacz_a{amp:.2f}_w20\"] = inject_event(df_sim_base, \"dip\", \"paczynski\", amp=amp, width=20.0, t0_offset=0.0)\n",
    "\n",
    "# Width sweep\n",
    "for width in [3.0, 5.0, 8.0, 10.0, 15.0, 20.0, 25.0, 30.0, 40.0, 50.0, 70.0]:\n",
    "    simulated_lcs[f\"dip_pacz_a0.20_w{width:.0f}\"] = inject_event(df_sim_base, \"dip\", \"paczynski\", amp=0.20, width=width, t0_offset=0.0)\n",
    "\n",
    "# ========== SINGLE JUMPS - GAUSSIAN ==========\n",
    "# Amplitude sweep\n",
    "for amp in [0.05, 0.08, 0.10, 0.12, 0.15, 0.18, 0.20, 0.25, 0.30, 0.40]:\n",
    "    simulated_lcs[f\"jump_gauss_a{amp:.2f}_w20\"] = inject_event(df_sim_base, \"jump\", \"gaussian\", amp=amp, width=20.0, t0_offset=0.0)\n",
    "\n",
    "# Width sweep\n",
    "for width in [5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 40.0, 60.0, 80.0]:\n",
    "    simulated_lcs[f\"jump_gauss_a0.20_w{width:.0f}\"] = inject_event(df_sim_base, \"jump\", \"gaussian\", amp=0.20, width=width, t0_offset=0.0)\n",
    "\n",
    "# ========== SINGLE JUMPS - PACZYNSKI (MICROLENSING) ==========\n",
    "# Amplitude sweep\n",
    "for amp in [0.05, 0.08, 0.10, 0.12, 0.15, 0.18, 0.20, 0.25, 0.30, 0.40, 0.50]:\n",
    "    simulated_lcs[f\"jump_pacz_a{amp:.2f}_w20\"] = inject_event(df_sim_base, \"jump\", \"paczynski\", amp=amp, width=20.0, t0_offset=0.0)\n",
    "\n",
    "# Width sweep (microlensing timescales)\n",
    "for width in [5.0, 8.0, 10.0, 15.0, 20.0, 25.0, 30.0, 40.0, 50.0, 70.0, 100.0]:\n",
    "    simulated_lcs[f\"jump_pacz_a0.20_w{width:.0f}\"] = inject_event(df_sim_base, \"jump\", \"paczynski\", amp=0.20, width=width, t0_offset=0.0)\n",
    "\n",
    "# ========== DOUBLE EVENTS ==========\n",
    "# Two dips - varying separation\n",
    "for sep in [50.0, 80.0, 120.0, 180.0]:\n",
    "    df_temp = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.18, width=15.0, t0_offset=-sep/2)\n",
    "    simulated_lcs[f\"dip_double_sep{sep:.0f}\"] = inject_event(df_temp, \"dip\", \"gaussian\", amp=0.15, width=12.0, t0_offset=sep/2)\n",
    "\n",
    "# Two jumps\n",
    "for sep in [60.0, 100.0, 150.0]:\n",
    "    df_temp = inject_event(df_sim_base, \"jump\", \"paczynski\", amp=0.15, width=18.0, t0_offset=-sep/2)\n",
    "    simulated_lcs[f\"jump_double_sep{sep:.0f}\"] = inject_event(df_temp, \"jump\", \"paczynski\", amp=0.13, width=15.0, t0_offset=sep/2)\n",
    "\n",
    "# Dip then jump\n",
    "for sep in [60.0, 100.0, 140.0]:\n",
    "    df_temp = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.18, width=20.0, t0_offset=-sep/2)\n",
    "    simulated_lcs[f\"dip_then_jump_sep{sep:.0f}\"] = inject_event(df_temp, \"jump\", \"paczynski\", amp=0.14, width=25.0, t0_offset=sep/2)\n",
    "\n",
    "# Jump then dip\n",
    "for sep in [60.0, 100.0]:\n",
    "    df_temp = inject_event(df_sim_base, \"jump\", \"gaussian\", amp=0.16, width=22.0, t0_offset=-sep/2)\n",
    "    simulated_lcs[f\"jump_then_dip_sep{sep:.0f}\"] = inject_event(df_temp, \"dip\", \"paczynski\", amp=0.15, width=18.0, t0_offset=sep/2)\n",
    "\n",
    "# ========== TRIPLE EVENTS ==========\n",
    "# Three dips\n",
    "df_temp = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.12, width=12.0, t0_offset=-100.0)\n",
    "df_temp = inject_event(df_temp, \"dip\", \"gaussian\", amp=0.15, width=15.0, t0_offset=0.0)\n",
    "simulated_lcs[\"dip_triple\"] = inject_event(df_temp, \"dip\", \"gaussian\", amp=0.10, width=10.0, t0_offset=100.0)\n",
    "\n",
    "# ========== NOISY VERSIONS ==========\n",
    "# Low noise\n",
    "for case in [\"dip_gauss_a0.20_w20\", \"dip_pacz_a0.20_w20\", \"jump_gauss_a0.20_w20\", \"jump_pacz_a0.20_w20\"]:\n",
    "    if case in simulated_lcs:\n",
    "        simulated_lcs[f\"{case}_noise_low\"] = add_noise_and_gaps(simulated_lcs[case], jitter_mag=0.01, gap_frac=0.05, spike_count=1)\n",
    "\n",
    "# Medium noise\n",
    "for case in [\"dip_gauss_a0.20_w20\", \"dip_pacz_a0.20_w20\", \"jump_gauss_a0.20_w20\", \"jump_pacz_a0.20_w20\"]:\n",
    "    if case in simulated_lcs:\n",
    "        simulated_lcs[f\"{case}_noise_med\"] = add_noise_and_gaps(simulated_lcs[case], jitter_mag=0.02, gap_frac=0.12, spike_count=2)\n",
    "\n",
    "# High noise\n",
    "for case in [\"dip_gauss_a0.20_w20\", \"dip_pacz_a0.20_w20\", \"jump_gauss_a0.20_w20\", \"jump_pacz_a0.20_w20\"]:\n",
    "    if case in simulated_lcs:\n",
    "        simulated_lcs[f\"{case}_noise_high\"] = add_noise_and_gaps(simulated_lcs[case], jitter_mag=0.04, gap_frac=0.25, spike_count=5)\n",
    "\n",
    "# Very high noise (challenging)\n",
    "for case in [\"dip_gauss_a0.20_w20\", \"dip_pacz_a0.20_w20\", \"jump_gauss_a0.20_w20\", \"jump_pacz_a0.20_w20\"]:\n",
    "    if case in simulated_lcs:\n",
    "        simulated_lcs[f\"{case}_noise_extreme\"] = add_noise_and_gaps(simulated_lcs[case], jitter_mag=0.06, gap_frac=0.35, spike_count=8)\n",
    "\n",
    "# ========== SHALLOW + NOISY (DETECTION LIMITS) ==========\n",
    "for amp in [0.05, 0.08, 0.10]:\n",
    "    for noise in [0.01, 0.02, 0.03]:\n",
    "        df_temp = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=amp, width=20.0, t0_offset=0.0)\n",
    "        simulated_lcs[f\"dip_shallow_a{amp:.2f}_n{noise:.2f}\"] = add_noise_and_gaps(df_temp, jitter_mag=noise, gap_frac=0.1, spike_count=2)\n",
    "\n",
    "# ========== FAST TRANSIENTS ==========\n",
    "for width in [2.0, 3.0, 4.0, 5.0]:\n",
    "    simulated_lcs[f\"dip_fast_w{width:.0f}\"] = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.25, width=width, t0_offset=0.0)\n",
    "    simulated_lcs[f\"jump_fast_w{width:.0f}\"] = inject_event(df_sim_base, \"jump\", \"gaussian\", amp=0.20, width=width, t0_offset=0.0)\n",
    "\n",
    "# ========== SLOW TRANSIENTS ==========\n",
    "for width in [80.0, 100.0, 120.0, 150.0]:\n",
    "    simulated_lcs[f\"dip_slow_w{width:.0f}\"] = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.22, width=width, t0_offset=0.0)\n",
    "    simulated_lcs[f\"jump_slow_w{width:.0f}\"] = inject_event(df_sim_base, \"jump\", \"paczynski\", amp=0.20, width=width, t0_offset=0.0)\n",
    "\n",
    "# ========== ASYMMETRIC EVENTS (MIXED SHAPES) ==========\n",
    "# Gaussian dip + Paczynski jump\n",
    "df_temp = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.18, width=18.0, t0_offset=-70.0)\n",
    "simulated_lcs[\"asym_gauss_dip_pacz_jump\"] = inject_event(df_temp, \"jump\", \"paczynski\", amp=0.16, width=25.0, t0_offset=50.0)\n",
    "\n",
    "# Paczynski dip + Gaussian jump\n",
    "df_temp = inject_event(df_sim_base, \"dip\", \"paczynski\", amp=0.20, width=22.0, t0_offset=-80.0)\n",
    "simulated_lcs[\"asym_pacz_dip_gauss_jump\"] = inject_event(df_temp, \"jump\", \"gaussian\", amp=0.15, width=20.0, t0_offset=60.0)\n",
    "\n",
    "# ========== EXTREME CASES ==========\n",
    "# Very deep dip\n",
    "simulated_lcs[\"dip_very_deep\"] = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.70, width=25.0, t0_offset=0.0)\n",
    "\n",
    "# Very shallow dip (near detection limit)\n",
    "simulated_lcs[\"dip_very_shallow\"] = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.03, width=20.0, t0_offset=0.0)\n",
    "\n",
    "# Very fast dip\n",
    "simulated_lcs[\"dip_very_fast\"] = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.25, width=1.5, t0_offset=0.0)\n",
    "\n",
    "# Very slow dip\n",
    "simulated_lcs[\"dip_very_slow\"] = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.20, width=200.0, t0_offset=0.0)\n",
    "\n",
    "# Strong microlensing\n",
    "simulated_lcs[\"microlens_strong\"] = inject_event(df_sim_base, \"jump\", \"paczynski\", amp=0.60, width=30.0, t0_offset=0.0)\n",
    "\n",
    "# Weak microlensing\n",
    "simulated_lcs[\"microlens_weak\"] = inject_event(df_sim_base, \"jump\", \"paczynski\", amp=0.04, width=15.0, t0_offset=0.0)\n",
    "\n",
    "# ========== EDGE CASES ==========\n",
    "# Event at beginning\n",
    "simulated_lcs[\"dip_at_start\"] = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.22, width=20.0, t0_offset=-jd_span*0.45)\n",
    "\n",
    "# Event at end\n",
    "simulated_lcs[\"dip_at_end\"] = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.22, width=20.0, t0_offset=jd_span*0.45)\n",
    "\n",
    "# Multiple small events\n",
    "df_temp = df_sim_base.copy()\n",
    "for offset in [-120.0, -60.0, 0.0, 60.0, 120.0]:\n",
    "    df_temp = inject_event(df_temp, \"dip\", \"gaussian\", amp=0.08, width=10.0, t0_offset=offset)\n",
    "simulated_lcs[\"dip_many_small\"] = df_temp\n",
    "\n",
    "# Overlapping events\n",
    "df_temp = inject_event(df_sim_base, \"dip\", \"gaussian\", amp=0.20, width=30.0, t0_offset=-10.0)\n",
    "simulated_lcs[\"dip_overlapping\"] = inject_event(df_temp, \"dip\", \"gaussian\", amp=0.15, width=25.0, t0_offset=10.0)\n",
    "\n",
    "print(f\"Created {len(simulated_lcs)} simulated light curves\")\n",
    "print(f\"\\\\nCategories:\")\n",
    "print(f\"  Single dips (Gaussian): {len([k for k in simulated_lcs.keys() if k.startswith('dip_gauss') and 'noise' not in k and 'shallow' not in k])}\")\n",
    "print(f\"  Single dips (Paczynski): {len([k for k in simulated_lcs.keys() if k.startswith('dip_pacz') and 'noise' not in k])}\")\n",
    "print(f\"  Single jumps (Gaussian): {len([k for k in simulated_lcs.keys() if k.startswith('jump_gauss') and 'noise' not in k])}\")\n",
    "print(f\"  Single jumps (Paczynski): {len([k for k in simulated_lcs.keys() if k.startswith('jump_pacz') and 'noise' not in k])}\")\n",
    "print(f\"  Double/Triple events: {len([k for k in simulated_lcs.keys() if 'double' in k or 'triple' in k])}\")\n",
    "print(f\"  Noisy variants: {len([k for k in simulated_lcs.keys() if 'noise' in k])}\")\n",
    "print(f\"  Shallow+noisy: {len([k for k in simulated_lcs.keys() if 'shallow' in k])}\")\n",
    "print(f\"  Fast/slow transients: {len([k for k in simulated_lcs.keys() if 'fast' in k or 'slow' in k])}\")\n",
    "print(f\"  Edge cases: {len([k for k in simulated_lcs.keys() if any(x in k for x in ['extreme', 'asym', 'start', 'end', 'many', 'overlap'])])}\")\n",
    "print(f\"\\\\nSample keys: {list(simulated_lcs.keys())[:10]}\")\n",
    "display(df_sim_base.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Comprehensive grid performance on simulated light curves\n",
    "Run all extended grid configurations on the full simulated light curve suite and analyze detection accuracy with emphasis on false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine ground truth for each simulated light curve\n",
    "def get_ground_truth(lc_name):\n",
    "    \"\"\"Return expected detection: ('dip', 'jump', 'both', 'none')\"\"\"\n",
    "    name_lower = lc_name.lower()\n",
    "    \n",
    "    # Cases with both dip and jump\n",
    "    if any(x in name_lower for x in ['mixed', 'dip_then_jump', 'jump_then_dip', 'asym']):\n",
    "        return 'both'\n",
    "    \n",
    "    # Dip-only cases\n",
    "    elif 'dip' in name_lower:\n",
    "        # Very shallow dips may be below detection threshold\n",
    "        if 'very_shallow' in name_lower or ('shallow' in name_lower and 'a0.03' in name_lower):\n",
    "            return 'dip_marginal'  # May or may not be detected\n",
    "        return 'dip'\n",
    "    \n",
    "    # Jump-only cases (including microlensing)\n",
    "    elif 'jump' in name_lower or 'microlens' in name_lower:\n",
    "        # Very weak microlensing may be below threshold\n",
    "        if 'microlens_weak' in name_lower or ('weak' in name_lower and 'a0.04' in name_lower):\n",
    "            return 'jump_marginal'\n",
    "        return 'jump'\n",
    "    \n",
    "    # No injected events\n",
    "    else:\n",
    "        return 'none'\n",
    "\n",
    "\n",
    "# Build ground truth dictionary\n",
    "ground_truth = {name: get_ground_truth(name) for name in simulated_lcs.keys()}\n",
    "\n",
    "# Preview ground truth distribution\n",
    "from collections import Counter\n",
    "truth_counts = Counter(ground_truth.values())\n",
    "print(\"Ground truth distribution:\")\n",
    "for category, count in sorted(truth_counts.items()):\n",
    "    print(f\"  {category}: {count}\")\n",
    "    \n",
    "print(f\"\\nExamples:\")\n",
    "for name in list(simulated_lcs.keys())[:10]:\n",
    "    print(f\"  {name}: {ground_truth[name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all 65 extended grid configurations (if not already defined)\n",
    "if 'extended_grid_configs' not in globals():\n",
    "    extended_grid_configs = []\n",
    "\n",
    "    # Absurdly coarse (2x2 to 10x10)\n",
    "    for n in [2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "        extended_grid_configs.append({\"label\": f\"absurd_{n}x{n}\", \"p_points\": n, \"mag_points\": n})\n",
    "\n",
    "    # Ultra coarse (12x12 to 20x20)\n",
    "    for n in [12, 15, 18, 20]:\n",
    "        extended_grid_configs.append({\"label\": f\"ultra_coarse_{n}x{n}\", \"p_points\": n, \"mag_points\": n})\n",
    "\n",
    "    # Very coarse (22x22 to 35x35)\n",
    "    for n in [22, 25, 28, 30, 35]:\n",
    "        extended_grid_configs.append({\"label\": f\"very_coarse_{n}x{n}\", \"p_points\": n, \"mag_points\": n})\n",
    "\n",
    "    # Coarse (asymmetric versions)\n",
    "    for p, m in [(20, 15), (25, 20), (30, 20), (30, 25), (35, 25), (40, 25), (40, 30), (45, 30), (50, 35)]:\n",
    "        extended_grid_configs.append({\"label\": f\"coarse_{p}x{m}\", \"p_points\": p, \"mag_points\": m})\n",
    "\n",
    "    # Medium (50x40 to 70x60)\n",
    "    for p, m in [(50, 40), (55, 45), (60, 45), (60, 50), (65, 50), (70, 50), (70, 60)]:\n",
    "        extended_grid_configs.append({\"label\": f\"medium_{p}x{m}\", \"p_points\": p, \"mag_points\": m})\n",
    "\n",
    "    # Baseline and near-baseline\n",
    "    extended_grid_configs.append({\"label\": \"baseline_80x60\", \"p_points\": 80, \"mag_points\": 60})\n",
    "    for p, m in [(75, 55), (85, 65), (90, 65), (90, 70)]:\n",
    "        extended_grid_configs.append({\"label\": f\"near_base_{p}x{m}\", \"p_points\": p, \"mag_points\": m})\n",
    "\n",
    "    # Fine (95x70 to 120x100)\n",
    "    for p, m in [(95, 70), (100, 75), (100, 80), (105, 80), (110, 85), (110, 90), (115, 90), (120, 90), (120, 100)]:\n",
    "        extended_grid_configs.append({\"label\": f\"fine_{p}x{m}\", \"p_points\": p, \"mag_points\": m})\n",
    "\n",
    "    print(f\"Created {len(extended_grid_configs)} grid configurations\")\n",
    "    print(f\"Grid sizes range from {min(c['p_points']*c['mag_points'] for c in extended_grid_configs)} to {max(c['p_points']*c['mag_points'] for c in extended_grid_configs)}\")\n",
    "else:\n",
    "    print(f\"Using existing extended_grid_configs with {len(extended_grid_configs)} configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": "# Define helper function for running grid configs on simulated light curves\nfrom malca.baseline import per_camera_gp_baseline\n\ndef prepare_df_for_sim(df):\n    df_clean = events.clean_lc(df)\n    df_base = per_camera_gp_baseline(df_clean, **BASELINE_KWARGS)\n    baseline_mags = df_base[\"baseline\"].to_numpy(float) if \"baseline\" in df_base.columns else df_base[\"mag\"].to_numpy(float)\n    baseline_mag = float(np.nanmedian(baseline_mags))\n    mags_for_grid = df_base[\"mag\"].to_numpy(float) if \"mag\" in df_base.columns else df_clean[\"mag\"].to_numpy(float)\n\n    def baseline_precomputed(df_in, **kwargs):\n        return df_base\n\n    return df_clean, baseline_precomputed, baseline_mag, mags_for_grid\n\n\ndef run_grid_setting_sim(name, df, p_points, mag_points, label):\n    df_clean, baseline_fn, baseline_mag, mags_for_grid = prepare_df_for_sim(df)\n    mag_grid_dip = events.default_mag_grid(baseline_mag, mags_for_grid, \"dip\", n=mag_points)\n    mag_grid_jump = events.default_mag_grid(baseline_mag, mags_for_grid, \"jump\", n=mag_points)\n\n    start = time.perf_counter()\n    res = events.run_bayesian_significance(\n        df_clean,\n        baseline_func=baseline_fn,\n        baseline_kwargs={},\n        p_points=p_points,\n        mag_grid_dip=mag_grid_dip,\n        mag_grid_jump=mag_grid_jump,\n        trigger_mode=\"posterior_prob\",\n        logbf_threshold_dip=5.0,\n        logbf_threshold_jump=5.0,\n        significance_threshold=99.99997,\n        run_min_points=3,\n        run_allow_gap_points=1,\n        run_max_gap_days=None,\n        run_min_duration_days=None,\n        use_sigma_eff=True,\n        require_sigma_eff=True,\n        compute_event_prob=True,\n    )\n    elapsed = time.perf_counter() - start\n\n    return {\n        \"case\": name,\n        \"config\": label,\n        \"p_points\": p_points,\n        \"mag_points\": mag_points,\n        \"elapsed_s\": elapsed,\n        \"dip_sig\": res[\"dip\"][\"significant\"],\n        \"jump_sig\": res[\"jump\"][\"significant\"],\n        \"dip_bf\": res[\"dip\"][\"bayes_factor\"],\n        \"jump_bf\": res[\"jump\"][\"bayes_factor\"],\n        \"dip_best_p\": res[\"dip\"][\"best_p\"],\n        \"jump_best_p\": res[\"jump\"][\"best_p\"],\n    }\n\nprint(\"Helper functions defined: prepare_df_for_sim, run_grid_setting_sim\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all extended grid configs on all simulated light curves (PARALLELIZED)\n",
    "# Using 10 cores to speed up computation\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create all task combinations upfront\n",
    "tasks = []\n",
    "for cfg in extended_grid_configs:\n",
    "    for lc_name, df_sim in simulated_lcs.items():\n",
    "        tasks.append({\n",
    "            'cfg': cfg,\n",
    "            'lc_name': lc_name,\n",
    "            'df_sim': df_sim,\n",
    "            'truth': ground_truth[lc_name]\n",
    "        })\n",
    "\n",
    "print(f\"Running {len(extended_grid_configs)} configs on {len(simulated_lcs)} simulated LCs\")\n",
    "print(f\"Total tasks: {len(tasks)}\")\n",
    "print(f\"Using 16 parallel workers\")\n",
    "print(f\"Estimated time: ~{len(tasks) * 6 * 0.15 / 60 / 15:.1f} minutes with 15 cores\")\n",
    "print()\n",
    "\n",
    "# Worker function for parallel execution\n",
    "def process_single_task(task):\n",
    "    \"\"\"Process one (config, light_curve) combination\"\"\"\n",
    "    try:\n",
    "        result = run_grid_setting_sim(\n",
    "            task['lc_name'], \n",
    "            task['df_sim'], \n",
    "            task['cfg'][\"p_points\"], \n",
    "            task['cfg'][\"mag_points\"], \n",
    "            task['cfg'][\"label\"]\n",
    "        )\n",
    "        result['truth'] = task['truth']\n",
    "        return result, None\n",
    "    except Exception as e:\n",
    "        return None, {'cfg': task['cfg']['label'], 'lc': task['lc_name'], 'error': str(e)}\n",
    "\n",
    "# Run in parallel with progress bar\n",
    "start_time = time.time()\n",
    "\n",
    "results = Parallel(n_jobs=15, verbose=0)(\n",
    "    delayed(process_single_task)(task) \n",
    "    for task in tqdm(tasks, desc=\"Processing\", unit=\"task\")\n",
    ")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Separate successful results from errors\n",
    "sim_extended_rows = []\n",
    "errors = []\n",
    "for result, error in results:\n",
    "    if result is not None:\n",
    "        sim_extended_rows.append(result)\n",
    "    if error is not None:\n",
    "        errors.append(error)\n",
    "\n",
    "# Report errors if any\n",
    "if len(errors) > 0:\n",
    "    print(f\"\\n⚠ {len(errors)} errors occurred:\")\n",
    "    for err in errors[:10]:  # Show first 10\n",
    "        print(f\"  [{err['cfg']}] on {err['lc']}: {err['error']}\")\n",
    "    if len(errors) > 10:\n",
    "        print(f\"  ... and {len(errors) - 10} more errors\")\n",
    "\n",
    "df_sim_extended = pd.DataFrame(sim_extended_rows)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPLETED: {len(df_sim_extended)} results in {total_time/60:.1f} minutes\")\n",
    "print(f\"Average: {total_time/len(df_sim_extended):.3f}s per run\")\n",
    "print(f\"Speedup: ~{len(tasks) * 0.15 / total_time:.1f}x compared to sequential\")\n",
    "print(f\"{'='*80}\")\n",
    "df_sim_extended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute detection metrics for each result\n",
    "def classify_detection(row):\n",
    "    \"\"\"\n",
    "    Classify detection as TP, FP, FN, TN for both dip and jump\n",
    "    Returns dict with metrics\n",
    "    \"\"\"\n",
    "    truth = row['truth']\n",
    "    detected_dip = row['dip_sig']\n",
    "    detected_jump = row['jump_sig']\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # DIP METRICS\n",
    "    if truth in ['dip', 'both']:\n",
    "        # Should detect dip\n",
    "        metrics['dip_tp'] = 1 if detected_dip else 0\n",
    "        metrics['dip_fn'] = 0 if detected_dip else 1\n",
    "        metrics['dip_fp'] = 0\n",
    "        metrics['dip_tn'] = 0\n",
    "    elif truth in ['dip_marginal']:\n",
    "        # Marginal case - don't penalize either way\n",
    "        metrics['dip_tp'] = 0\n",
    "        metrics['dip_fn'] = 0\n",
    "        metrics['dip_fp'] = 0\n",
    "        metrics['dip_tn'] = 0\n",
    "    else:\n",
    "        # Should NOT detect dip\n",
    "        metrics['dip_tp'] = 0\n",
    "        metrics['dip_fn'] = 0\n",
    "        metrics['dip_fp'] = 1 if detected_dip else 0\n",
    "        metrics['dip_tn'] = 0 if detected_dip else 1\n",
    "    \n",
    "    # JUMP METRICS\n",
    "    if truth in ['jump', 'both']:\n",
    "        # Should detect jump\n",
    "        metrics['jump_tp'] = 1 if detected_jump else 0\n",
    "        metrics['jump_fn'] = 0 if detected_jump else 1\n",
    "        metrics['jump_fp'] = 0\n",
    "        metrics['jump_tn'] = 0\n",
    "    elif truth in ['jump_marginal']:\n",
    "        # Marginal case - don't penalize either way\n",
    "        metrics['jump_tp'] = 0\n",
    "        metrics['jump_fn'] = 0\n",
    "        metrics['jump_fp'] = 0\n",
    "        metrics['jump_tn'] = 0\n",
    "    else:\n",
    "        # Should NOT detect jump\n",
    "        metrics['jump_tp'] = 0\n",
    "        metrics['jump_fn'] = 0\n",
    "        metrics['jump_fp'] = 1 if detected_jump else 0\n",
    "        metrics['jump_tn'] = 0 if detected_jump else 1\n",
    "    \n",
    "    # COMBINED: Any false positive\n",
    "    metrics['any_fp'] = 1 if (metrics['dip_fp'] > 0 or metrics['jump_fp'] > 0) else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Apply classification to all results\n",
    "metrics_rows = []\n",
    "for idx, row in df_sim_extended.iterrows():\n",
    "    metrics = classify_detection(row)\n",
    "    metrics['config'] = row['config']\n",
    "    metrics['case'] = row['case']\n",
    "    metrics['truth'] = row['truth']\n",
    "    metrics['elapsed_s'] = row['elapsed_s']\n",
    "    metrics['dip_bf'] = row['dip_bf']\n",
    "    metrics['jump_bf'] = row['jump_bf']\n",
    "    metrics_rows.append(metrics)\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_rows)\n",
    "\n",
    "print(f\"Computed metrics for {len(df_metrics)} results\")\n",
    "df_metrics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics by configuration\n",
    "config_metrics = df_metrics.groupby('config').agg({\n",
    "    'dip_tp': 'sum',\n",
    "    'dip_fp': 'sum',\n",
    "    'dip_fn': 'sum',\n",
    "    'dip_tn': 'sum',\n",
    "    'jump_tp': 'sum',\n",
    "    'jump_fp': 'sum',\n",
    "    'jump_fn': 'sum',\n",
    "    'jump_tn': 'sum',\n",
    "    'any_fp': 'sum',\n",
    "    'elapsed_s': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate derived metrics\n",
    "config_metrics['dip_precision'] = config_metrics['dip_tp'] / (config_metrics['dip_tp'] + config_metrics['dip_fp']).replace(0, np.nan)\n",
    "config_metrics['dip_recall'] = config_metrics['dip_tp'] / (config_metrics['dip_tp'] + config_metrics['dip_fn']).replace(0, np.nan)\n",
    "config_metrics['dip_f1'] = 2 * (config_metrics['dip_precision'] * config_metrics['dip_recall']) / (config_metrics['dip_precision'] + config_metrics['dip_recall'])\n",
    "config_metrics['dip_fpr'] = config_metrics['dip_fp'] / (config_metrics['dip_fp'] + config_metrics['dip_tn']).replace(0, np.nan)\n",
    "\n",
    "config_metrics['jump_precision'] = config_metrics['jump_tp'] / (config_metrics['jump_tp'] + config_metrics['jump_fp']).replace(0, np.nan)\n",
    "config_metrics['jump_recall'] = config_metrics['jump_tp'] / (config_metrics['jump_tp'] + config_metrics['jump_fn']).replace(0, np.nan)\n",
    "config_metrics['jump_f1'] = 2 * (config_metrics['jump_precision'] * config_metrics['jump_recall']) / (config_metrics['jump_precision'] + config_metrics['jump_recall'])\n",
    "config_metrics['jump_fpr'] = config_metrics['jump_fp'] / (config_metrics['jump_fp'] + config_metrics['jump_tn']).replace(0, np.nan)\n",
    "\n",
    "# Overall metrics\n",
    "config_metrics['total_tp'] = config_metrics['dip_tp'] + config_metrics['jump_tp']\n",
    "config_metrics['total_fp'] = config_metrics['dip_fp'] + config_metrics['jump_fp']\n",
    "config_metrics['total_fn'] = config_metrics['dip_fn'] + config_metrics['jump_fn']\n",
    "config_metrics['overall_precision'] = config_metrics['total_tp'] / (config_metrics['total_tp'] + config_metrics['total_fp']).replace(0, np.nan)\n",
    "config_metrics['overall_recall'] = config_metrics['total_tp'] / (config_metrics['total_tp'] + config_metrics['total_fn']).replace(0, np.nan)\n",
    "config_metrics['overall_f1'] = 2 * (config_metrics['overall_precision'] * config_metrics['overall_recall']) / (config_metrics['overall_precision'] + config_metrics['overall_recall'])\n",
    "\n",
    "# Get grid size info from extended_grid_configs\n",
    "config_info = {cfg['label']: {'p_points': cfg['p_points'], 'mag_points': cfg['mag_points']} for cfg in extended_grid_configs}\n",
    "config_metrics['p_points'] = config_metrics['config'].map(lambda x: config_info.get(x, {}).get('p_points', np.nan))\n",
    "config_metrics['mag_points'] = config_metrics['config'].map(lambda x: config_info.get(x, {}).get('mag_points', np.nan))\n",
    "config_metrics['grid_size'] = config_metrics['p_points'] * config_metrics['mag_points']\n",
    "\n",
    "# Sort by false positive rate (ascending - lower is better)\n",
    "config_metrics_sorted = config_metrics.sort_values('total_fp')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CONFIGURATION PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal configurations tested: {len(config_metrics)}\")\n",
    "print(f\"Total test cases per config: {len(simulated_lcs)}\")\n",
    "print(f\"\\nTop 10 configs by LOWEST false positives:\")\n",
    "print(config_metrics_sorted[['config', 'grid_size', 'total_fp', 'dip_fp', 'jump_fp', \n",
    "                              'overall_precision', 'overall_recall', 'overall_f1', 'elapsed_s']].head(10))\n",
    "\n",
    "print(f\"\\n\\nTop 10 configs by HIGHEST false positives (worst performers):\")\n",
    "print(config_metrics.sort_values('total_fp', ascending=False)[['config', 'grid_size', 'total_fp', 'dip_fp', 'jump_fp', \n",
    "                                                                 'overall_precision', 'overall_recall', 'overall_f1', 'elapsed_s']].head(10))\n",
    "\n",
    "config_metrics_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed false positive analysis\n",
    "print(\"=\"*80)\n",
    "print(\"FALSE POSITIVE DEEP DIVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Which light curves trigger the most false positives across all configs?\n",
    "fp_by_case = df_metrics[df_metrics['any_fp'] > 0].groupby('case').agg({\n",
    "    'any_fp': 'sum',  # How many configs falsely detected this case\n",
    "    'truth': 'first',\n",
    "}).sort_values('any_fp', ascending=False)\n",
    "\n",
    "print(f\"\\nLight curves with most false positive detections (across all {len(config_metrics)} configs):\")\n",
    "print(fp_by_case.head(20))\n",
    "\n",
    "# Which types of events are most prone to false positives?\n",
    "fp_by_truth = df_metrics[df_metrics['any_fp'] > 0].groupby('truth').agg({\n",
    "    'any_fp': 'sum',\n",
    "    'dip_fp': 'sum',\n",
    "    'jump_fp': 'sum',\n",
    "}).sort_values('any_fp', ascending=False)\n",
    "\n",
    "print(f\"\\n\\nFalse positives by ground truth category:\")\n",
    "print(fp_by_truth)\n",
    "\n",
    "# For baseline config specifically\n",
    "baseline_fps = df_metrics[df_metrics['config'] == 'baseline_80x60']\n",
    "baseline_fp_cases = baseline_fps[baseline_fps['any_fp'] > 0].sort_values('case')\n",
    "\n",
    "print(f\"\\n\\nBaseline (80x60) false positives: {baseline_fps['any_fp'].sum()} total\")\n",
    "print(f\"Breakdown:\")\n",
    "print(f\"  Dip false positives: {baseline_fps['dip_fp'].sum()}\")\n",
    "print(f\"  Jump false positives: {baseline_fps['jump_fp'].sum()}\")\n",
    "\n",
    "if len(baseline_fp_cases) > 0:\n",
    "    print(f\"\\nBaseline false positive cases ({len(baseline_fp_cases)} cases):\")\n",
    "    print(baseline_fp_cases[['case', 'truth', 'dip_fp', 'jump_fp', 'dip_bf', 'jump_bf']].head(15))\n",
    "else:\n",
    "    print(\"\\n✓ Baseline has ZERO false positives - excellent!\")\n",
    "\n",
    "# Compare false positive rates across grid size ranges\n",
    "config_metrics['size_category'] = pd.cut(config_metrics['grid_size'], \n",
    "                                          bins=[0, 100, 500, 1500, 3000, 15000],\n",
    "                                          labels=['tiny (<100)', 'small (100-500)', 'medium (500-1500)', \n",
    "                                                  'large (1500-3000)', 'very large (>3000)'])\n",
    "\n",
    "fp_by_size = config_metrics.groupby('size_category').agg({\n",
    "    'total_fp': 'mean',\n",
    "    'dip_fp': 'mean',\n",
    "    'jump_fp': 'mean',\n",
    "    'overall_precision': 'mean',\n",
    "    'overall_recall': 'mean',\n",
    "    'elapsed_s': 'mean',\n",
    "    'config': 'count',\n",
    "})\n",
    "\n",
    "print(f\"\\n\\nFalse positive rates by grid size category:\")\n",
    "print(fp_by_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualizations emphasizing false positives\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 20))\n",
    "\n",
    "# Plot 1: Grid size vs False Positives\n",
    "ax = axes[0, 0]\n",
    "scatter = ax.scatter(config_metrics['grid_size'], config_metrics['total_fp'], \n",
    "                     s=120, alpha=0.6, c=config_metrics['elapsed_s'], cmap='viridis')\n",
    "ax.set_xlabel('Grid Size (p_points × mag_points)', fontsize=12)\n",
    "ax.set_ylabel('Total False Positives', fontsize=12)\n",
    "ax.set_title('Grid Size vs False Positive Count', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax, label='Avg Time (s)')\n",
    "# Annotate baseline\n",
    "baseline_row = config_metrics[config_metrics['config'] == 'baseline_80x60'].iloc[0]\n",
    "ax.annotate('baseline\\n80×60', (baseline_row['grid_size'], baseline_row['total_fp']),\n",
    "            fontsize=10, fontweight='bold', color='red',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "\n",
    "# Plot 2: Precision vs Recall (colored by FP count)\n",
    "ax = axes[0, 1]\n",
    "scatter = ax.scatter(config_metrics['overall_recall'], config_metrics['overall_precision'],\n",
    "                     s=120, alpha=0.6, c=config_metrics['total_fp'], cmap='Reds_r')\n",
    "ax.set_xlabel('Recall (Sensitivity)', fontsize=12)\n",
    "ax.set_ylabel('Precision (1 - FPR)', fontsize=12)\n",
    "ax.set_title('Precision-Recall Trade-off\\n(color = false positives, darker = worse)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0.0, 1.05])\n",
    "ax.set_ylim([0.85, 1.05])\n",
    "plt.colorbar(scatter, ax=ax, label='False Positives')\n",
    "ax.annotate('baseline', \n",
    "            (baseline_row['overall_recall'], baseline_row['overall_precision']),\n",
    "            fontsize=9, color='red')\n",
    "\n",
    "# Plot 3: Dip vs Jump False Positives\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(config_metrics['dip_fp'], config_metrics['jump_fp'], \n",
    "           s=120, alpha=0.6, c=config_metrics['grid_size'], cmap='plasma')\n",
    "ax.set_xlabel('Dip False Positives', fontsize=12)\n",
    "ax.set_ylabel('Jump False Positives', fontsize=12)\n",
    "ax.set_title('Dip vs Jump False Positive Comparison', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.plot([0, ax.get_xlim()[1]], [0, ax.get_xlim()[1]], 'k--', alpha=0.3, label='Equal FP rate')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 4: Speed vs False Positives (Pareto frontier)\n",
    "ax = axes[1, 1]\n",
    "# Normalize to baseline speed\n",
    "baseline_time = baseline_row['elapsed_s']\n",
    "config_metrics['speedup'] = baseline_time / config_metrics['elapsed_s']\n",
    "scatter = ax.scatter(config_metrics['speedup'], config_metrics['total_fp'],\n",
    "                     s=120, alpha=0.6, c=config_metrics['overall_f1'], cmap='RdYlGn')\n",
    "ax.set_xlabel('Speedup vs Baseline', fontsize=12)\n",
    "ax.set_ylabel('Total False Positives', fontsize=12)\n",
    "ax.set_title('Speed vs False Positive Trade-off\\n(color = F1 score, green = better)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(baseline_row['total_fp'], color='red', linestyle='--', alpha=0.5, label='Baseline FP count')\n",
    "ax.axvline(1.0, color='red', linestyle='--', alpha=0.5, label='Baseline speed')\n",
    "ax.legend()\n",
    "plt.colorbar(scatter, ax=ax, label='F1 Score')\n",
    "\n",
    "# Plot 5: False Positive Rate by Grid Size Category\n",
    "ax = axes[2, 0]\n",
    "fp_by_size_plot = fp_by_size.reset_index()\n",
    "x_pos = np.arange(len(fp_by_size_plot))\n",
    "width = 0.35\n",
    "ax.bar(x_pos - width/2, fp_by_size_plot['dip_fp'], width, label='Dip FP', alpha=0.8, color='#d62728')\n",
    "ax.bar(x_pos + width/2, fp_by_size_plot['jump_fp'], width, label='Jump FP', alpha=0.8, color='#ff7f0e')\n",
    "ax.set_xlabel('Grid Size Category', fontsize=12)\n",
    "ax.set_ylabel('Average False Positives', fontsize=12)\n",
    "ax.set_title('False Positive Rates by Grid Size', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(fp_by_size_plot['size_category'], rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 6: ROC-style curve (FPR vs TPR for configs)\n",
    "ax = axes[2, 1]\n",
    "# Calculate TPR (True Positive Rate = Recall)\n",
    "config_metrics['tpr'] = config_metrics['overall_recall']\n",
    "config_metrics['fpr'] = config_metrics['total_fp'] / len(simulated_lcs)  # FP rate normalized\n",
    "\n",
    "scatter = ax.scatter(config_metrics['fpr'], config_metrics['tpr'],\n",
    "                     s=120, alpha=0.6, c=config_metrics['grid_size'], cmap='viridis')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "ax.set_title('ROC-style: Detection Rate vs False Positive Rate', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random classifier')\n",
    "# Annotate best performers (high TPR, low FPR)\n",
    "best_configs = config_metrics.nsmallest(5, 'fpr')\n",
    "for idx, row in best_configs.iterrows():\n",
    "    if row['tpr'] > 0.90:  # Only annotate if high recall\n",
    "        ax.annotate(row['config'].replace('_', ' ')[:15], \n",
    "                   (row['fpr'], row['tpr']),\n",
    "                   fontsize=7, alpha=0.7)\n",
    "ax.legend()\n",
    "plt.colorbar(scatter, ax=ax, label='Grid Size')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommendations based on false positive analysis\n",
    "print(\"=\"*80)\n",
    "print(\"RECOMMENDATIONS: OPTIMAL GRID CONFIGURATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find configs that are Pareto-optimal on multiple fronts\n",
    "# Criteria: low FP, high recall, high precision, fast speed\n",
    "\n",
    "# Normalize metrics to [0, 1] for comparison\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "metrics_for_scoring = config_metrics[['total_fp', 'overall_recall', 'overall_precision', \n",
    "                                       'elapsed_s', 'overall_f1']].copy()\n",
    "scaler = MinMaxScaler()\n",
    "metrics_normalized = pd.DataFrame(\n",
    "    scaler.fit_transform(metrics_for_scoring),\n",
    "    columns=metrics_for_scoring.columns,\n",
    "    index=config_metrics.index\n",
    ")\n",
    "\n",
    "# Composite score: minimize FP, minimize time, maximize recall/precision/F1\n",
    "# Weight FP heavily since that's the emphasis\n",
    "config_metrics['composite_score'] = (\n",
    "    -2.0 * metrics_normalized['total_fp'] +      # Weight FP 2x (lower is better)\n",
    "    -0.5 * metrics_normalized['elapsed_s'] +     # Speed matters but less\n",
    "    1.0 * metrics_normalized['overall_recall'] + # Maximize recall\n",
    "    1.5 * metrics_normalized['overall_precision'] + # Weight precision heavily\n",
    "    1.0 * metrics_normalized['overall_f1']       # Overall quality\n",
    ")\n",
    "\n",
    "top_configs = config_metrics.nlargest(10, 'composite_score')\n",
    "\n",
    "print(\"\\nTop 10 overall configurations (balanced scoring with emphasis on low FP):\")\n",
    "print(top_configs[['config', 'grid_size', 'total_fp', 'dip_fp', 'jump_fp',\n",
    "                   'overall_precision', 'overall_recall', 'overall_f1', \n",
    "                   'elapsed_s', 'speedup', 'composite_score']].to_string())\n",
    "\n",
    "# Best config for MINIMUM false positives (regardless of other factors)\n",
    "best_fp = config_metrics.nsmallest(1, 'total_fp').iloc[0]\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"BEST FOR MINIMUM FALSE POSITIVES:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Config: {best_fp['config']}\")\n",
    "print(f\"Grid size: {int(best_fp['p_points'])}×{int(best_fp['mag_points'])} = {int(best_fp['grid_size'])} points\")\n",
    "print(f\"False positives: {int(best_fp['total_fp'])} (dip: {int(best_fp['dip_fp'])}, jump: {int(best_fp['jump_fp'])})\")\n",
    "print(f\"Precision: {best_fp['overall_precision']:.4f}\")\n",
    "print(f\"Recall: {best_fp['overall_recall']:.4f}\")\n",
    "print(f\"F1 Score: {best_fp['overall_f1']:.4f}\")\n",
    "print(f\"Speed: {best_fp['elapsed_s']:.4f}s (speedup: {best_fp['speedup']:.2f}x)\")\n",
    "\n",
    "# Best config balancing speed and low FP\n",
    "fast_accurate = config_metrics[\n",
    "    (config_metrics['speedup'] > 2.0) &  # At least 2x faster\n",
    "    (config_metrics['overall_recall'] > 0.90)  # High recall\n",
    "].nsmallest(1, 'total_fp')\n",
    "\n",
    "if len(fast_accurate) > 0:\n",
    "    best_fast = fast_accurate.iloc[0]\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(\"BEST FAST CONFIG (>2x speedup, high recall, low FP):\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Config: {best_fast['config']}\")\n",
    "    print(f\"Grid size: {int(best_fast['p_points'])}×{int(best_fast['mag_points'])} = {int(best_fast['grid_size'])} points\")\n",
    "    print(f\"False positives: {int(best_fast['total_fp'])} (dip: {int(best_fast['dip_fp'])}, jump: {int(best_fast['jump_fp'])})\")\n",
    "    print(f\"Precision: {best_fast['overall_precision']:.4f}\")\n",
    "    print(f\"Recall: {best_fast['overall_recall']:.4f}\")\n",
    "    print(f\"F1 Score: {best_fast['overall_f1']:.4f}\")\n",
    "    print(f\"Speed: {best_fast['elapsed_s']:.4f}s (speedup: {best_fast['speedup']:.2f}x)\")\n",
    "\n",
    "# Baseline comparison\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"BASELINE (80×60) PERFORMANCE:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"False positives: {int(baseline_row['total_fp'])} (dip: {int(baseline_row['dip_fp'])}, jump: {int(baseline_row['jump_fp'])})\")\n",
    "print(f\"Precision: {baseline_row['overall_precision']:.4f}\")\n",
    "print(f\"Recall: {baseline_row['overall_recall']:.4f}\")\n",
    "print(f\"F1 Score: {baseline_row['overall_f1']:.4f}\")\n",
    "print(f\"Speed: {baseline_row['elapsed_s']:.4f}s (reference)\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"SUMMARY STATISTICS ACROSS ALL CONFIGS:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total configs tested: {len(config_metrics)}\")\n",
    "print(f\"Grid sizes: {int(config_metrics['grid_size'].min())} to {int(config_metrics['grid_size'].max())}\")\n",
    "print(f\"\\nFalse Positives:\")\n",
    "print(f\"  Min: {int(config_metrics['total_fp'].min())}\")\n",
    "print(f\"  Max: {int(config_metrics['total_fp'].max())}\")\n",
    "print(f\"  Mean: {config_metrics['total_fp'].mean():.1f}\")\n",
    "print(f\"  Median: {config_metrics['total_fp'].median():.1f}\")\n",
    "print(f\"\\nPrecision:\")\n",
    "print(f\"  Min: {config_metrics['overall_precision'].min():.4f}\")\n",
    "print(f\"  Max: {config_metrics['overall_precision'].max():.4f}\")\n",
    "print(f\"  Mean: {config_metrics['overall_precision'].mean():.4f}\")\n",
    "print(f\"\\nRecall:\")\n",
    "print(f\"  Min: {config_metrics['overall_recall'].min():.4f}\")\n",
    "print(f\"  Max: {config_metrics['overall_recall'].max():.4f}\")\n",
    "print(f\"  Mean: {config_metrics['overall_recall'].mean():.4f}\")\n",
    "print(f\"\\nSpeed (relative to baseline):\")\n",
    "print(f\"  Fastest: {config_metrics['speedup'].max():.2f}x\")\n",
    "print(f\"  Slowest: {config_metrics['speedup'].min():.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify problematic light curves that consistently trigger false positives\n",
    "print(\"=\"*80)\n",
    "print(\"PROBLEMATIC LIGHT CURVES (HIGH FALSE POSITIVE TRIGGERS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Count how many configs trigger FP on each light curve\n",
    "fp_trigger_counts = df_metrics[df_metrics['any_fp'] > 0].groupby('case').size().sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nLight curves that trigger false positives in >50% of configs:\")\n",
    "threshold = len(config_metrics) * 0.5\n",
    "problematic_lcs = fp_trigger_counts[fp_trigger_counts > threshold]\n",
    "\n",
    "if len(problematic_lcs) > 0:\n",
    "    print(f\"Found {len(problematic_lcs)} problematic cases:\\n\")\n",
    "    for lc_name, fp_count in problematic_lcs.items():\n",
    "        pct = (fp_count / len(config_metrics)) * 100\n",
    "        truth = ground_truth[lc_name]\n",
    "        print(f\"  {lc_name:50s} | Truth: {truth:15s} | FP in {fp_count:2d}/{len(config_metrics)} configs ({pct:.1f}%)\")\n",
    "    \n",
    "    # Show details for most problematic case\n",
    "    worst_case = problematic_lcs.index[0]\n",
    "    print(f\"\\n\\nDetails for most problematic case: {worst_case}\")\n",
    "    print(f\"Ground truth: {ground_truth[worst_case]}\")\n",
    "    \n",
    "    case_results = df_metrics[df_metrics['case'] == worst_case]\n",
    "    fp_configs = case_results[case_results['any_fp'] > 0].sort_values('config')\n",
    "    \n",
    "    print(f\"\\nConfigs that falsely detected ({len(fp_configs)}/{len(config_metrics)}):\")\n",
    "    print(fp_configs[['config', 'dip_fp', 'jump_fp', 'dip_bf', 'jump_bf']].head(15))\n",
    "    \n",
    "else:\n",
    "    print(\"\\n✓ No consistently problematic light curves found!\")\n",
    "    print(\"  All false positives are isolated to specific configs.\\n\")\n",
    "\n",
    "# Identify light curves that NEVER trigger false positives (most robust test cases)\n",
    "never_fp = []\n",
    "for lc_name in simulated_lcs.keys():\n",
    "    case_fps = df_metrics[df_metrics['case'] == lc_name]['any_fp'].sum()\n",
    "    if case_fps == 0:\n",
    "        never_fp.append(lc_name)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ROBUST TEST CASES (never trigger false positives in any config):\")\n",
    "print(f\"{'='*80}\")\n",
    "if len(never_fp) > 0:\n",
    "    print(f\"Found {len(never_fp)} cases that never trigger FPs:\\n\")\n",
    "    for lc_name in never_fp[:20]:  # Show first 20\n",
    "        truth = ground_truth[lc_name]\n",
    "        print(f\"  {lc_name:50s} | Truth: {truth}\")\n",
    "    if len(never_fp) > 20:\n",
    "        print(f\"  ... and {len(never_fp) - 20} more\")\n",
    "else:\n",
    "    print(\"All light curves trigger at least one false positive in some configuration.\")\n",
    "\n",
    "# Analysis: Which event types are most/least prone to FP?\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FALSE POSITIVE SUSCEPTIBILITY BY EVENT TYPE:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "fp_by_event_type = {}\n",
    "for truth_type in set(ground_truth.values()):\n",
    "    matching_cases = [k for k, v in ground_truth.items() if v == truth_type]\n",
    "    total_possible_fps = len(matching_cases) * len(config_metrics)\n",
    "    actual_fps = df_metrics[df_metrics['case'].isin(matching_cases)]['any_fp'].sum()\n",
    "    fp_rate = (actual_fps / total_possible_fps) * 100 if total_possible_fps > 0 else 0\n",
    "    \n",
    "    fp_by_event_type[truth_type] = {\n",
    "        'n_cases': len(matching_cases),\n",
    "        'total_fps': actual_fps,\n",
    "        'fp_rate': fp_rate\n",
    "    }\n",
    "\n",
    "for event_type, stats in sorted(fp_by_event_type.items(), key=lambda x: x[1]['fp_rate'], reverse=True):\n",
    "    print(f\"\\n{event_type:20s}:\")\n",
    "    print(f\"  # cases: {stats['n_cases']}\")\n",
    "    print(f\"  Total FPs: {stats['total_fps']} / {stats['n_cases'] * len(config_metrics)} possible\")\n",
    "    print(f\"  FP rate: {stats['fp_rate']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on the false positive analysis above, you can now:\n",
    "\n",
    "1. **Choose your grid configuration** based on your priorities:\n",
    "   - **Minimum false positives**: Use the config identified in \"BEST FOR MINIMUM FALSE POSITIVES\"\n",
    "   - **Speed + accuracy balance**: Use the \"BEST FAST CONFIG\" recommendation\n",
    "   - **Custom criteria**: Review the top 10 configs and choose based on your specific needs\n",
    "\n",
    "2. **Update your pipeline** with the selected `p_points` and `mag_points` values\n",
    "\n",
    "3. **Investigate problematic cases**: If certain light curves consistently trigger false positives, consider:\n",
    "   - Adjusting significance thresholds\n",
    "   - Examining those specific light curve patterns in detail\n",
    "   - Adding additional filtering criteria\n",
    "\n",
    "4. **Validate on real data**: Test the selected configuration on real ASAS-SN light curves to confirm performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and run 65 extended grid configurations\n",
    "# From absurdly coarse (2x2) to fine (120x100)\n",
    "\n",
    "extended_grid_configs = []\n",
    "\n",
    "# Absurdly coarse (2x2 to 10x10)\n",
    "for n in [2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    extended_grid_configs.append({\"label\": f\"absurd_{n}x{n}\", \"p_points\": n, \"mag_points\": n})\n",
    "\n",
    "# Ultra coarse (12x12 to 20x20)\n",
    "for n in [12, 15, 18, 20]:\n",
    "    extended_grid_configs.append({\"label\": f\"ultra_coarse_{n}x{n}\", \"p_points\": n, \"mag_points\": n})\n",
    "\n",
    "# Very coarse (22x22 to 35x35)\n",
    "for n in [22, 25, 28, 30, 35]:\n",
    "    extended_grid_configs.append({\"label\": f\"very_coarse_{n}x{n}\", \"p_points\": n, \"mag_points\": n})\n",
    "\n",
    "# Coarse (asymmetric versions)\n",
    "for p, m in [(20, 15), (25, 20), (30, 20), (30, 25), (35, 25), (40, 25), (40, 30), (45, 30), (50, 35)]:\n",
    "    extended_grid_configs.append({\"label\": f\"coarse_{p}x{m}\", \"p_points\": p, \"mag_points\": m})\n",
    "\n",
    "# Medium (50x40 to 70x60)\n",
    "for p, m in [(50, 40), (55, 45), (60, 45), (60, 50), (65, 50), (70, 50), (70, 60)]:\n",
    "    extended_grid_configs.append({\"label\": f\"medium_{p}x{m}\", \"p_points\": p, \"mag_points\": m})\n",
    "\n",
    "# Baseline and near-baseline\n",
    "extended_grid_configs.append({\"label\": \"baseline_80x60\", \"p_points\": 80, \"mag_points\": 60})\n",
    "for p, m in [(75, 55), (85, 65), (90, 65), (90, 70)]:\n",
    "    extended_grid_configs.append({\"label\": f\"near_base_{p}x{m}\", \"p_points\": p, \"mag_points\": m})\n",
    "\n",
    "# Fine (95x70 to 120x100)\n",
    "for p, m in [(95, 70), (100, 75), (100, 80), (105, 80), (110, 85), (110, 90), (115, 90), (120, 90), (120, 100)]:\n",
    "    extended_grid_configs.append({\"label\": f\"fine_{p}x{m}\", \"p_points\": p, \"mag_points\": m})\n",
    "\n",
    "print(f\"Total configurations: {len(extended_grid_configs)}\")\n",
    "print(f\"Grid sizes range from {min(c['p_points']*c['mag_points'] for c in extended_grid_configs)} to {max(c['p_points']*c['mag_points'] for c in extended_grid_configs)}\")\n",
    "print(f\"\\nSample configs: {extended_grid_configs[:5]}\")\n",
    "\n",
    "# Run all configurations on real light curves\n",
    "N_LC_EXTENDED = 3\n",
    "subset_paths_extended = lc_paths[:N_LC_EXTENDED]\n",
    "\n",
    "extended_rows = []\n",
    "print(f\"\\nRunning {len(extended_grid_configs)} configs on {N_LC_EXTENDED} light curves...\")\n",
    "for i, path in enumerate(subset_paths_extended):\n",
    "    print(f\"  Light curve {i+1}/{N_LC_EXTENDED}: {path.name}\")\n",
    "    for j, cfg in enumerate(extended_grid_configs):\n",
    "        if (j + 1) % 10 == 0:\n",
    "            print(f\"    Config {j+1}/{len(extended_grid_configs)}\")\n",
    "        try:\n",
    "            extended_rows.append(run_grid_setting(path, cfg[\"p_points\"], cfg[\"mag_points\"], cfg[\"label\"]))\n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR on {cfg['label']}: {e}\")\n",
    "\n",
    "df_extended = pd.DataFrame(extended_rows)\n",
    "print(f\"\\nCompleted: {len(df_extended)} results\")\n",
    "df_extended.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze extended grid results relative to baseline\n",
    "baseline_label = \"baseline_80x60\"\n",
    "\n",
    "# Compute average metrics across all light curves for each config\n",
    "summary = df_extended.groupby(\"config\").agg({\n",
    "    \"elapsed_s\": \"mean\",\n",
    "    \"p_points\": \"first\",\n",
    "    \"mag_points\": \"first\",\n",
    "    \"dip_sig\": \"sum\",  # Count how many were significant\n",
    "    \"jump_sig\": \"sum\",\n",
    "    \"dip_bf\": \"mean\",\n",
    "    \"jump_bf\": \"mean\",\n",
    "}).reset_index()\n",
    "\n",
    "# Get baseline values\n",
    "baseline = summary[summary[\"config\"] == baseline_label].iloc[0]\n",
    "\n",
    "# Compute relative metrics\n",
    "summary[\"speedup\"] = baseline[\"elapsed_s\"] / summary[\"elapsed_s\"]\n",
    "summary[\"dip_bf_ratio\"] = summary[\"dip_bf\"] / baseline[\"dip_bf\"]\n",
    "summary[\"jump_bf_ratio\"] = summary[\"jump_bf\"] / baseline[\"jump_bf\"]\n",
    "summary[\"grid_size\"] = summary[\"p_points\"] * summary[\"mag_points\"]\n",
    "\n",
    "# Sort by speed (fastest first)\n",
    "summary_sorted = summary.sort_values(\"elapsed_s\")\n",
    "\n",
    "print(f\"\\\\nBaseline (80x60): {baseline['elapsed_s']:.3f}s average\")\n",
    "print(f\"\\\\nTop 10 fastest configurations:\")\n",
    "print(summary_sorted[[\"config\", \"p_points\", \"mag_points\", \"grid_size\", \"elapsed_s\", \"speedup\", \n",
    "                       \"dip_sig\", \"jump_sig\", \"dip_bf_ratio\", \"jump_bf_ratio\"]].head(10))\n",
    "\n",
    "print(f\"\\\\n\\\\nFull ranking by speed:\")\n",
    "summary_sorted[[\"config\", \"elapsed_s\", \"speedup\", \"dip_sig\", \"jump_sig\", \"dip_bf_ratio\", \"jump_bf_ratio\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize speed vs accuracy tradeoff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Grid size vs elapsed time\n",
    "axes[0, 0].scatter(summary[\"grid_size\"], summary[\"elapsed_s\"], s=100, alpha=0.6, c=summary[\"speedup\"], cmap=\"viridis\")\n",
    "axes[0, 0].set_xlabel(\"Grid Size (p_points × mag_points)\")\n",
    "axes[0, 0].set_ylabel(\"Elapsed Time (s)\")\n",
    "axes[0, 0].set_title(\"Grid Size vs Speed\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "# Annotate a few key points\n",
    "for idx, row in summary.iterrows():\n",
    "    if row[\"config\"] in [\"ultra_coarse_10x10\", \"coarse_30x30\", \"coarse_40x30\", \"baseline_80x60\"]:\n",
    "        axes[0, 0].annotate(row[\"config\"].replace(\"_\", \" \"), \n",
    "                           (row[\"grid_size\"], row[\"elapsed_s\"]),\n",
    "                           fontsize=8, alpha=0.7)\n",
    "\n",
    "# Plot 2: Speedup vs Bayes factor accuracy (dip)\n",
    "axes[0, 1].scatter(summary[\"speedup\"], summary[\"dip_bf_ratio\"], s=100, alpha=0.6, c=summary[\"grid_size\"], cmap=\"plasma\")\n",
    "axes[0, 1].axhline(1.0, color='red', linestyle='--', alpha=0.5, label=\"Perfect accuracy\")\n",
    "axes[0, 1].axhline(0.99, color='orange', linestyle='--', alpha=0.5, label=\"99% accuracy\")\n",
    "axes[0, 1].set_xlabel(\"Speedup vs Baseline\")\n",
    "axes[0, 1].set_ylabel(\"Dip Bayes Factor Ratio\")\n",
    "axes[0, 1].set_title(\"Speed vs Dip BF Accuracy\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Speedup vs Bayes factor accuracy (jump)\n",
    "axes[1, 0].scatter(summary[\"speedup\"], summary[\"jump_bf_ratio\"], s=100, alpha=0.6, c=summary[\"grid_size\"], cmap=\"plasma\")\n",
    "axes[1, 0].axhline(1.0, color='red', linestyle='--', alpha=0.5, label=\"Perfect accuracy\")\n",
    "axes[1, 0].axhline(0.90, color='orange', linestyle='--', alpha=0.5, label=\"90% accuracy\")\n",
    "axes[1, 0].set_xlabel(\"Speedup vs Baseline\")\n",
    "axes[1, 0].set_ylabel(\"Jump Bayes Factor Ratio\")\n",
    "axes[1, 0].set_title(\"Speed vs Jump BF Accuracy\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Pareto frontier (speedup vs combined BF error)\n",
    "summary[\"bf_error\"] = np.abs(1 - summary[\"dip_bf_ratio\"]) + np.abs(1 - summary[\"jump_bf_ratio\"])\n",
    "axes[1, 1].scatter(summary[\"speedup\"], summary[\"bf_error\"], s=100, alpha=0.6, c=summary[\"grid_size\"], cmap=\"coolwarm\")\n",
    "axes[1, 1].set_xlabel(\"Speedup vs Baseline\")\n",
    "axes[1, 1].set_ylabel(\"Combined BF Error (lower is better)\")\n",
    "axes[1, 1].set_title(\"Speed vs Combined Accuracy\")\n",
    "axes[1, 1].set_yscale(\"log\")\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "# Annotate sweet spot\n",
    "for idx, row in summary.iterrows():\n",
    "    if row[\"config\"] in [\"coarse_30x30\", \"coarse_40x30\", \"baseline_80x60\"]:\n",
    "        axes[1, 1].annotate(row[\"config\"].replace(\"_\", \" \"), \n",
    "                           (row[\"speedup\"], row[\"bf_error\"]),\n",
    "                           fontsize=8, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify the \"sweet spot\" configurations\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"SWEET SPOT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\\\nConfigurations with >3x speedup and <5% BF error:\")\n",
    "sweet_spot = summary[(summary[\"speedup\"] > 3) & (summary[\"bf_error\"] < 0.05)]\n",
    "if len(sweet_spot) > 0:\n",
    "    print(sweet_spot[[\"config\", \"p_points\", \"mag_points\", \"speedup\", \"elapsed_s\", \"bf_error\"]].sort_values(\"speedup\", ascending=False))\n",
    "else:\n",
    "    print(\"No configurations meet these criteria. Relaxing to >2x speedup and <10% error:\")\n",
    "    sweet_spot = summary[(summary[\"speedup\"] > 2) & (summary[\"bf_error\"] < 0.10)]\n",
    "    print(sweet_spot[[\"config\", \"p_points\", \"mag_points\", \"speedup\", \"elapsed_s\", \"bf_error\"]].sort_values(\"speedup\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Simulated dips and jumps\n",
    "Create a small suite of synthetic light curves (reusing a real SkyPatrol cadence) with injected dips, jumps, and a mixed case so grid sensitivity can be tested on known events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build simulated light curves by injecting analytic events into a real cadence\n",
    "from malca import events\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "base_sim_path = lc_paths[0]\n",
    "df_sim_base_raw = read_skypatrol_csv(base_sim_path)\n",
    "mask = (\n",
    "    df_sim_base_raw[\"JD\"].pipe(np.isfinite)\n",
    "    & df_sim_base_raw[\"mag\"].pipe(np.isfinite)\n",
    "    & df_sim_base_raw[\"error\"].pipe(np.isfinite)\n",
    "    & (df_sim_base_raw[\"error\"] > 0)\n",
    "    & (df_sim_base_raw[\"error\"] < 10)\n",
    ")\n",
    "df_sim_base = df_sim_base_raw[mask].copy()\n",
    "df_sim_base = events.clean_lc(df_sim_base)\n",
    "\n",
    "jd_med = float(df_sim_base[\"JD\"].median())\n",
    "\n",
    "\n",
    "def inject_event(df_in, kind=\"dip\", shape=\"gaussian\", amp=0.25, width=25.0, t0_offset=0.0):\n",
    "    # Return a copy with an injected event (positive amp = dip, negative amp = jump).\n",
    "    df = df_in.copy()\n",
    "    t0 = jd_med + float(t0_offset)\n",
    "    amp_signed = float(amp) if kind == \"dip\" else -float(amp)\n",
    "    t_arr = df[\"JD\"].to_numpy(float)\n",
    "    if shape == \"gaussian\":\n",
    "        delta = events.gaussian(t_arr, amp_signed, t0, float(width), 0.0)\n",
    "    elif shape == \"paczynski\":\n",
    "        delta = events.paczynski(t_arr, amp_signed, t0, float(width), 0.0)\n",
    "    else:\n",
    "        raise ValueError(\"shape must be gaussian or paczynski\")\n",
    "    df[\"mag\"] = df[\"mag\"].to_numpy(float) + delta\n",
    "    return df\n",
    "\n",
    "\n",
    "def inject_mixed(df_in):\n",
    "    df = inject_event(df_in, kind=\"dip\", shape=\"gaussian\", amp=0.18, width=20.0, t0_offset=-60.0)\n",
    "    df = inject_event(df, kind=\"jump\", shape=\"paczynski\", amp=0.14, width=30.0, t0_offset=40.0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_noise_and_gaps(df_in, jitter_mag=0.02, gap_frac=0.15, spike_amp=0.25, spike_width=0.08, spike_count=3):\n",
    "    # Add photometric jitter, drop random points, and sprinkle outliers to make messy cases.\n",
    "    df = df_in.copy()\n",
    "    n = len(df)\n",
    "    df[\"mag\"] = df[\"mag\"].to_numpy(float) + rng.normal(0.0, jitter_mag, n)\n",
    "    df[\"error\"] = (df[\"error\"].to_numpy(float) * (1 + rng.normal(0.0, 0.05, n))).clip(min=1e-3)\n",
    "    keep = rng.random(n) > gap_frac\n",
    "    df = df.loc[keep].copy().reset_index(drop=True)\n",
    "    if len(df) == 0:\n",
    "        return df_in.copy()\n",
    "    for _ in range(int(spike_count)):\n",
    "        idx = int(rng.integers(0, len(df)))\n",
    "        df.loc[idx, \"mag\"] += rng.normal(spike_amp, spike_width)\n",
    "    df = df.sort_values(\"JD\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "simulated_lcs = {\n",
    "    \"dip_gaussian\": inject_event(df_sim_base, kind=\"dip\", shape=\"gaussian\", amp=0.22, width=25.0, t0_offset=-30.0),\n",
    "    \"dip_paczynski\": inject_event(df_sim_base, kind=\"dip\", shape=\"paczynski\", amp=0.28, width=18.0, t0_offset=20.0),\n",
    "    \"dip_shallow_fast\": inject_event(df_sim_base, kind=\"dip\", shape=\"gaussian\", amp=0.12, width=8.0, t0_offset=-10.0),\n",
    "    \"dip_double\": inject_event(inject_event(df_sim_base, kind=\"dip\", shape=\"gaussian\", amp=0.18, width=15.0, t0_offset=-50.0), kind=\"dip\", shape=\"paczynski\", amp=0.15, width=12.0, t0_offset=25.0),\n",
    "    \"jump_gaussian\": inject_event(df_sim_base, kind=\"jump\", shape=\"gaussian\", amp=0.20, width=22.0, t0_offset=-15.0),\n",
    "    \"jump_paczynski\": inject_event(df_sim_base, kind=\"jump\", shape=\"paczynski\", amp=0.24, width=16.0, t0_offset=45.0),\n",
    "    \"microlens_weak\": inject_event(df_sim_base, kind=\"jump\", shape=\"paczynski\", amp=0.10, width=10.0, t0_offset=5.0),\n",
    "    \"microlens_strong\": inject_event(df_sim_base, kind=\"jump\", shape=\"paczynski\", amp=0.30, width=26.0, t0_offset=70.0),\n",
    "    \"mixed\": inject_mixed(df_sim_base),\n",
    "    \"messy_dip\": add_noise_and_gaps(inject_event(df_sim_base, kind=\"dip\", shape=\"gaussian\", amp=0.20, width=18.0, t0_offset=-5.0), jitter_mag=0.03, gap_frac=0.2, spike_count=3),\n",
    "    \"messy_jump\": add_noise_and_gaps(inject_event(df_sim_base, kind=\"jump\", shape=\"paczynski\", amp=0.18, width=14.0, t0_offset=35.0), jitter_mag=0.03, gap_frac=0.2, spike_count=3),\n",
    "    \"messy_mixed\": add_noise_and_gaps(inject_mixed(df_sim_base), jitter_mag=0.04, gap_frac=0.25, spike_count=4),\n",
    "}\n",
    "\n",
    "print(f\"Simulated light curves: {list(simulated_lcs.keys())}\")\n",
    "display(df_sim_base.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": "from malca.baseline import per_camera_gp_baseline\n\n\ndef prepare_df_for_sim(df):\n    df_clean = events.clean_lc(df)\n    df_base = per_camera_gp_baseline(df_clean, **BASELINE_KWARGS)\n    baseline_mags = df_base[\"baseline\"].to_numpy(float) if \"baseline\" in df_base.columns else df_base[\"mag\"].to_numpy(float)\n    baseline_mag = float(np.nanmedian(baseline_mags))\n    mags_for_grid = df_base[\"mag\"].to_numpy(float) if \"mag\" in df_base.columns else df_clean[\"mag\"].to_numpy(float)\n\n    def baseline_precomputed(df_in, **kwargs):\n        return df_base\n\n    return df_clean, baseline_precomputed, baseline_mag, mags_for_grid\n\n\ndef run_grid_setting_sim(name, df, p_points, mag_points, label):\n    df_clean, baseline_fn, baseline_mag, mags_for_grid = prepare_df_for_sim(df)\n    mag_grid_dip = events.default_mag_grid(baseline_mag, mags_for_grid, \"dip\", n=mag_points)\n    mag_grid_jump = events.default_mag_grid(baseline_mag, mags_for_grid, \"jump\", n=mag_points)\n\n    start = time.perf_counter()\n    res = events.run_bayesian_significance(\n        df_clean,\n        baseline_func=baseline_fn,\n        baseline_kwargs={},\n        p_points=p_points,\n        mag_grid_dip=mag_grid_dip,\n        mag_grid_jump=mag_grid_jump,\n        trigger_mode=\"posterior_prob\",\n        logbf_threshold_dip=5.0,\n        logbf_threshold_jump=5.0,\n        significance_threshold=99.99997,\n        run_min_points=3,\n        run_allow_gap_points=1,\n        run_max_gap_days=None,\n        run_min_duration_days=None,\n        use_sigma_eff=True,\n        require_sigma_eff=True,\n        compute_event_prob=True,\n    )\n    elapsed = time.perf_counter() - start\n\n    return {\n        \"case\": name,\n        \"config\": label,\n        \"p_points\": p_points,\n        \"mag_points\": mag_points,\n        \"elapsed_s\": elapsed,\n        \"dip_sig\": res[\"dip\"][\"significant\"],\n        \"jump_sig\": res[\"jump\"][\"significant\"],\n        \"dip_bf\": res[\"dip\"][\"bayes_factor\"],\n        \"jump_bf\": res[\"jump\"][\"bayes_factor\"],\n        \"dip_best_p\": res[\"dip\"][\"best_p\"],\n        \"jump_best_p\": res[\"jump\"][\"best_p\"],\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep grid sizes on the simulated cases\n",
    "sim_grid_settings = [\n",
    "    {\"label\": \"sim_10x10\", \"p_points\": 10, \"mag_points\": 10},\n",
    "    {\"label\": \"sim_25x25\", \"p_points\": 25, \"mag_points\": 25},\n",
    "    {\"label\": \"sim_50x50\", \"p_points\": 50, \"mag_points\": 50},\n",
    "    {\"label\": \"sim_80x60\", \"p_points\": 80, \"mag_points\": 60},\n",
    "]\n",
    "\n",
    "sim_rows = []\n",
    "for name, df_sim in simulated_lcs.items():\n",
    "    for cfg in sim_grid_settings:\n",
    "        sim_rows.append(run_grid_setting_sim(name, df_sim, cfg[\"p_points\"], cfg[\"mag_points\"], cfg[\"label\"]))\n",
    "\n",
    "sim_results = pd.DataFrame(sim_rows)\n",
    "sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}