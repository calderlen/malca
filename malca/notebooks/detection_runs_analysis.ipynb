{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection Runs Analysis\n",
    "\n",
    "Comprehensive analysis and interpretation of all detection runs from `malca detect`.\n",
    "\n",
    "This notebook:\n",
    "1. Discovers all runs in `output/runs/`\n",
    "2. Reads `run_params.json` to extract mag bin and parameters\n",
    "3. Combines results across runs\n",
    "4. Provides detailed statistics and visualizations\n",
    "5. Identifies top candidates for follow-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Larger default figure size\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Discover Detection Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repo_root(start: Path) -> Path:\n",
    "    \"\"\"Find repository root by looking for pyproject.toml.\"\"\"\n",
    "    for p in (start, *start.parents):\n",
    "        if (p / \"pyproject.toml\").exists() and (p / \"malca\").is_dir():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd().resolve())\n",
    "RUNS_DIR = REPO_ROOT / \"output\" / \"runs\"\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Runs directory: {RUNS_DIR}\")\n",
    "print(f\"Runs directory exists: {RUNS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_run_metadata(run_dir: Path) -> dict:\n",
    "    \"\"\"Load run parameters and metadata from a run directory.\"\"\"\n",
    "    metadata = {\n",
    "        \"run_id\": run_dir.name,\n",
    "        \"run_path\": str(run_dir),\n",
    "        \"timestamp\": None,\n",
    "        \"mag_bin\": None,\n",
    "        \"n_sources\": 0,\n",
    "        \"params\": {},\n",
    "    }\n",
    "    \n",
    "    # Parse timestamp from directory name (format: YYYYMMDD_HHMMSS)\n",
    "    try:\n",
    "        ts_str = run_dir.name\n",
    "        metadata[\"timestamp\"] = datetime.strptime(ts_str, \"%Y%m%d_%H%M%S\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # Load run_params.json\n",
    "    params_file = run_dir / \"run_params.json\"\n",
    "    if params_file.exists():\n",
    "        with open(params_file) as f:\n",
    "            params = json.load(f)\n",
    "            metadata[\"params\"] = params\n",
    "            metadata[\"mag_bin\"] = params.get(\"mag_bin\", params.get(\"mag_bins\", [\"unknown\"]))\n",
    "            if isinstance(metadata[\"mag_bin\"], list):\n",
    "                metadata[\"mag_bin\"] = metadata[\"mag_bin\"][0] if metadata[\"mag_bin\"] else \"unknown\"\n",
    "    \n",
    "    # Check for results\n",
    "    results_dir = run_dir / \"results\"\n",
    "    if results_dir.exists():\n",
    "        result_files = list(results_dir.glob(\"*.csv\")) + list(results_dir.glob(\"*.parquet\"))\n",
    "        metadata[\"result_files\"] = [f.name for f in result_files]\n",
    "    else:\n",
    "        metadata[\"result_files\"] = []\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def discover_runs(runs_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Discover all detection runs and return summary DataFrame.\"\"\"\n",
    "    runs = []\n",
    "    \n",
    "    for run_dir in sorted(runs_dir.iterdir()):\n",
    "        if not run_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            metadata = load_run_metadata(run_dir)\n",
    "            runs.append(metadata)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to load {run_dir}: {e}\")\n",
    "    \n",
    "    if not runs:\n",
    "        print(\"No runs found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(runs)\n",
    "    return df\n",
    "\n",
    "\n",
    "runs_df = discover_runs(RUNS_DIR)\n",
    "print(f\"\\nDiscovered {len(runs_df)} detection runs:\")\n",
    "runs_df[[\"run_id\", \"timestamp\", \"mag_bin\", \"result_files\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Combine Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_run_results(run_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load detection results from a run directory.\"\"\"\n",
    "    run_dir = Path(run_path)\n",
    "    results_dir = run_dir / \"results\"\n",
    "    \n",
    "    if not results_dir.exists():\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Try different result file patterns\n",
    "    result_files = (\n",
    "        list(results_dir.glob(\"lc_events_results_filtered.csv\")) +\n",
    "        list(results_dir.glob(\"*_filtered.csv\")) +\n",
    "        list(results_dir.glob(\"lc_events_results.csv\")) +\n",
    "        list(results_dir.glob(\"*.csv\"))\n",
    "    )\n",
    "    \n",
    "    if not result_files:\n",
    "        # Try parquet\n",
    "        result_files = list(results_dir.glob(\"*.parquet\"))\n",
    "    \n",
    "    if not result_files:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    result_file = result_files[0]\n",
    "    \n",
    "    if result_file.suffix == \".parquet\":\n",
    "        df = pd.read_parquet(result_file)\n",
    "    else:\n",
    "        df = pd.read_csv(result_file)\n",
    "    \n",
    "    df[\"run_id\"] = run_dir.name\n",
    "    df[\"source_file\"] = result_file.name\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Load all results\n",
    "all_results = []\n",
    "for _, run in runs_df.iterrows():\n",
    "    if run[\"result_files\"]:\n",
    "        df = load_run_results(run[\"run_path\"])\n",
    "        if not df.empty:\n",
    "            df[\"mag_bin\"] = run[\"mag_bin\"]\n",
    "            all_results.append(df)\n",
    "            print(f\"Loaded {len(df):,} rows from {run['run_id']} (mag_bin={run['mag_bin']})\")\n",
    "\n",
    "if all_results:\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(f\"\\n=== Combined Results ===\")\n",
    "    print(f\"Total rows: {len(combined_df):,}\")\n",
    "    print(f\"Unique runs: {combined_df['run_id'].nunique()}\")\n",
    "    print(f\"Unique mag bins: {combined_df['mag_bin'].nunique()}\")\n",
    "else:\n",
    "    combined_df = pd.DataFrame()\n",
    "    print(\"No results loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview of columns\n",
    "if not combined_df.empty:\n",
    "    print(\"Available columns:\")\n",
    "    for i, col in enumerate(combined_df.columns):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "else:\n",
    "    print(\"No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detection Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DETECTION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total light curves analyzed: {len(combined_df):,}\")\n",
    "    print()\n",
    "    \n",
    "    if 'dip_significant' in combined_df.columns:\n",
    "        n_dip = combined_df['dip_significant'].sum()\n",
    "        n_jump = combined_df['jump_significant'].sum()\n",
    "        n_either = (combined_df['dip_significant'] | combined_df['jump_significant']).sum()\n",
    "        n_both = (combined_df['dip_significant'] & combined_df['jump_significant']).sum()\n",
    "        \n",
    "        print(f\"Significant dip detections:   {n_dip:6,} ({n_dip/len(combined_df)*100:5.2f}%)\")\n",
    "        print(f\"Significant jump detections:  {n_jump:6,} ({n_jump/len(combined_df)*100:5.2f}%)\")\n",
    "        print(f\"Either dip or jump:           {n_either:6,} ({n_either/len(combined_df)*100:5.2f}%)\")\n",
    "        print(f\"Both dip and jump:            {n_both:6,} ({n_both/len(combined_df)*100:5.2f}%)\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=== Per Magnitude Bin ===\")\n",
    "    for mag_bin in sorted(combined_df['mag_bin'].unique()):\n",
    "        subset = combined_df[combined_df['mag_bin'] == mag_bin]\n",
    "        if 'dip_significant' in subset.columns:\n",
    "            n_sig = (subset['dip_significant'] | subset['jump_significant']).sum()\n",
    "            print(f\"  {mag_bin}: {len(subset):,} sources, {n_sig:,} significant ({n_sig/len(subset)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-filter statistics (if available)\n",
    "filter_cols = [c for c in combined_df.columns if c.startswith('failed_')]\n",
    "\n",
    "if filter_cols:\n",
    "    print(\"=== Post-Filter Failure Rates ===\")\n",
    "    for col in filter_cols:\n",
    "        n_failed = combined_df[col].sum()\n",
    "        filter_name = col.replace('failed_', '')\n",
    "        print(f\"  {filter_name:30s}: {n_failed:6,} / {len(combined_df):,} ({n_failed/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Count passing all filters\n",
    "    all_pass = ~combined_df[filter_cols].any(axis=1)\n",
    "    print(f\"\\n  Passed ALL filters: {all_pass.sum():,} ({all_pass.sum()/len(combined_df)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"No post-filter columns found (run with --run-post-filter to get these)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Light Curve Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty and 'n_points' in combined_df.columns:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Number of points\n",
    "    axes[0, 0].hist(combined_df['n_points'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Number of Points')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].set_title('Distribution of Light Curve Length')\n",
    "    axes[0, 0].axvline(combined_df['n_points'].median(), color='red', linestyle='--', \n",
    "                       label=f'Median: {combined_df[\"n_points\"].median():.0f}')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Time span\n",
    "    if 'jd_first' in combined_df.columns and 'jd_last' in combined_df.columns:\n",
    "        time_span = combined_df['jd_last'] - combined_df['jd_first']\n",
    "        axes[0, 1].hist(time_span, bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('Time Span (days)')\n",
    "        axes[0, 1].set_ylabel('Count')\n",
    "        axes[0, 1].set_title('Distribution of Observing Baseline')\n",
    "        axes[0, 1].axvline(time_span.median(), color='red', linestyle='--', \n",
    "                           label=f'Median: {time_span.median():.0f} days')\n",
    "        axes[0, 1].legend()\n",
    "    \n",
    "    # Cadence\n",
    "    if 'cadence_median_days' in combined_df.columns:\n",
    "        axes[0, 2].hist(combined_df['cadence_median_days'], bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[0, 2].set_xlabel('Median Cadence (days)')\n",
    "        axes[0, 2].set_ylabel('Count')\n",
    "        axes[0, 2].set_title('Distribution of Cadence')\n",
    "        axes[0, 2].axvline(combined_df['cadence_median_days'].median(), color='red', linestyle='--')\n",
    "    \n",
    "    # Number of cameras\n",
    "    if 'n_cameras' in combined_df.columns:\n",
    "        axes[1, 0].hist(combined_df['n_cameras'], bins=range(0, int(combined_df['n_cameras'].max())+2), \n",
    "                        edgecolor='black', alpha=0.7)\n",
    "        axes[1, 0].set_xlabel('Number of Cameras')\n",
    "        axes[1, 0].set_ylabel('Count')\n",
    "        axes[1, 0].set_title('Camera Coverage')\n",
    "    \n",
    "    # Points vs time span\n",
    "    if 'jd_first' in combined_df.columns:\n",
    "        axes[1, 1].scatter(combined_df['n_points'], time_span, alpha=0.1, s=5)\n",
    "        axes[1, 1].set_xlabel('Number of Points')\n",
    "        axes[1, 1].set_ylabel('Time Span (days)')\n",
    "        axes[1, 1].set_title('Points vs Time Span')\n",
    "    \n",
    "    # By mag bin\n",
    "    mag_bins = sorted(combined_df['mag_bin'].unique())\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(mag_bins)))\n",
    "    for i, mag_bin in enumerate(mag_bins):\n",
    "        subset = combined_df[combined_df['mag_bin'] == mag_bin]\n",
    "        axes[1, 2].hist(subset['n_points'], bins=50, alpha=0.5, label=mag_bin, color=colors[i])\n",
    "    axes[1, 2].set_xlabel('Number of Points')\n",
    "    axes[1, 2].set_ylabel('Count')\n",
    "    axes[1, 2].set_title('Points by Magnitude Bin')\n",
    "    axes[1, 2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detection Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty and 'dip_bayes_factor' in combined_df.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Bayes Factor distributions\n",
    "    dip_bf = combined_df['dip_bayes_factor'].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    jump_bf = combined_df['jump_bayes_factor'].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    # Only plot positive BF values on log scale\n",
    "    dip_bf_pos = dip_bf[dip_bf > 0]\n",
    "    jump_bf_pos = jump_bf[jump_bf > 0]\n",
    "    \n",
    "    axes[0, 0].hist(np.log10(dip_bf_pos), bins=50, alpha=0.6, label='Dip', edgecolor='black')\n",
    "    axes[0, 0].hist(np.log10(jump_bf_pos), bins=50, alpha=0.6, label='Jump', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('log₁₀(Bayes Factor)')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].set_title('Bayes Factor Distribution')\n",
    "    axes[0, 0].axvline(np.log10(10), color='red', linestyle='--', label='BF=10')\n",
    "    axes[0, 0].axvline(np.log10(100), color='orange', linestyle='--', label='BF=100')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Event probability\n",
    "    if 'dip_max_event_prob' in combined_df.columns:\n",
    "        axes[0, 1].hist(combined_df['dip_max_event_prob'].dropna(), bins=50, alpha=0.6, label='Dip')\n",
    "        axes[0, 1].hist(combined_df['jump_max_event_prob'].dropna(), bins=50, alpha=0.6, label='Jump')\n",
    "        axes[0, 1].set_xlabel('Max Event Probability')\n",
    "        axes[0, 1].set_ylabel('Count')\n",
    "        axes[0, 1].set_title('Event Probability Distribution')\n",
    "        axes[0, 1].axvline(0.5, color='red', linestyle='--', label='p=0.5')\n",
    "        axes[0, 1].legend()\n",
    "    \n",
    "    # Run counts\n",
    "    if 'dip_count' in combined_df.columns:\n",
    "        max_runs = max(combined_df['dip_count'].max(), combined_df['jump_count'].max())\n",
    "        bins = range(0, min(int(max_runs) + 2, 50))\n",
    "        axes[1, 0].hist(combined_df['dip_count'], bins=bins, alpha=0.6, label='Dip', edgecolor='black')\n",
    "        axes[1, 0].hist(combined_df['jump_count'], bins=bins, alpha=0.6, label='Jump', edgecolor='black')\n",
    "        axes[1, 0].set_xlabel('Event Count')\n",
    "        axes[1, 0].set_ylabel('Count')\n",
    "        axes[1, 0].set_title('Number of Events per Light Curve')\n",
    "        axes[1, 0].legend()\n",
    "    \n",
    "    # Scatter: BF vs event count\n",
    "    if 'dip_count' in combined_df.columns:\n",
    "        sig_mask = combined_df['dip_significant'] | combined_df['jump_significant']\n",
    "        axes[1, 1].scatter(\n",
    "            combined_df.loc[~sig_mask, 'dip_count'],\n",
    "            np.log10(combined_df.loc[~sig_mask, 'dip_bayes_factor'].clip(lower=0.1)),\n",
    "            alpha=0.1, s=5, label='Not significant', color='gray'\n",
    "        )\n",
    "        axes[1, 1].scatter(\n",
    "            combined_df.loc[sig_mask, 'dip_count'],\n",
    "            np.log10(combined_df.loc[sig_mask, 'dip_bayes_factor'].clip(lower=0.1)),\n",
    "            alpha=0.5, s=20, label='Significant', color='red'\n",
    "        )\n",
    "        axes[1, 1].set_xlabel('Dip Count')\n",
    "        axes[1, 1].set_ylabel('log₁₀(Dip Bayes Factor)')\n",
    "        axes[1, 1].set_title('Detection Strength vs Event Count')\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Morphology Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty and 'dip_best_morph' in combined_df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Dip morphologies (exclude 'none')\n",
    "    dip_morphs = combined_df[combined_df['dip_best_morph'] != 'none']['dip_best_morph'].value_counts()\n",
    "    if not dip_morphs.empty:\n",
    "        colors = plt.cm.Set2(np.linspace(0, 1, len(dip_morphs)))\n",
    "        bars = axes[0].bar(range(len(dip_morphs)), dip_morphs.values, color=colors, edgecolor='black')\n",
    "        axes[0].set_xticks(range(len(dip_morphs)))\n",
    "        axes[0].set_xticklabels(dip_morphs.index, rotation=45, ha='right')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].set_title(f'Dip Morphologies (n={dip_morphs.sum():,})')\n",
    "        \n",
    "        # Add count labels\n",
    "        for bar, count in zip(bars, dip_morphs.values):\n",
    "            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                        f'{count:,}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Jump morphologies\n",
    "    jump_morphs = combined_df[combined_df['jump_best_morph'] != 'none']['jump_best_morph'].value_counts()\n",
    "    if not jump_morphs.empty:\n",
    "        colors = plt.cm.Set2(np.linspace(0, 1, len(jump_morphs)))\n",
    "        bars = axes[1].bar(range(len(jump_morphs)), jump_morphs.values, color=colors, edgecolor='black')\n",
    "        axes[1].set_xticks(range(len(jump_morphs)))\n",
    "        axes[1].set_xticklabels(jump_morphs.index, rotation=45, ha='right')\n",
    "        axes[1].set_ylabel('Count')\n",
    "        axes[1].set_title(f'Jump Morphologies (n={jump_morphs.sum():,})')\n",
    "        \n",
    "        for bar, count in zip(bars, jump_morphs.values):\n",
    "            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                        f'{count:,}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n=== Morphology Summary ===\")\n",
    "    print(\"\\nDip morphologies:\")\n",
    "    for morph, count in dip_morphs.items():\n",
    "        print(f\"  {morph:15s}: {count:6,} ({count/dip_morphs.sum()*100:5.1f}%)\")\n",
    "    \n",
    "    print(\"\\nJump morphologies:\")\n",
    "    for morph, count in jump_morphs.items():\n",
    "        print(f\"  {morph:15s}: {count:6,} ({count/jump_morphs.sum()*100:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Top Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty and 'dip_bayes_factor' in combined_df.columns:\n",
    "    # Create combined score\n",
    "    combined_df['max_bayes_factor'] = combined_df[['dip_bayes_factor', 'jump_bayes_factor']].max(axis=1)\n",
    "    \n",
    "    # Filter to significant detections only\n",
    "    if 'dip_significant' in combined_df.columns:\n",
    "        significant = combined_df[combined_df['dip_significant'] | combined_df['jump_significant']].copy()\n",
    "    else:\n",
    "        significant = combined_df[combined_df['max_bayes_factor'] > 10].copy()\n",
    "    \n",
    "    print(f\"=== Top 20 Candidates by Bayes Factor ===\")\n",
    "    print(f\"(From {len(significant):,} significant detections)\\n\")\n",
    "    \n",
    "    # Select display columns\n",
    "    display_cols = ['path', 'mag_bin', 'dip_bayes_factor', 'jump_bayes_factor', \n",
    "                    'dip_best_morph', 'jump_best_morph', 'dip_count', 'jump_count']\n",
    "    display_cols = [c for c in display_cols if c in significant.columns]\n",
    "    \n",
    "    top_20 = significant.nlargest(20, 'max_bayes_factor')[display_cols]\n",
    "    \n",
    "    # Extract source ID from path\n",
    "    if 'path' in top_20.columns:\n",
    "        top_20['source_id'] = top_20['path'].str.extract(r'(\\d+)')[0]\n",
    "        top_20 = top_20.drop(columns=['path'])\n",
    "    \n",
    "    print(top_20.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidates by morphology (exclude noise)\n",
    "if not combined_df.empty and 'dip_best_morph' in combined_df.columns:\n",
    "    print(\"\\n=== Top Candidates by Morphology (Non-Noise) ===\")\n",
    "    \n",
    "    for morph_type in ['gaussian', 'paczynski', 'exponential', 'linear']:\n",
    "        dip_matches = combined_df[\n",
    "            (combined_df['dip_best_morph'] == morph_type) & \n",
    "            (combined_df['dip_significant'] == True)\n",
    "        ]\n",
    "        jump_matches = combined_df[\n",
    "            (combined_df['jump_best_morph'] == morph_type) & \n",
    "            (combined_df['jump_significant'] == True)\n",
    "        ]\n",
    "        \n",
    "        matches = pd.concat([dip_matches, jump_matches]).drop_duplicates()\n",
    "        \n",
    "        if not matches.empty:\n",
    "            print(f\"\\n{morph_type.upper()} morphology ({len(matches)} candidates):\")\n",
    "            top_5 = matches.nlargest(5, 'max_bayes_factor')\n",
    "            for _, row in top_5.iterrows():\n",
    "                source = row.get('path', 'unknown')\n",
    "                if isinstance(source, str) and '/' in source:\n",
    "                    source = source.split('/')[-1].replace('.csv', '').replace('.dat2', '')\n",
    "                bf = row['max_bayes_factor']\n",
    "                print(f\"  • {source}: BF={bf:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison Across Magnitude Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty and combined_df['mag_bin'].nunique() > 1:\n",
    "    print(\"=== Detection Rates by Magnitude Bin ===\")\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for mag_bin in sorted(combined_df['mag_bin'].unique()):\n",
    "        subset = combined_df[combined_df['mag_bin'] == mag_bin]\n",
    "        n_total = len(subset)\n",
    "        \n",
    "        if 'dip_significant' in subset.columns:\n",
    "            n_dip = subset['dip_significant'].sum()\n",
    "            n_jump = subset['jump_significant'].sum()\n",
    "            n_either = (subset['dip_significant'] | subset['jump_significant']).sum()\n",
    "        else:\n",
    "            n_dip = n_jump = n_either = 0\n",
    "        \n",
    "        summary_data.append({\n",
    "            'mag_bin': mag_bin,\n",
    "            'n_total': n_total,\n",
    "            'n_dip': n_dip,\n",
    "            'n_jump': n_jump,\n",
    "            'n_either': n_either,\n",
    "            'pct_dip': n_dip / n_total * 100 if n_total > 0 else 0,\n",
    "            'pct_jump': n_jump / n_total * 100 if n_total > 0 else 0,\n",
    "            'pct_either': n_either / n_total * 100 if n_total > 0 else 0,\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    x = range(len(summary_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0].bar([i - width/2 for i in x], summary_df['n_dip'], width, label='Dips', color='#e74c3c')\n",
    "    axes[0].bar([i + width/2 for i in x], summary_df['n_jump'], width, label='Jumps', color='#3498db')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(summary_df['mag_bin'], rotation=45, ha='right')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Detection Counts by Magnitude Bin')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].bar([i - width/2 for i in x], summary_df['pct_dip'], width, label='Dips', color='#e74c3c')\n",
    "    axes[1].bar([i + width/2 for i in x], summary_df['pct_jump'], width, label='Jumps', color='#3498db')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(summary_df['mag_bin'], rotation=45, ha='right')\n",
    "    axes[1].set_ylabel('Detection Rate (%)')\n",
    "    axes[1].set_title('Detection Rates by Magnitude Bin')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Only one magnitude bin found - no comparison available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Filter Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty and 'dip_bayes_factor' in combined_df.columns:\n",
    "    print(\"=== Detection Counts at Different Thresholds ===\")\n",
    "    print()\n",
    "    \n",
    "    bf_thresholds = [1, 3, 10, 30, 100, 1000]\n",
    "    print(\"Bayes Factor Thresholds:\")\n",
    "    for thresh in bf_thresholds:\n",
    "        n_dip = (combined_df['dip_bayes_factor'] > thresh).sum()\n",
    "        n_jump = (combined_df['jump_bayes_factor'] > thresh).sum()\n",
    "        n_either = ((combined_df['dip_bayes_factor'] > thresh) | \n",
    "                    (combined_df['jump_bayes_factor'] > thresh)).sum()\n",
    "        print(f\"  BF > {thresh:4d}: dip={n_dip:5,} jump={n_jump:5,} either={n_either:5,}\")\n",
    "    \n",
    "    if 'dip_max_event_prob' in combined_df.columns:\n",
    "        print(\"\\nEvent Probability Thresholds:\")\n",
    "        for thresh in [0.5, 0.7, 0.9, 0.95, 0.99]:\n",
    "            n_dip = (combined_df['dip_max_event_prob'] > thresh).sum()\n",
    "            n_jump = (combined_df['jump_max_event_prob'] > thresh).sum()\n",
    "            print(f\"  P > {thresh:.2f}: dip={n_dip:5,} jump={n_jump:5,}\")\n",
    "    \n",
    "    if 'dip_count' in combined_df.columns:\n",
    "        print(\"\\nEvent Count Requirements:\")\n",
    "        for min_count in [1, 2, 3, 5, 10]:\n",
    "            n_dip = (combined_df['dip_count'] >= min_count).sum()\n",
    "            n_jump = (combined_df['jump_count'] >= min_count).sum()\n",
    "            print(f\"  >= {min_count:2d} events: dip={n_dip:5,} jump={n_jump:5,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Filtered Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty:\n",
    "    # Define filtering criteria\n",
    "    criteria = (\n",
    "        (combined_df['max_bayes_factor'] > 100) &  # Strong detection\n",
    "        (\n",
    "            (combined_df['dip_best_morph'].isin(['gaussian', 'exponential', 'linear', 'paczynski'])) |\n",
    "            (combined_df['jump_best_morph'].isin(['gaussian', 'exponential', 'linear', 'paczynski']))\n",
    "        )  # Non-noise morphology\n",
    "    )\n",
    "    \n",
    "    high_quality = combined_df[criteria].copy()\n",
    "    \n",
    "    print(f\"=== High-Quality Candidates ===\")\n",
    "    print(f\"Criteria: BF > 100 AND non-noise morphology\")\n",
    "    print(f\"Found: {len(high_quality):,} candidates\")\n",
    "    \n",
    "    if not high_quality.empty:\n",
    "        # Save to output\n",
    "        output_path = REPO_ROOT / \"output\" / \"high_quality_candidates.csv\"\n",
    "        high_quality.to_csv(output_path, index=False)\n",
    "        print(f\"\\nSaved to: {output_path}\")\n",
    "        \n",
    "        # Show top 10\n",
    "        print(\"\\nTop 10:\")\n",
    "        display_cols = ['path', 'mag_bin', 'max_bayes_factor', 'dip_best_morph', 'jump_best_morph']\n",
    "        display_cols = [c for c in display_cols if c in high_quality.columns]\n",
    "        print(high_quality.nlargest(10, 'max_bayes_factor')[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run Parameters Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare parameters across runs\n",
    "if not runs_df.empty:\n",
    "    print(\"=== Run Parameters ===\")\n",
    "    \n",
    "    for _, run in runs_df.iterrows():\n",
    "        params = run.get('params', {})\n",
    "        if params:\n",
    "            print(f\"\\n{run['run_id']} (mag_bin={run['mag_bin']}):\")\n",
    "            # Print key parameters\n",
    "            key_params = [\n",
    "                'logbf_threshold_dip', 'logbf_threshold_jump',\n",
    "                'run_min_points', 'run_max_gap_days',\n",
    "                'baseline_func', 'workers'\n",
    "            ]\n",
    "            for param in key_params:\n",
    "                if param in params:\n",
    "                    print(f\"  {param}: {params[param]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nAnalyzed {len(runs_df)} detection runs\")\n",
    "    print(f\"Total light curves: {len(combined_df):,}\")\n",
    "    \n",
    "    if 'dip_significant' in combined_df.columns:\n",
    "        n_sig = (combined_df['dip_significant'] | combined_df['jump_significant']).sum()\n",
    "        print(f\"Significant detections: {n_sig:,} ({n_sig/len(combined_df)*100:.2f}%)\")\n",
    "    \n",
    "    if 'max_bayes_factor' in combined_df.columns:\n",
    "        print(f\"\\nTop detection strength: BF = {combined_df['max_bayes_factor'].max():,.1f}\")\n",
    "    \n",
    "    # Morphology breakdown for significant detections\n",
    "    if 'dip_best_morph' in combined_df.columns:\n",
    "        sig_mask = combined_df['dip_significant'] | combined_df['jump_significant']\n",
    "        sig_df = combined_df[sig_mask]\n",
    "        \n",
    "        dip_morphs = sig_df[sig_df['dip_significant']]['dip_best_morph'].value_counts()\n",
    "        jump_morphs = sig_df[sig_df['jump_significant']]['jump_best_morph'].value_counts()\n",
    "        \n",
    "        print(\"\\nMorphology of significant detections:\")\n",
    "        print(f\"  Dips - noise: {dip_morphs.get('noise', 0):,}, \" \n",
    "              f\"gaussian: {dip_morphs.get('gaussian', 0):,}, \"\n",
    "              f\"other: {(dip_morphs.sum() - dip_morphs.get('noise', 0) - dip_morphs.get('gaussian', 0)):,}\")\n",
    "        print(f\"  Jumps - noise: {jump_morphs.get('noise', 0):,}, \"\n",
    "              f\"paczynski: {jump_morphs.get('paczynski', 0):,}, \"\n",
    "              f\"other: {(jump_morphs.sum() - jump_morphs.get('noise', 0) - jump_morphs.get('paczynski', 0)):,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"NEXT STEPS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "1. Generate plots for top candidates:\n",
    "   python -m malca postprocess --detect-run output/runs/<run_id> --max-plots 50 -v\n",
    "\n",
    "2. Run characterization to get stellar parameters:\n",
    "   python -m malca detect --mag-bin <bin> --run-characterize --gaia-cache output/gaia_cache.parquet\n",
    "\n",
    "3. Cross-match with known variable catalogs:\n",
    "   - VSX (Variable Star Index)\n",
    "   - GCVS (General Catalog of Variable Stars)\n",
    "   - Gaia DR3 variables\n",
    "\n",
    "4. Visual inspection of light curves for promising candidates\n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
