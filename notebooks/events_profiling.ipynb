{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Profiling `malca.events` on SkyPatrol light curves\n",
        "\n",
        "Use `cProfile` to time the Bayesian event scorer on real SkyPatrol CSVs stored in `input/skypatrol2`. The notebook keeps everything self contained so it can run directly against the repo without extra setup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment setup\n",
        "\n",
        "Locate the repo root, add it to `sys.path`, and collect the SkyPatrol CSVs we'll profile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Find repo root whether the notebook is run from notebooks/ or repo root\n",
        "candidates = [Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent]\n",
        "repo_root = next((p for p in candidates if (p / \"malca\").exists()), Path.cwd())\n",
        "\n",
        "for path in (repo_root, repo_root / \"malca\"):\n",
        "    sp = str(path.resolve())\n",
        "    if sp not in sys.path:\n",
        "        sys.path.insert(0, sp)\n",
        "\n",
        "data_dir = repo_root / \"input\" / \"skypatrol2\"\n",
        "lc_paths = sorted(data_dir.glob(\"*-light-curves.csv\"))\n",
        "\n",
        "print(f\"Repo root: {repo_root}\")\n",
        "print(f\"Found {len(lc_paths)} light curves in {data_dir}\")\n",
        "if not lc_paths:\n",
        "    raise FileNotFoundError(\"No SkyPatrol CSVs found; adjust data_dir above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick peek at one SkyPatrol light curve\n",
        "\n",
        "Read a single CSV to confirm the loader works and to see the columns that flow into the scorer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from malca.plot import read_skypatrol_csv\n",
        "\n",
        "example_path = lc_paths[0]\n",
        "df_example = read_skypatrol_csv(example_path)\n",
        "print(f\"{example_path.name}: {len(df_example)} rows\")\n",
        "display(df_example.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profiling helpers\n",
        "\n",
        "Wrap `_process_one` so we can reuse the same kwargs as the CLI and capture both `cProfile` output and a tidy summary table of the hottest functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import cProfile\n",
        "import io\n",
        "import pstats\n",
        "import time\n",
        "\n",
        "import malca.events as events\n",
        "\n",
        "DEFAULT_EVENT_KWARGS = {\n",
        "    \"trigger_mode\": \"posterior_prob\",\n",
        "    \"logbf_threshold_dip\": 5.0,\n",
        "    \"logbf_threshold_jump\": 5.0,\n",
        "    \"significance_threshold\": 99.99997,\n",
        "    \"p_points\": 80,\n",
        "    \"p_min_dip\": None,\n",
        "    \"p_max_dip\": None,\n",
        "    \"p_min_jump\": None,\n",
        "    \"p_max_jump\": None,\n",
        "    \"run_min_points\": 3,\n",
        "    \"run_allow_gap_points\": 1,\n",
        "    \"run_max_gap_days\": None,\n",
        "    \"run_min_duration_days\": None,\n",
        "    \"run_sum_threshold\": None,\n",
        "    \"run_sum_multiplier\": 2.5,\n",
        "    \"baseline_tag\": \"gp\",\n",
        "    \"use_sigma_eff\": True,\n",
        "    \"require_sigma_eff\": True,\n",
        "    \"compute_event_prob\": True,\n",
        "}\n",
        "\n",
        "def score_single_lc(path: Path, **overrides):\n",
        "    kwargs = dict(DEFAULT_EVENT_KWARGS)\n",
        "    kwargs.update(overrides)\n",
        "    return events._process_one(str(path), **kwargs)\n",
        "\n",
        "def stats_to_frame(stats_obj, limit=20):\n",
        "    rows = []\n",
        "    for func, (cc, nc, tt, ct, callers) in stats_obj.stats.items():\n",
        "        rows.append(\n",
        "            {\n",
        "                \"func\": f\"{func[2]} ({Path(func[0]).name}:{func[1]})\",\n",
        "                \"ncalls\": nc,\n",
        "                \"tottime_s\": tt,\n",
        "                \"cumtime_s\": ct,\n",
        "            }\n",
        "        )\n",
        "    df_stats = pd.DataFrame(rows)\n",
        "    if df_stats.empty:\n",
        "        return df_stats\n",
        "    return df_stats.sort_values(\"cumtime_s\", ascending=False).head(limit)\n",
        "\n",
        "def profile_light_curve(path: Path, stats_limit=25, **overrides):\n",
        "    profiler = cProfile.Profile()\n",
        "    start = time.perf_counter()\n",
        "    result = profiler.runcall(score_single_lc, path, **overrides)\n",
        "    elapsed = time.perf_counter() - start\n",
        "\n",
        "    stats_buffer = io.StringIO()\n",
        "    stats_obj = pstats.Stats(profiler, stream=stats_buffer).strip_dirs().sort_stats(\"cumtime\")\n",
        "    stats_obj.print_stats(stats_limit)\n",
        "    stats_text = stats_buffer.getvalue()\n",
        "\n",
        "    top_df = stats_to_frame(stats_obj, limit=stats_limit)\n",
        "    return result, stats_text, top_df, elapsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profile a single light curve\n",
        "\n",
        "Run the Bayesian scorer on one SkyPatrol CSV (default parameters match the CLI). `stats_df_single` highlights the functions consuming the most cumulative time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "single_path = lc_paths[0]\n",
        "result_single, stats_text_single, stats_df_single, elapsed_single = profile_light_curve(single_path, stats_limit=25)\n",
        "\n",
        "print(f\"Profiled {single_path.name} in {elapsed_single:.2f} s\")\n",
        "print(stats_text_single)\n",
        "stats_df_single"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch timing on a handful of files (optional)\n",
        "\n",
        "Process a small subset sequentially to gauge throughput without full profiling. Adjust `N_LC` to cover more files if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "N_LC = 3\n",
        "subset_paths = lc_paths[:N_LC]\n",
        "\n",
        "timing_rows = []\n",
        "for path in subset_paths:\n",
        "    t0 = time.perf_counter()\n",
        "    res = score_single_lc(path)\n",
        "    timing_rows.append(\n",
        "        {\n",
        "            \"path\": path.name,\n",
        "            \"elapsed_s\": time.perf_counter() - t0,\n",
        "            \"n_points\": res.get(\"n_points\"),\n",
        "            \"dip_sig\": res.get(\"dip_significant\"),\n",
        "            \"jump_sig\": res.get(\"jump_significant\"),\n",
        "            \"dip_bf\": res.get(\"dip_bayes_factor\"),\n",
        "            \"jump_bf\": res.get(\"jump_bayes_factor\"),\n",
        "        }\n",
        "    )\n",
        "\n",
        "pd.DataFrame(timing_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grid resolution sweep\n",
        "Compare how `p_points` (probability grid) and `mag_points` (magnitude grid) affect runtime and outputs on a small SkyPatrol subset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import time\n",
        "from malca.baseline import per_camera_gp_baseline\n",
        "import malca.events as events\n",
        "\n",
        "BASELINE_KWARGS = dict(events.DEFAULT_BASELINE_KWARGS)\n",
        "\n",
        "def prepare_light_curve(path):\n",
        "    df_raw = read_skypatrol_csv(path)\n",
        "    valid_mask = (\n",
        "        df_raw[\"JD\"].pipe(np.isfinite)\n",
        "        & df_raw[\"mag\"].pipe(np.isfinite)\n",
        "        & df_raw[\"error\"].pipe(np.isfinite)\n",
        "        & (df_raw[\"error\"] > 0)\n",
        "        & (df_raw[\"error\"] < 10)\n",
        "    )\n",
        "    df = df_raw[valid_mask].copy()\n",
        "    if len(df) < 10:\n",
        "        raise ValueError(f\"Insufficient valid data points ({len(df)}) in {path.name}\")\n",
        "    df = events.clean_lc(df)\n",
        "\n",
        "    df_base = per_camera_gp_baseline(df, **BASELINE_KWARGS)\n",
        "    baseline_mags = df_base[\"baseline\"].to_numpy(float) if \"baseline\" in df_base.columns else df_base[\"mag\"].to_numpy(float)\n",
        "    baseline_mag = float(np.nanmedian(baseline_mags))\n",
        "    mags_for_grid = df_base[\"mag\"].to_numpy(float) if \"mag\" in df_base.columns else df[\"mag\"].to_numpy(float)\n",
        "\n",
        "    def baseline_precomputed(df_in, **kwargs):\n",
        "        return df_base\n",
        "\n",
        "    return df, baseline_precomputed, baseline_mag, mags_for_grid\n",
        "\n",
        "def run_grid_setting(path, p_points, mag_points, label):\n",
        "    df, baseline_fn, baseline_mag, mags_for_grid = prepare_light_curve(path)\n",
        "    mag_grid_dip = events.default_mag_grid(baseline_mag, mags_for_grid, \"dip\", n=mag_points)\n",
        "    mag_grid_jump = events.default_mag_grid(baseline_mag, mags_for_grid, \"jump\", n=mag_points)\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    res = events.run_bayesian_significance(\n",
        "        df,\n",
        "        baseline_func=baseline_fn,\n",
        "        baseline_kwargs={},\n",
        "        p_points=p_points,\n",
        "        mag_grid_dip=mag_grid_dip,\n",
        "        mag_grid_jump=mag_grid_jump,\n",
        "        trigger_mode=\"posterior_prob\",\n",
        "        logbf_threshold_dip=5.0,\n",
        "        logbf_threshold_jump=5.0,\n",
        "        significance_threshold=99.99997,\n",
        "        run_min_points=3,\n",
        "        run_allow_gap_points=1,\n",
        "        run_max_gap_days=None,\n",
        "        run_min_duration_days=None,\n",
        "        run_sum_threshold=None,\n",
        "        run_sum_multiplier=2.5,\n",
        "        use_sigma_eff=True,\n",
        "        require_sigma_eff=True,\n",
        "        compute_event_prob=True,\n",
        "    )\n",
        "    elapsed = time.perf_counter() - start\n",
        "\n",
        "    return {\n",
        "        \"path\": path.name,\n",
        "        \"config\": label,\n",
        "        \"p_points\": p_points,\n",
        "        \"mag_points\": mag_points,\n",
        "        \"elapsed_s\": elapsed,\n",
        "        \"dip_sig\": res[\"dip\"][\"significant\"],\n",
        "        \"jump_sig\": res[\"jump\"][\"significant\"],\n",
        "        \"dip_bf\": res[\"dip\"][\"bayes_factor\"],\n",
        "        \"jump_bf\": res[\"jump\"][\"bayes_factor\"],\n",
        "        \"dip_best_p\": res[\"dip\"][\"best_p\"],\n",
        "        \"jump_best_p\": res[\"jump\"][\"best_p\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Evaluate a few grid settings on a small subset of light curves\n",
        "N_LC = 3\n",
        "subset_paths = lc_paths[:N_LC]\n",
        "\n",
        "grid_settings = [\n",
        "    {\"label\": \"baseline_80x60\", \"p_points\": 80, \"mag_points\": 60},\n",
        "    {\"label\": \"coarse_40x30\", \"p_points\": 40, \"mag_points\": 30},\n",
        "    {\"label\": \"fine_p_160x60\", \"p_points\": 160, \"mag_points\": 60},\n",
        "    {\"label\": \"fine_mag_80x120\", \"p_points\": 80, \"mag_points\": 120},\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for path in subset_paths:\n",
        "    for cfg in grid_settings:\n",
        "        rows.append(run_grid_setting(path, cfg[\"p_points\"], cfg[\"mag_points\"], cfg[\"label\"]))\n",
        "\n",
        "df_grid = pd.DataFrame(rows)\n",
        "df_grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Compare each setting to the baseline configuration\n",
        "baseline_label = \"baseline_80x60\"\n",
        "base = df_grid[df_grid[\"config\"] == baseline_label]\n",
        "comparison = df_grid.merge(base, on=\"path\", suffixes=(\"\", \"_base\"))\n",
        "comparison[\"dip_bf_delta\"] = comparison[\"dip_bf\"] - comparison[\"dip_bf_base\"]\n",
        "comparison[\"jump_bf_delta\"] = comparison[\"jump_bf\"] - comparison[\"jump_bf_base\"]\n",
        "comparison[\"elapsed_delta_s\"] = comparison[\"elapsed_s\"] - comparison[\"elapsed_s_base\"]\n",
        "cols = [\n",
        "    \"path\",\n",
        "    \"config\",\n",
        "    \"p_points\",\n",
        "    \"mag_points\",\n",
        "    \"elapsed_s\",\n",
        "    \"elapsed_delta_s\",\n",
        "    \"dip_sig\",\n",
        "    \"dip_sig_base\",\n",
        "    \"dip_bf\",\n",
        "    \"dip_bf_delta\",\n",
        "    \"jump_sig\",\n",
        "    \"jump_sig_base\",\n",
        "    \"jump_bf\",\n",
        "    \"jump_bf_delta\",\n",
        "]\n",
        "comparison[cols]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulated dips and jumps\n",
        "Create a small suite of synthetic light curves (reusing a real SkyPatrol cadence) with injected dips, jumps, and a mixed case so grid sensitivity can be tested on known events."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Build simulated light curves by injecting analytic events into a real cadence\n",
        "from malca import events\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "base_sim_path = lc_paths[0]\n",
        "df_sim_base_raw = read_skypatrol_csv(base_sim_path)\n",
        "mask = (\n",
        "    df_sim_base_raw[\"JD\"].pipe(np.isfinite)\n",
        "    & df_sim_base_raw[\"mag\"].pipe(np.isfinite)\n",
        "    & df_sim_base_raw[\"error\"].pipe(np.isfinite)\n",
        "    & (df_sim_base_raw[\"error\"] > 0)\n",
        "    & (df_sim_base_raw[\"error\"] < 10)\n",
        ")\n",
        "df_sim_base = df_sim_base_raw[mask].copy()\n",
        "df_sim_base = events.clean_lc(df_sim_base)\n",
        "\n",
        "jd_med = float(df_sim_base[\"JD\"].median())\n",
        "\n",
        "\n",
        "def inject_event(df_in, kind=\"dip\", shape=\"gaussian\", amp=0.25, width=25.0, t0_offset=0.0):\n",
        "    # Return a copy with an injected event (positive amp = dip, negative amp = jump).\n",
        "    df = df_in.copy()\n",
        "    t0 = jd_med + float(t0_offset)\n",
        "    amp_signed = float(amp) if kind == \"dip\" else -float(amp)\n",
        "    t_arr = df[\"JD\"].to_numpy(float)\n",
        "    if shape == \"gaussian\":\n",
        "        delta = events.gaussian(t_arr, amp_signed, t0, float(width), 0.0)\n",
        "    elif shape == \"paczynski\":\n",
        "        delta = events.paczynski(t_arr, amp_signed, t0, float(width), 0.0)\n",
        "    else:\n",
        "        raise ValueError(\"shape must be gaussian or paczynski\")\n",
        "    df[\"mag\"] = df[\"mag\"].to_numpy(float) + delta\n",
        "    return df\n",
        "\n",
        "\n",
        "def inject_mixed(df_in):\n",
        "    df = inject_event(df_in, kind=\"dip\", shape=\"gaussian\", amp=0.18, width=20.0, t0_offset=-60.0)\n",
        "    df = inject_event(df, kind=\"jump\", shape=\"paczynski\", amp=0.14, width=30.0, t0_offset=40.0)\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_noise_and_gaps(df_in, jitter_mag=0.02, gap_frac=0.15, spike_amp=0.25, spike_width=0.08, spike_count=3):\n",
        "    # Add photometric jitter, drop random points, and sprinkle outliers to make messy cases.\n",
        "    df = df_in.copy()\n",
        "    n = len(df)\n",
        "    df[\"mag\"] = df[\"mag\"].to_numpy(float) + rng.normal(0.0, jitter_mag, n)\n",
        "    df[\"error\"] = (df[\"error\"].to_numpy(float) * (1 + rng.normal(0.0, 0.05, n))).clip(min=1e-3)\n",
        "    keep = rng.random(n) > gap_frac\n",
        "    df = df.loc[keep].copy()\n",
        "    if len(df) == 0:\n",
        "        return df_in.copy()\n",
        "    for _ in range(int(spike_count)):\n",
        "        idx = int(rng.integers(0, len(df)))\n",
        "        df.loc[idx, \"mag\"] += rng.normal(spike_amp, spike_width)\n",
        "    df = df.sort_values(\"JD\").reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "simulated_lcs = {\n",
        "    \"dip_gaussian\": inject_event(df_sim_base, kind=\"dip\", shape=\"gaussian\", amp=0.22, width=25.0, t0_offset=-30.0),\n",
        "    \"dip_paczynski\": inject_event(df_sim_base, kind=\"dip\", shape=\"paczynski\", amp=0.28, width=18.0, t0_offset=20.0),\n",
        "    \"dip_shallow_fast\": inject_event(df_sim_base, kind=\"dip\", shape=\"gaussian\", amp=0.12, width=8.0, t0_offset=-10.0),\n",
        "    \"dip_double\": inject_event(inject_event(df_sim_base, kind=\"dip\", shape=\"gaussian\", amp=0.18, width=15.0, t0_offset=-50.0), kind=\"dip\", shape=\"paczynski\", amp=0.15, width=12.0, t0_offset=25.0),\n",
        "    \"jump_gaussian\": inject_event(df_sim_base, kind=\"jump\", shape=\"gaussian\", amp=0.20, width=22.0, t0_offset=-15.0),\n",
        "    \"jump_paczynski\": inject_event(df_sim_base, kind=\"jump\", shape=\"paczynski\", amp=0.24, width=16.0, t0_offset=45.0),\n",
        "    \"microlens_weak\": inject_event(df_sim_base, kind=\"jump\", shape=\"paczynski\", amp=0.10, width=10.0, t0_offset=5.0),\n",
        "    \"microlens_strong\": inject_event(df_sim_base, kind=\"jump\", shape=\"paczynski\", amp=0.30, width=26.0, t0_offset=70.0),\n",
        "    \"mixed\": inject_mixed(df_sim_base),\n",
        "    \"messy_dip\": add_noise_and_gaps(inject_event(df_sim_base, kind=\"dip\", shape=\"gaussian\", amp=0.20, width=18.0, t0_offset=-5.0), jitter_mag=0.03, gap_frac=0.2, spike_count=3),\n",
        "    \"messy_jump\": add_noise_and_gaps(inject_event(df_sim_base, kind=\"jump\", shape=\"paczynski\", amp=0.18, width=14.0, t0_offset=35.0), jitter_mag=0.03, gap_frac=0.2, spike_count=3),\n",
        "    \"messy_mixed\": add_noise_and_gaps(inject_mixed(df_sim_base), jitter_mag=0.04, gap_frac=0.25, spike_count=4),\n",
        "}\n",
        "\n",
        "print(f\"Simulated light curves: {list(simulated_lcs.keys())}\")\n",
        "display(df_sim_base.head())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from malca.baseline import per_camera_gp_baseline\n",
        "\n",
        "\n",
        "def prepare_df_for_sim(df):\n",
        "    df_clean = events.clean_lc(df)\n",
        "    df_base = per_camera_gp_baseline(df_clean, **BASELINE_KWARGS)\n",
        "    baseline_mags = df_base[\"baseline\"].to_numpy(float) if \"baseline\" in df_base.columns else df_base[\"mag\"].to_numpy(float)\n",
        "    baseline_mag = float(np.nanmedian(baseline_mags))\n",
        "    mags_for_grid = df_base[\"mag\"].to_numpy(float) if \"mag\" in df_base.columns else df_clean[\"mag\"].to_numpy(float)\n",
        "\n",
        "    def baseline_precomputed(df_in, **kwargs):\n",
        "        return df_base\n",
        "\n",
        "    return df_clean, baseline_precomputed, baseline_mag, mags_for_grid\n",
        "\n",
        "\n",
        "def run_grid_setting_sim(name, df, p_points, mag_points, label):\n",
        "    df_clean, baseline_fn, baseline_mag, mags_for_grid = prepare_df_for_sim(df)\n",
        "    mag_grid_dip = events.default_mag_grid(baseline_mag, mags_for_grid, \"dip\", n=mag_points)\n",
        "    mag_grid_jump = events.default_mag_grid(baseline_mag, mags_for_grid, \"jump\", n=mag_points)\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    res = events.run_bayesian_significance(\n",
        "        df_clean,\n",
        "        baseline_func=baseline_fn,\n",
        "        baseline_kwargs={},\n",
        "        p_points=p_points,\n",
        "        mag_grid_dip=mag_grid_dip,\n",
        "        mag_grid_jump=mag_grid_jump,\n",
        "        trigger_mode=\"posterior_prob\",\n",
        "        logbf_threshold_dip=5.0,\n",
        "        logbf_threshold_jump=5.0,\n",
        "        significance_threshold=99.99997,\n",
        "        run_min_points=3,\n",
        "        run_allow_gap_points=1,\n",
        "        run_max_gap_days=None,\n",
        "        run_min_duration_days=None,\n",
        "        run_sum_threshold=None,\n",
        "        run_sum_multiplier=2.5,\n",
        "        use_sigma_eff=True,\n",
        "        require_sigma_eff=True,\n",
        "        compute_event_prob=True,\n",
        "    )\n",
        "    elapsed = time.perf_counter() - start\n",
        "\n",
        "    return {\n",
        "        \"case\": name,\n",
        "        \"config\": label,\n",
        "        \"p_points\": p_points,\n",
        "        \"mag_points\": mag_points,\n",
        "        \"elapsed_s\": elapsed,\n",
        "        \"dip_sig\": res[\"dip\"][\"significant\"],\n",
        "        \"jump_sig\": res[\"jump\"][\"significant\"],\n",
        "        \"dip_bf\": res[\"dip\"][\"bayes_factor\"],\n",
        "        \"jump_bf\": res[\"jump\"][\"bayes_factor\"],\n",
        "        \"dip_best_p\": res[\"dip\"][\"best_p\"],\n",
        "        \"jump_best_p\": res[\"jump\"][\"best_p\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Sweep grid sizes on the simulated cases\n",
        "sim_grid_settings = [\n",
        "    {\"label\": \"sim_10x10\", \"p_points\": 10, \"mag_points\": 10},\n",
        "    {\"label\": \"sim_25x25\", \"p_points\": 25, \"mag_points\": 25},\n",
        "    {\"label\": \"sim_50x50\", \"p_points\": 50, \"mag_points\": 50},\n",
        "    {\"label\": \"sim_80x60\", \"p_points\": 80, \"mag_points\": 60},\n",
        "]\n",
        "\n",
        "sim_rows = []\n",
        "for name, df_sim in simulated_lcs.items():\n",
        "    for cfg in sim_grid_settings:\n",
        "        sim_rows.append(run_grid_setting_sim(name, df_sim, cfg[\"p_points\"], cfg[\"mag_points\"], cfg[\"label\"]))\n",
        "\n",
        "sim_results = pd.DataFrame(sim_rows)\n",
        "sim_results"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}