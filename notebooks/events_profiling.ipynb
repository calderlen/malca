{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Profiling `malca.events` on SkyPatrol light curves\n",
        "\n",
        "Use `cProfile` to time the Bayesian event scorer on real SkyPatrol CSVs stored in `input/skypatrol2`. The notebook keeps everything self contained so it can run directly against the repo without extra setup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment setup\n",
        "\n",
        "Locate the repo root, add it to `sys.path`, and collect the SkyPatrol CSVs we'll profile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Find repo root whether the notebook is run from notebooks/ or repo root\n",
        "candidates = [Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent]\n",
        "repo_root = next((p for p in candidates if (p / \"malca\").exists()), Path.cwd())\n",
        "\n",
        "for path in (repo_root, repo_root / \"malca\"):\n",
        "    sp = str(path.resolve())\n",
        "    if sp not in sys.path:\n",
        "        sys.path.insert(0, sp)\n",
        "\n",
        "data_dir = repo_root / \"input\" / \"skypatrol2\"\n",
        "lc_paths = sorted(data_dir.glob(\"*-light-curves.csv\"))\n",
        "\n",
        "print(f\"Repo root: {repo_root}\")\n",
        "print(f\"Found {len(lc_paths)} light curves in {data_dir}\")\n",
        "if not lc_paths:\n",
        "    raise FileNotFoundError(\"No SkyPatrol CSVs found; adjust data_dir above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick peek at one SkyPatrol light curve\n",
        "\n",
        "Read a single CSV to confirm the loader works and to see the columns that flow into the scorer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from malca.plot import read_skypatrol_csv\n",
        "\n",
        "example_path = lc_paths[0]\n",
        "df_example = read_skypatrol_csv(example_path)\n",
        "print(f\"{example_path.name}: {len(df_example)} rows\")\n",
        "display(df_example.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profiling helpers\n",
        "\n",
        "Wrap `_process_one` so we can reuse the same kwargs as the CLI and capture both `cProfile` output and a tidy summary table of the hottest functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import cProfile\n",
        "import io\n",
        "import pstats\n",
        "import time\n",
        "\n",
        "import malca.events as events\n",
        "\n",
        "DEFAULT_EVENT_KWARGS = {\n",
        "    \"trigger_mode\": \"posterior_prob\",\n",
        "    \"logbf_threshold_dip\": 5.0,\n",
        "    \"logbf_threshold_jump\": 5.0,\n",
        "    \"significance_threshold\": 99.99997,\n",
        "    \"p_points\": 80,\n",
        "    \"p_min_dip\": None,\n",
        "    \"p_max_dip\": None,\n",
        "    \"p_min_jump\": None,\n",
        "    \"p_max_jump\": None,\n",
        "    \"run_min_points\": 3,\n",
        "    \"run_allow_gap_points\": 1,\n",
        "    \"run_max_gap_days\": None,\n",
        "    \"run_min_duration_days\": None,\n",
        "    \"run_sum_threshold\": None,\n",
        "    \"run_sum_multiplier\": 2.5,\n",
        "    \"baseline_tag\": \"gp\",\n",
        "    \"use_sigma_eff\": True,\n",
        "    \"require_sigma_eff\": True,\n",
        "    \"compute_event_prob\": True,\n",
        "}\n",
        "\n",
        "def score_single_lc(path: Path, **overrides):\n",
        "    kwargs = dict(DEFAULT_EVENT_KWARGS)\n",
        "    kwargs.update(overrides)\n",
        "    return events._process_one(str(path), **kwargs)\n",
        "\n",
        "def stats_to_frame(stats_obj, limit=20):\n",
        "    rows = []\n",
        "    for func, (cc, nc, tt, ct, callers) in stats_obj.stats.items():\n",
        "        rows.append(\n",
        "            {\n",
        "                \"func\": f\"{func[2]} ({Path(func[0]).name}:{func[1]})\",\n",
        "                \"ncalls\": nc,\n",
        "                \"tottime_s\": tt,\n",
        "                \"cumtime_s\": ct,\n",
        "            }\n",
        "        )\n",
        "    df_stats = pd.DataFrame(rows)\n",
        "    if df_stats.empty:\n",
        "        return df_stats\n",
        "    return df_stats.sort_values(\"cumtime_s\", ascending=False).head(limit)\n",
        "\n",
        "def profile_light_curve(path: Path, stats_limit=25, **overrides):\n",
        "    profiler = cProfile.Profile()\n",
        "    start = time.perf_counter()\n",
        "    result = profiler.runcall(score_single_lc, path, **overrides)\n",
        "    elapsed = time.perf_counter() - start\n",
        "\n",
        "    stats_buffer = io.StringIO()\n",
        "    stats_obj = pstats.Stats(profiler, stream=stats_buffer).strip_dirs().sort_stats(\"cumtime\")\n",
        "    stats_obj.print_stats(stats_limit)\n",
        "    stats_text = stats_buffer.getvalue()\n",
        "\n",
        "    top_df = stats_to_frame(stats_obj, limit=stats_limit)\n",
        "    return result, stats_text, top_df, elapsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profile a single light curve\n",
        "\n",
        "Run the Bayesian scorer on one SkyPatrol CSV (default parameters match the CLI). `stats_df_single` highlights the functions consuming the most cumulative time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "single_path = lc_paths[0]\n",
        "result_single, stats_text_single, stats_df_single, elapsed_single = profile_light_curve(single_path, stats_limit=25)\n",
        "\n",
        "print(f\"Profiled {single_path.name} in {elapsed_single:.2f} s\")\n",
        "print(stats_text_single)\n",
        "stats_df_single"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch timing on a handful of files (optional)\n",
        "\n",
        "Process a small subset sequentially to gauge throughput without full profiling. Adjust `N_LC` to cover more files if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "N_LC = 3\n",
        "subset_paths = lc_paths[:N_LC]\n",
        "\n",
        "timing_rows = []\n",
        "for path in subset_paths:\n",
        "    t0 = time.perf_counter()\n",
        "    res = score_single_lc(path)\n",
        "    timing_rows.append(\n",
        "        {\n",
        "            \"path\": path.name,\n",
        "            \"elapsed_s\": time.perf_counter() - t0,\n",
        "            \"n_points\": res.get(\"n_points\"),\n",
        "            \"dip_sig\": res.get(\"dip_significant\"),\n",
        "            \"jump_sig\": res.get(\"jump_significant\"),\n",
        "            \"dip_bf\": res.get(\"dip_bayes_factor\"),\n",
        "            \"jump_bf\": res.get(\"jump_bayes_factor\"),\n",
        "        }\n",
        "    )\n",
        "\n",
        "pd.DataFrame(timing_rows)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}